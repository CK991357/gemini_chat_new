# ğŸ“š æ–‡æœ¬åˆ†æä¸ç»“æ„åŒ–æå–æ•™ç¨‹ (v2.2 - æ²™ç›’ä¼˜åŒ–ç‰ˆ)

## ğŸ¯ æ–‡æ¡£ç›®æ ‡
ä¸ºAIåŠ©æ‰‹æä¾›ä¸€å¥—**æ— éœ€ç½‘ç»œæƒé™**ã€**å®‰å…¨å¯é **çš„æ–‡æœ¬åˆ†æè§£å†³æ–¹æ¡ˆï¼Œä¸“é—¨ç”¨äºå¤„ç†å·²è·å–çš„ç½‘é¡µå†…å®¹ã€æ–‡æ¡£æ•°æ®ç­‰ç»“æ„åŒ–ä¿¡æ¯æå–ã€‚

---

## ğŸ§  æ ¸å¿ƒè®¾è®¡åŸåˆ™

### âœ… å¿…é¡»éµå®ˆ
1. **é›¶ç½‘ç»œä¾èµ–** - æ‰€æœ‰åˆ†æåŸºäºå·²æä¾›çš„æ–‡æœ¬æ•°æ®
2. **å®‰å…¨ç¬¬ä¸€** - ä»…ä½¿ç”¨Pythonæ ‡å‡†åº“å’Œé¢„è£…å®‰å…¨åº“
3. **æ ¼å¼æ ‡å‡†åŒ–** - è¾“å‡ºå¿…é¡»ç¬¦åˆç³»ç»Ÿå¯è¯†åˆ«çš„JSONç»“æ„
4. **é”™è¯¯åŒ…å®¹æ€§** - æå–å¤±è´¥æ—¶æä¾›åˆç†çš„é»˜è®¤å€¼
5. **å‡½æ•°å¼ç¼–ç¨‹** - é¿å…ä½¿ç”¨ç±»å®šä¹‰ï¼Œæ²™ç›’ç¯å¢ƒå¯¹ç±»æ”¯æŒæœ‰é™

### âŒ å¿…é¡»é¿å…
1. ç½‘ç»œè¯·æ±‚ã€APIè°ƒç”¨
2. æ–‡ä»¶ç³»ç»Ÿè¶Šæƒè®¿é—®
3. éå®‰å…¨çš„åº“å¯¼å…¥
4. æ— é™å¾ªç¯æˆ–èµ„æºè€—å°½æ“ä½œ
5. ç±»å®šä¹‰ï¼ˆä½¿ç”¨å‡½æ•°å¼æ›¿ä»£ï¼‰

---

## ğŸš€ å¿«é€Ÿå¼€å§‹æ¨¡æ¿

### åœºæ™¯ä¸€ï¼šç›´æ¥åˆ†æç½‘é¡µæŠ“å–å†…å®¹
```python
# ===================== åŸºç¡€åˆ†ææ¨¡æ¿ =====================
import json
import re
from datetime import datetime

def analyze_webpage_content(text_content: str) -> dict:
    """
    åŸºç¡€ç½‘é¡µå†…å®¹åˆ†æå™¨
    è¾“å…¥ï¼šä»»ä½•ç½‘é¡µçš„æ–‡æœ¬å†…å®¹
    è¾“å‡ºï¼šç»“æ„åŒ–æå–ç»“æœ
    """
    # åˆå§‹åŒ–æ ‡å‡†è¾“å‡ºç»“æ„
    result = {
        "type": "analysis_report",
        "title": "ç½‘é¡µå†…å®¹åˆ†ææŠ¥å‘Š",
        "timestamp": datetime.now().isoformat(),
        "data": {
            "åŸºæœ¬ä¿¡æ¯": {},
            "ä»·æ ¼ä¿¡æ¯": {},
            "äº§å“è§„æ ¼": {},
            "æå–æ‘˜è¦": ""
        }
    }
    
    # 1. åŸºæœ¬ä¿¡æ¯æå–ï¼ˆç¤ºä¾‹ï¼‰
    if "äº§å“" in text_content or "Product" in text_content:
        result["data"]["åŸºæœ¬ä¿¡æ¯"]["ç±»å‹"] = "äº§å“é¡µé¢"
    
    # 2. ä»·æ ¼æå–ï¼ˆå¤šå¸ç§æ”¯æŒï¼‰
    price_patterns = {
        "USD": r'\$\s*(\d+[,\d]*\.?\d*)',
        "CNY": r'Â¥\s*(\d+[,\d]*)',
        "HKD": r'HK\$\s*(\d+[,\d]*\.?\d*)'
    }
    
    for currency, pattern in price_patterns.items():
        match = re.search(pattern, text_content)
        if match:
            result["data"]["ä»·æ ¼ä¿¡æ¯"][currency] = match.group(1)
    
    # 3. å…³é”®ä¿¡æ¯æ‘˜è¦
    lines = text_content.split('\n')
    key_lines = [line.strip() for line in lines if len(line.strip()) > 20][:5]
    result["data"]["æå–æ‘˜è¦"] = " | ".join(key_lines)
    
    return result

# ===================== æ‰§è¡Œç¤ºä¾‹ =====================
if __name__ == "__main__":
    # å°†æ‚¨çš„data_contextç²˜è´´åœ¨è¿™é‡Œ
    sample_text = """
    äº§å“åç§°ï¼šJimmy Choo DIDI 45
    ä»·æ ¼ï¼š$299.99
    æè´¨ï¼šçš®é©é‹é¢ï¼Œç»¸ç¼å†…è¡¬
    è·Ÿé«˜ï¼š45mm
    ç‰¹ç‚¹ï¼šå°–å¤´è®¾è®¡ï¼Œä¼˜é›…å¥³æ€§é‹å±¥
    """
    
    analysis_result = analyze_webpage_content(sample_text)
    
    # ğŸ”¥ å…³é”®ï¼šå¿…é¡»ä½¿ç”¨printè¾“å‡ºJSONæ ¼å¼
    print(json.dumps(analysis_result, ensure_ascii=False, indent=2))
```

### åœºæ™¯äºŒï¼šå¤šé¡µé¢æ‰¹é‡åˆ†æ
```python
import json

def analyze_multiple_pages(pages_data: str) -> dict:
    """
    å¤„ç†åŒ…å«å¤šä¸ªé¡µé¢çš„æ–‡æœ¬æ•°æ®
    æ ¼å¼ï¼šä»¥"## é¡µé¢"åˆ†éš”çš„ä¸åŒé¡µé¢
    """
    results = []
    
    # åˆ†å‰²é¡µé¢
    if "## é¡µé¢" in pages_data:
        pages = pages_data.split("## é¡µé¢")[1:]
        
        for i, page_content in enumerate(pages[:3]):  # é™åˆ¶å‰3é¡µ
            # è°ƒç”¨å•é¡µåˆ†æå™¨
            page_result = analyze_webpage_content(page_content)
            page_result["page_number"] = i + 1
            results.append(page_result)
    else:
        # å•é¡µæƒ…å†µ
        results.append(analyze_webpage_content(pages_data))
    
    final_output = {
        "type": "multi_page_analysis",
        "total_pages": len(results),
        "pages": results,
        "summary": f"æˆåŠŸåˆ†æ {len(results)} ä¸ªé¡µé¢"
    }
    
    return final_output
```

---

## ğŸ“Š è¾“å‡ºæ ¼å¼è§„èŒƒï¼ˆç³»ç»Ÿå¼ºåˆ¶è¦æ±‚ï¼‰

### âœ… æ­£ç¡®æ ¼å¼ç¤ºä¾‹
```json
{
    "type": "analysis_report",  // å¿…é¡»å­—æ®µï¼Œå®šä¹‰è¾“å‡ºç±»å‹
    "title": "åˆ†ææŠ¥å‘Šæ ‡é¢˜",     // ç”¨æˆ·å¯è§çš„æ ‡é¢˜
    "data": {                  // å®é™…åˆ†ææ•°æ®
        "field1": "value1",
        "field2": ["item1", "item2"]
    }
}
```

### âŒ é”™è¯¯æ ¼å¼ç¤ºä¾‹
```python
# é”™è¯¯1ï¼šç›´æ¥æ‰“å°å­—å…¸
print(analysis_result)  # ç³»ç»Ÿæ— æ³•è§£æ

# é”™è¯¯2ï¼šéJSONå­—ç¬¦ä¸²
print("ä»·æ ¼ï¼š$299.99")  # ç³»ç»Ÿæ— æ³•ç»“æ„åŒ–å¤„ç†

# é”™è¯¯3ï¼šç¼ºå°‘typeå­—æ®µ
{"data": {...}}  # ç³»ç»Ÿæ— æ³•è¯†åˆ«ç±»å‹

# é”™è¯¯4ï¼šä½¿ç”¨ç±»å®šä¹‰
class Extractor:  # æ²™ç›’ç¯å¢ƒå¯èƒ½ä¸æ”¯æŒ
    def extract(self): pass
```

---

## ğŸ› ï¸ ä¸“ä¸šåˆ†æå·¥å…·ç®±

### 1. ä»·æ ¼æå–å™¨

## ğŸ”§ ä»·æ ¼ä¿¡æ¯æå–ï¼ˆå…³é”®æ›´æ–°ï¼‰

### ğŸš« ç¦æ­¢æ“ä½œ
- âŒ ç±»å®šä¹‰ï¼ˆ`class PriceExtractor:`ï¼‰ - æ²™ç›’ç¯å¢ƒä¸æ”¯æŒ
- âŒ ä½¿ç”¨ä¸å­˜åœ¨çš„åº“ï¼ˆå¦‚ `PriceExtractor`ï¼‰

### âœ… æ¨èæ–¹æ¡ˆï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ä»·æ ¼
```python
import re
import json

def extract_price_info(text):
    """ä»æ–‡æœ¬ä¸­æå–ä»·æ ¼ä¿¡æ¯"""
    price_patterns = [
        r'(\$\d+(?:\.\d+)?)\s*per\s*1[kK]\s*tokens?',
        r'(\d+(?:\.\d+)?)\s*USD\s*per\s*1[kK]\s*tokens?',
        r'è¾“å…¥\s*:\s*(\$\d+\.\d+)\s*è¾“å‡º\s*:\s*(\$\d+\.\d+)',
        r'(\$\d+(?:\.\d+)?)\s*/\s*1[kK]\s*tokens?'
    ]
    
    prices = []
    for pattern in price_patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        if matches:
            prices.extend(matches)
    
    return {
        'extraction_method': 'regex',
        'price_matches': prices,
        'sample_text': text[:500]  # ä¿ç•™æ ·æœ¬ç”¨äºéªŒè¯
    }

# ä½¿ç”¨ç¤ºä¾‹
text_content = "ä»æ‰€æœ‰æ­¥éª¤æ”¶é›†çš„æ–‡æœ¬..."
price_info = extract_price_info(text_content)
print(json.dumps(price_info, indent=2))
```

### 2. æŠ€æœ¯å‚æ•°æå–å™¨
```python
import re

def extract_tech_specs(text):
    """æå–æŠ€æœ¯å‚æ•°"""
    specs = {}
    
    # å‚æ•°æ•°é‡
    param_match = re.search(r'(\d+(?:\.\d+)?)\s*ä¸‡äº¿?\s*å‚æ•°', text)
    if param_match:
        specs['parameter_count'] = param_match.group(1) + 'ä¸‡äº¿'
    
    # ä¸Šä¸‹æ–‡é•¿åº¦
    context_match = re.search(r'(\d+(?:,\d+)?[kK]?)\s*tokens?\s*ä¸Šä¸‹æ–‡', text)
    if context_match:
        specs['context_length'] = context_match.group(1)
    
    # MMLU åˆ†æ•°
    mmlu_match = re.search(r'MMLU\s*[:ï¼š]?\s*(\d+(?:\.\d+)?)', text)
    if mmlu_match:
        specs['mmlu_score'] = float(mmlu_match.group(1))
    
    return specs

# ä½¿ç”¨ç¤ºä¾‹
text_content = "æŸæ¨¡å‹å…·æœ‰3.5ä¸‡äº¿å‚æ•°ï¼Œæ”¯æŒ128K tokensä¸Šä¸‹æ–‡é•¿åº¦ï¼ŒMMLUåˆ†æ•°ä¸º85.2"
tech_specs = extract_tech_specs(text_content)
print(json.dumps(tech_specs, ensure_ascii=False, indent=2))
```

### 3. è§„æ ¼æå–å™¨ï¼ˆå‡½æ•°å¼ç‰ˆæœ¬ï¼‰
```python
import re

def extract_dimensions(text: str) -> dict:
    """äº§å“è§„æ ¼ä¿¡æ¯æå– - å‡½æ•°å¼ç‰ˆæœ¬"""
    dimensions = {}
    
    # æå–å°ºå¯¸ä¿¡æ¯
    patterns = {
        "height": [r'(\d+(?:\.\d+)?)\s*(cm|mm|m)\s*é«˜', r'é«˜åº¦[:ï¼š]\s*(\d+)'],
        "width": [r'(\d+(?:\.\d+)?)\s*(cm|mm|m)\s*å®½', r'å®½åº¦[:ï¼š]\s*(\d+)'],
        "weight": [r'(\d+(?:\.\d+)?)\s*(kg|g)\s*é‡', r'é‡é‡[:ï¼š]\s*(\d+)']
    }
    
    for dim, pattern_list in patterns.items():
        for pattern in pattern_list:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                # å¤„ç†åŒ¹é…ç»„
                value = match.group(1)
                unit = match.group(2) if len(match.groups()) > 1 else ""
                dimensions[dim] = f"{value}{unit}"
                break
    
    return dimensions

# å¢å¼ºç‰ˆï¼šæ”¯æŒæ›´å¤šè§„æ ¼ç±»å‹
def extract_all_specs(text: str) -> dict:
    """æå–æ‰€æœ‰è§„æ ¼å‚æ•°"""
    specs = {}
    
    # æè´¨æå–
    material_match = re.search(r'æè´¨[:ï¼š]\s*([^\nï¼Œã€‚]+)', text)
    if material_match:
        specs['material'] = material_match.group(1)
    
    # é¢œè‰²æå–
    color_match = re.search(r'é¢œè‰²[:ï¼š]\s*([^\nï¼Œã€‚]+)', text)
    if color_match:
        specs['color'] = color_match.group(1)
    
    # å°ºå¯¸ç»„åˆ
    dimensions = extract_dimensions(text)
    if dimensions:
        specs['dimensions'] = dimensions
    
    # å‹å·æå–
    model_match = re.search(r'å‹å·[:ï¼š]\s*([A-Za-z0-9\-_]+)', text)
    if model_match:
        specs['model'] = model_match.group(1)
    
    return specs

# ä½¿ç”¨ç¤ºä¾‹
text_content = "äº§å“å°ºå¯¸ï¼šé«˜åº¦45mmï¼Œå®½åº¦30cmï¼Œé‡é‡2.5kgï¼Œæè´¨ï¼šçš®é©"
specs = extract_all_specs(text_content)
print(json.dumps(specs, ensure_ascii=False, indent=2))
```

### 4. å…³é”®è¯åˆ†æå™¨ï¼ˆå‡½æ•°å¼ç‰ˆæœ¬ï¼‰
```python
def categorize_content(text: str) -> list:
    """åŸºäºå…³é”®è¯çš„åˆ†ç±»åˆ†æ - å‡½æ•°å¼ç‰ˆæœ¬"""
    CATEGORY_KEYWORDS = {
        "å¥¢ä¾ˆå“": ["å¥¢ä¾ˆ", "é«˜ç«¯", "premium", "luxury", "designer"],
        "ç”µå­äº§å“": ["ç”µå­", "æ™ºèƒ½", "tech", "digital", "gadget"],
        "æœè£…é‹å±¥": ["æœè£…", "é‹", "wear", "apparel", "footwear"],
        "å®¶å±…ç”¨å“": ["å®¶å±…", "å®¶å…·", "home", "furniture", "decor"]
    }
    
    text_lower = text.lower()
    categories = []
    
    for category, keywords in CATEGORY_KEYWORDS.items():
        if any(keyword.lower() in text_lower for keyword in keywords):
            categories.append(category)
    
    return categories if categories else ["æœªåˆ†ç±»"]

# å¢å¼ºç‰ˆï¼šå¸¦ç½®ä¿¡åº¦çš„åˆ†ç±»
def categorize_with_confidence(text: str) -> dict:
    """å¸¦ç½®ä¿¡åº¦çš„å†…å®¹åˆ†ç±»"""
    CATEGORY_KEYWORDS = {
        "å¥¢ä¾ˆå“": ["å¥¢ä¾ˆ", "é«˜ç«¯", "premium", "luxury", "designer", "è±ªå", "å°Šäº«"],
        "ç”µå­äº§å“": ["ç”µå­", "æ™ºèƒ½", "tech", "digital", "gadget", "æ‰‹æœº", "ç”µè„‘", "æ•°ç "],
        "æœè£…é‹å±¥": ["æœè£…", "é‹", "wear", "apparel", "footwear", "æœé¥°", "ç©¿æˆ´"],
        "å®¶å±…ç”¨å“": ["å®¶å±…", "å®¶å…·", "home", "furniture", "decor", "å®¶ç”¨", "æ‘†è®¾"],
        "ç¾å¦†æŠ¤è‚¤": ["ç¾å¦†", "æŠ¤è‚¤", "åŒ–å¦†å“", "ç¾å®¹", "skincare", "makeup"]
    }
    
    text_lower = text.lower()
    scores = {}
    
    for category, keywords in CATEGORY_KEYWORDS.items():
        score = sum(1 for keyword in keywords if keyword.lower() in text_lower)
        if score > 0:
            scores[category] = min(score / 5, 1.0)  # å½’ä¸€åŒ–åˆ°0-1
    
    if scores:
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        sorted_categories = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        return {
            "primary_category": sorted_categories[0][0],
            "confidence": round(sorted_categories[0][1], 2),
            "all_categories": {cat: round(conf, 2) for cat, conf in sorted_categories[:3]}
        }
    else:
        return {"primary_category": "æœªåˆ†ç±»", "confidence": 0.0, "all_categories": {}}

# ä½¿ç”¨ç¤ºä¾‹
text_content = "è¿™æ¬¾å¥¢ä¾ˆå“æ‰‹è¡¨é‡‡ç”¨é«˜ç«¯è®¾è®¡ï¼Œé€‚åˆå•†åŠ¡åœºåˆ"
categorization = categorize_with_confidence(text_content)
print(json.dumps(categorization, ensure_ascii=False, indent=2))
```

### 5. HTMLç»“æ„åŒ–æå–å™¨ï¼ˆå‡½æ•°å¼ç‰ˆæœ¬ï¼‰
```python
def extract_html_title_and_links(html_content: str) -> dict:
    """
    æå–HTMLé¡µé¢æ ‡é¢˜å’Œé“¾æ¥ - å‡½æ•°å¼ç‰ˆæœ¬
    æ³¨æ„ï¼šæ²™ç›’ç¯å¢ƒä¸­å¯èƒ½æ²¡æœ‰BeautifulSoupï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼
    """
    # ä½¿ç”¨æ­£åˆ™æå–æ ‡é¢˜
    title_match = re.search(r'<title[^>]*>(.*?)</title>', html_content, re.IGNORECASE | re.DOTALL)
    title = title_match.group(1).strip() if title_match else "æ— æ ‡é¢˜"
    
    # ä½¿ç”¨æ­£åˆ™æå–é“¾æ¥
    links = []
    link_pattern = r'<a[^>]*href="([^"]*)"[^>]*>(.*?)</a>'
    
    for match in re.finditer(link_pattern, html_content, re.IGNORECASE | re.DOTALL):
        href = match.group(1)
        text = re.sub(r'<[^>]+>', '', match.group(2)).strip()  # ç§»é™¤HTMLæ ‡ç­¾
        
        if href and (href.startswith('http://') or href.startswith('https://') or href.startswith('/')):
            links.append({
                "text": text[:50],  # é™åˆ¶æ–‡æœ¬é•¿åº¦
                "href": href[:200]  # é™åˆ¶URLé•¿åº¦
            })
        
        if len(links) >= 10:  # æœ€å¤šæå–10ä¸ªé“¾æ¥
            break
    
    return {
        "title": title,
        "links": links,
        "total_links_found": len(links)
    }

def extract_simple_table_data(html_content: str) -> list:
    """
    ç®€å•æå–HTMLè¡¨æ ¼æ•°æ® - å‡½æ•°å¼ç‰ˆæœ¬
    ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼Œä¸ä¾èµ–å¤–éƒ¨åº“
    """
    tables = []
    
    # æŸ¥æ‰¾æ‰€æœ‰<table>æ ‡ç­¾
    table_pattern = r'<table[^>]*>(.*?)</table>'
    
    for table_match in re.finditer(table_pattern, html_content, re.IGNORECASE | re.DOTALL):
        table_html = table_match.group(1)
        rows = []
        
        # æå–è¡Œ
        row_pattern = r'<tr[^>]*>(.*?)</tr>'
        for row_match in re.finditer(row_pattern, table_html, re.IGNORECASE | re.DOTALL):
            row_html = row_match.group(1)
            cells = []
            
            # æå–å•å…ƒæ ¼
            cell_pattern = r'<t[dh][^>]*>(.*?)</t[dh]>'
            for cell_match in re.finditer(cell_pattern, row_html, re.IGNORECASE | re.DOTALL):
                cell_content = re.sub(r'<[^>]+>', '', cell_match.group(1)).strip()
                cells.append(cell_content)
            
            if cells:  # åªæ·»åŠ éç©ºè¡Œ
                rows.append(cells)
        
        if rows:  # åªæ·»åŠ æœ‰æ•°æ®çš„è¡¨æ ¼
            tables.append({
                "row_count": len(rows),
                "col_count": len(rows[0]) if rows else 0,
                "data": rows[:20]  # é™åˆ¶è¡Œæ•°
            })
    
    return tables

# ä½¿ç”¨ç¤ºä¾‹
html_content = """
<html>
<head><title>ç¤ºä¾‹é¡µé¢</title></head>
<body>
    <h1>äº§å“åˆ—è¡¨</h1>
    <a href="/products/1">äº§å“1</a>
    <a href="/products/2">äº§å“2</a>
    <table>
        <tr><th>åç§°</th><th>ä»·æ ¼</th></tr>
        <tr><td>äº§å“A</td><td>$100</td></tr>
    </table>
</body>
</html>
"""

title_links = extract_html_title_and_links(html_content)
tables = extract_simple_table_data(html_content)

print("æ ‡é¢˜å’Œé“¾æ¥:", json.dumps(title_links, ensure_ascii=False, indent=2))
print("\nè¡¨æ ¼æ•°æ®:", json.dumps(tables, ensure_ascii=False, indent=2))
```

---

## ğŸ¯ AIä½¿ç”¨æŒ‡å—

### æ­¥éª¤ä¸€ï¼šè¯†åˆ«åˆ†æéœ€æ±‚
å½“ç”¨æˆ·è¯·æ±‚åˆ†ææ–‡æœ¬æ—¶ï¼ŒAIåº”ï¼š
1. ç¡®è®¤æ–‡æœ¬å†…å®¹æ˜¯å¦å·²æä¾›
2. è¯†åˆ«åˆ†æç›®æ ‡ï¼ˆä»·æ ¼ã€è§„æ ¼ã€åˆ†ç±»ç­‰ï¼‰
3. é€‰æ‹©åˆé€‚çš„æå–å™¨ç»„åˆ
4. **é¿å…ä½¿ç”¨ç±»å®šä¹‰ï¼Œä½¿ç”¨å‡½æ•°å¼ç¼–ç¨‹**

### æ­¥éª¤äºŒï¼šç”Ÿæˆæ‰§è¡Œä»£ç 
```python
def generate_analysis_code_for_ai(user_text: str, analysis_type: str) -> str:
    """
    AIè°ƒç”¨æ­¤å‡½æ•°ç”Ÿæˆå¯æ‰§è¡Œçš„æ²™ç›’ä»£ç 
    æ³¨æ„ï¼šè¿™æ˜¯ç»™AIçœ‹çš„æ¨¡æ¿ï¼Œä¸æ˜¯ç›´æ¥åœ¨æ²™ç›’ä¸­æ‰§è¡Œçš„ä»£ç 
    """
    # ç¤ºä¾‹ä»£ç æ¨¡æ¿
    code_template = f'''
import json
import re
from datetime import datetime

# ç”¨æˆ·æä¾›çš„åˆ†ææ–‡æœ¬
TEXT_TO_ANALYZE = """{user_text}"""

def analyze_content(text):
    """åˆ†æå‡½æ•° - å‡½æ•°å¼ç‰ˆæœ¬"""
    result = {{
        "type": "analysis_report",
        "title": "{analysis_type}åˆ†æç»“æœ",
        "timestamp": datetime.now().isoformat(),
        "data": {{}}
    }}
    
    # ä»·æ ¼æå–
    price_match = re.search(r'\\$\\s*(\\d+[,\\d]*\\.?\\d*)', text)
    if price_match:
        result["data"]["price_usd"] = price_match.group(1)
    
    # è§„æ ¼æå–
    dimensions = {{
        "height": re.search(r'(\\d+(?:\\.\\d+)?)\\s*(cm|mm|m)\\s*é«˜', text, re.IGNORECASE),
        "width": re.search(r'(\\d+(?:\\.\\d+)?)\\s*(cm|mm|m)\\s*å®½', text, re.IGNORECASE)
    }}
    
    for key, match in dimensions.items():
        if match:
            result["data"][key] = match.group(1) + (match.group(2) if match.group(2) else "")
    
    return result

# æ‰§è¡Œåˆ†æ
analysis_result = analyze_content(TEXT_TO_ANALYZE)

# ğŸ”¥ å¿…é¡»ï¼šä»¥JSONæ ¼å¼è¾“å‡º
print(json.dumps(analysis_result, ensure_ascii=False, indent=2))
'''
    return code_template
```

### æ­¥éª¤ä¸‰ï¼šå¤„ç†è¿”å›ç»“æœ
AIæ”¶åˆ°æ²™ç›’æ‰§è¡Œç»“æœåï¼š
1. éªŒè¯è¾“å‡ºæ ¼å¼æ˜¯å¦æ­£ç¡®
2. æå–å…³é”®ä¿¡æ¯å‘ˆç°ç»™ç”¨æˆ·
3. æä¾›è¿›ä¸€æ­¥åˆ†æå»ºè®®

---

## ğŸ”§ æ•…éšœæ’é™¤ä¸æœ€ä½³å®è·µ

### å¸¸è§é—®é¢˜è§£å†³æ–¹æ¡ˆ

| é—®é¢˜ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| æ— è¾“å‡º | ä»£ç æœªæ‰§è¡Œprint | ç¡®ä¿æœ€åä¸€è¡Œæ˜¯print(json.dumps(...)) |
| æ ¼å¼é”™è¯¯ | éJSONè¾“å‡º | ä½¿ç”¨json.dumps()è€Œéstr() |
| æå–ä¸ºç©º | æ–‡æœ¬æ ¼å¼ä¸åŒ¹é… | æ·»åŠ æ›´çµæ´»çš„æ­£åˆ™è¡¨è¾¾å¼ |
| ç¼–ç é—®é¢˜ | ä¸­æ–‡å­—ç¬¦ä¹±ç  | ä½¿ç”¨ensure_ascii=Falseå‚æ•° |
| ç±»å®šä¹‰é”™è¯¯ | æ²™ç›’ä¸æ”¯æŒç±» | ä½¿ç”¨å‡½æ•°å¼ç¼–ç¨‹æ›¿ä»£ |

### ä¼˜åŒ–å»ºè®®
1. **å¢é‡æå–**ï¼šå…ˆå°è¯•ç®€å•è§„åˆ™ï¼Œå†é€æ­¥å¤æ‚åŒ–
2. **é”™è¯¯æ¢å¤**ï¼šæå–å¤±è´¥æ—¶æä¾›é»˜è®¤å€¼è€Œéä¸­æ–­
3. **æ€§èƒ½ä¼˜åŒ–**ï¼šé™åˆ¶æ­£åˆ™è¡¨è¾¾å¼å¤æ‚åº¦
4. **ç»“æœéªŒè¯**ï¼šæ£€æŸ¥æå–ç»“æœçš„åˆç†æ€§
5. **å‡½æ•°å¼ä¼˜å…ˆ**ï¼šé¿å…ç±»å®šä¹‰ï¼Œä½¿ç”¨çº¯å‡½æ•°

---

## ğŸ“‹ å®Œæ•´å·¥ä½œæµç¤ºä¾‹

```python
# ===================== å®Œæ•´åˆ†æå·¥ä½œæµï¼ˆå‡½æ•°å¼ç‰ˆæœ¬ï¼‰=====================
import json
import re
from datetime import datetime

def complete_analysis_workflow(data_context: str) -> str:
    """
    ç«¯åˆ°ç«¯çš„æ–‡æœ¬åˆ†æå·¥ä½œæµ - å‡½æ•°å¼ç‰ˆæœ¬
    è¾“å…¥ï¼šçˆ¬è™«è·å–çš„æ–‡æœ¬æ•°æ®
    è¾“å‡ºï¼šæ ‡å‡†åŒ–çš„åˆ†ææŠ¥å‘Š
    """
    
    # 1. å¹¶è¡Œæå–å„ç±»ä¿¡æ¯ï¼ˆä½¿ç”¨å‡½æ•°è€Œéç±»ï¼‰
    price_info = extract_price_info(data_context)
    dimensions = extract_dimensions(data_context)
    categories = categorize_with_confidence(data_context)
    
    # 2. æ„å»ºç»“æœ
    report = {
        "type": "comprehensive_analysis",
        "title": "ç»¼åˆæ–‡æœ¬åˆ†ææŠ¥å‘Š",
        "data": {
            "ä»·æ ¼ä¿¡æ¯": price_info,
            "è§„æ ¼å‚æ•°": dimensions,
            "å†…å®¹åˆ†ç±»": categories,
            "æ–‡æœ¬é•¿åº¦": len(data_context),
            "å…³é”®å¥å­": extract_key_sentences(data_context)
        },
        "metadata": {
            "åˆ†æå·¥å…·": "æ²™ç›’å†…ç½®åˆ†æå¥—ä»¶",
            "åˆ†ææ—¶é—´": datetime.now().isoformat(),
            "ç½®ä¿¡åº¦": calculate_confidence(price_info, dimensions)
        }
    }
    
    return json.dumps(report, ensure_ascii=False, indent=2)

# è¾…åŠ©å‡½æ•°
def extract_key_sentences(text: str, max_sentences: int = 3) -> list:
    """æå–å…³é”®å¥å­"""
    # ç®€å•åˆ†å¥é€»è¾‘
    sentences = []
    current = ""
    
    for char in text:
        current += char
        if char in 'ã€‚ï¼ï¼Ÿ.!?':
            sentence = current.strip()
            if len(sentence) > 10:
                sentences.append(sentence)
            current = ""
        
        if len(sentences) >= max_sentences:
            break
    
    # å¦‚æœæ²¡æ‰¾åˆ°è¶³å¤Ÿå¥å­ï¼ŒæŒ‰æ¢è¡Œåˆ†å‰²
    if len(sentences) < max_sentences:
        lines = [line.strip() for line in text.split('\n') if len(line.strip()) > 10]
        sentences.extend(lines[:max_sentences - len(sentences)])
    
    return sentences[:max_sentences]

def calculate_confidence(price_info: dict, dimensions: dict) -> str:
    """è®¡ç®—åˆ†æç½®ä¿¡åº¦"""
    price_matches = price_info.get('price_matches', [])
    has_dimensions = bool(dimensions)
    
    if price_matches and has_dimensions:
        return "é«˜"
    elif price_matches or has_dimensions:
        return "ä¸­"
    else:
        return "ä½"

# ä¸»æ‰§è¡Œé€»è¾‘
if __name__ == "__main__":
    # ç¤ºä¾‹æ–‡æœ¬
    sample_text = """
    äº§å“ï¼šé«˜ç«¯æ™ºèƒ½æ‰‹è¡¨
    ä»·æ ¼ï¼š$299.99
    å°ºå¯¸ï¼šé«˜åº¦45mmï¼Œå®½åº¦38mm
    æè´¨ï¼šä¸é”ˆé’¢è¡¨å£³ï¼Œè“å®çŸ³ç»ç’ƒ
    åŠŸèƒ½ï¼šå¿ƒç‡ç›‘æµ‹ï¼ŒGPSå®šä½
    """
    
    result = complete_analysis_workflow(sample_text)
    print(result)
```

---

## âœ… éªŒè¯æµ‹è¯•

è¿è¡Œä»¥ä¸‹ä»£ç éªŒè¯æ‚¨çš„åˆ†æå™¨ï¼š

```python
# æµ‹è¯•ç”¨ä¾‹ - å‡½æ•°å¼ç‰ˆæœ¬
import json

test_cases = [
    ("Jimmy Choo DIDI 45 ä»·æ ¼ $299.99 æè´¨çš®é© é«˜åº¦45mm", "äº§å“é¡µé¢åˆ†æ"),
    ("iPhone 15 Pro Max å”®ä»· Â¥9999 é‡é‡ 221g å®½åº¦78mm", "ç”µå­äº§å“åˆ†æ"),
    ("å®æœ¨é¤æ¡Œ å°ºå¯¸ 180x90cm ä»·æ ¼ â‚¬459 é«˜åº¦75cm", "å®¶å±…äº§å“åˆ†æ")
]

for test_text, expected_type in test_cases:
    # ä½¿ç”¨å‡½æ•°å¼åˆ†æå™¨
    dimensions = extract_dimensions(test_text)
    categories = categorize_content(test_text)
    
    result = {
        "type": "test_result",
        "test_case": expected_type,
        "dimensions": dimensions,
        "categories": categories,
        "has_price": "$" in test_text or "Â¥" in test_text or "â‚¬" in test_text
    }
    
    print(f"æµ‹è¯•: {expected_type}")
    print(f"ç»“æœ: {json.dumps(result, ensure_ascii=False, indent=2)}")
    print("-" * 50)
```

---

## ğŸ“Œ æ€»ç»“è¦ç‚¹

1. **å®‰å…¨ç¬¬ä¸€**ï¼šæ‰€æœ‰ä»£ç åœ¨æ²™ç›’ä¸­è¿è¡Œï¼Œæ— ç½‘ç»œæ— æ–‡ä»¶é£é™©
2. **æ ¼å¼ä¸ºç‹**ï¼šè¾“å‡ºå¿…é¡»ç¬¦åˆæ ‡å‡†JSONç»“æ„ï¼ŒåŒ…å«typeå­—æ®µ
3. **å‡½æ•°å¼ä¼˜å…ˆ**ï¼šé¿å…ç±»å®šä¹‰ï¼Œä½¿ç”¨çº¯å‡½æ•°è¿›è¡Œæ•°æ®æå–
4. **æ¸è¿›æå–**ï¼šä»ç®€å•è§„åˆ™å¼€å§‹ï¼Œé€æ­¥å¢åŠ å¤æ‚æ€§
5. **é”™è¯¯å¤„ç†**ï¼šæå–å¤±è´¥æ—¶æä¾›åˆç†é»˜è®¤å€¼
6. **æ€§èƒ½æ„è¯†**ï¼šé¿å…å¤æ‚æ­£åˆ™å’Œæ— é™å¾ªç¯

## ğŸ”„ ä»ç±»åˆ°å‡½æ•°çš„è½¬æ¢æŒ‡å—

| åŸç±»å®šä¹‰ | è½¬æ¢åçš„å‡½æ•° | ä½¿ç”¨æ–¹å¼ |
|---------|------------|---------|
| `class Extractor:`<br>`def extract(self, text):` | `def extract_data(text):` | `result = extract_data(text)` |
| `obj = Extractor()`<br>`obj.extract(text)` | ç›´æ¥è°ƒç”¨å‡½æ•° | `extract_data(text)` |
| ç±»å±æ€§ï¼ˆ`self.config`ï¼‰ | å‡½æ•°å‚æ•°æˆ–å…¨å±€å¸¸é‡ | `def func(text, config={})` |
| å¤šä¸ªç›¸å…³æ–¹æ³• | å¤šä¸ªç‹¬ç«‹å‡½æ•°æˆ–ä¸»å‡½æ•°è°ƒç”¨å­å‡½æ•° | `def main_func():`<br>`data1 = func1()`<br>`data2 = func2()` |

## ğŸ¯ æœ€ç»ˆæ£€æŸ¥æ¸…å•

åœ¨ç”Ÿæˆæ²™ç›’ä»£ç å‰ï¼Œè¯·ç¡®è®¤ï¼š
- [ ] æ²¡æœ‰`class`å…³é”®å­—
- [ ] æ‰€æœ‰åŠŸèƒ½éƒ½æ˜¯å‡½æ•°
- [ ] è¾“å‡ºåŒ…å«`type`å­—æ®µ
- [ ] ä½¿ç”¨`json.dumps()`è¾“å‡º
- [ ] æ²¡æœ‰ç½‘ç»œè¯·æ±‚æˆ–æ–‡ä»¶ç³»ç»Ÿè®¿é—®
- [ ] æ­£åˆ™è¡¨è¾¾å¼æœ‰é™åˆ¶ï¼ˆé¿å…ReDoSï¼‰

---
