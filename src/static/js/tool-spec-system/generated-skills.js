/**
 * @file è‡ªåŠ¨ç”Ÿæˆçš„æŠ€èƒ½æ³¨å†Œè¡¨ - ç”± build-skills.js è„šæœ¬ç”Ÿæˆ
 * !!! è¯·å‹¿ç›´æ¥ç¼–è¾‘æ­¤æ–‡ä»¶ !!!
 * ç”Ÿæˆæ—¶é—´: 2025-12-25T09:04:36.446Z
 * æŠ€èƒ½æ•°é‡: 6
 */

export const SKILLS_DATA = {
  "crawl4ai": {
    "metadata": {
      "name": "crawl4ai",
      "description": "åŠŸèƒ½å¼ºå¤§çš„å¼€æºç½‘é¡µæŠ“å–å’Œæ•°æ®å¤„ç†å·¥å…·ï¼Œæ”¯æŒ6ç§å·¥ä½œæ¨¡å¼ï¼ŒåŒ…æ‹¬æˆªå›¾ã€PDFå¯¼å‡ºå’Œæ™ºèƒ½çˆ¬å–",
      "tool_name": "crawl4ai",
      "category": "web-crawling",
      "priority": 9,
      "tags": [
        "web-scraping",
        "screenshot",
        "pdf-export",
        "structured-data-extraction",
        "deep-crawling",
        "batch-processing",
        "content-extraction",
        "anti-detection",
        "automation"
      ],
      "version": 1.2
    },
    "content": "# Crawl4AI ç½‘é¡µæŠ“å–å·¥å…·æŒ‡å—\r\n\r\nCrawl4AI æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„å¼€æºç½‘é¡µæŠ“å–å’Œæ•°æ®å¤„ç†å·¥å…·ï¼Œæ”¯æŒ 6 ç§ä¸åŒçš„å·¥ä½œæ¨¡å¼ã€‚æ‰€æœ‰äºŒè¿›åˆ¶è¾“å‡ºï¼ˆæˆªå›¾ã€PDFï¼‰éƒ½ä»¥ base64 ç¼–ç è¿”å›ï¼Œä¾¿äºæ¨¡å‹å¤„ç†ã€‚\r\n\r\n## ğŸ¯ ã€è‡³å…³é‡è¦ã€‘é€šç”¨è°ƒç”¨ç»“æ„\r\n\r\n**æ‰€æœ‰å¯¹ `crawl4ai` çš„è°ƒç”¨éƒ½å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹åµŒå¥—ç»“æ„ï¼** è¿™æ˜¯ä¸€ä¸ªé€šç”¨è§„åˆ™ï¼Œé€‚ç”¨äºæ‰€æœ‰æ¨¡å¼ã€‚\r\n\r\n```json\r\n{\r\n  \"mode\": \"<æ¨¡å¼åç§°>\",\r\n  \"parameters\": {\r\n    \"param1\": \"value1\",\r\n    \"param2\": \"value2\"\r\n    // ...å…·ä½“æ¨¡å¼çš„æ‰€æœ‰å‚æ•°éƒ½æ”¾åœ¨è¿™é‡Œ\r\n  }\r\n}\r\n```\r\n\r\n### âŒ å¸¸è§è‡´å‘½é”™è¯¯ï¼šæœªåµŒå¥—å‚æ•°\r\n\r\n```json\r\n// è¿™æ˜¯ä¸€ä¸ªç»å¯¹ä¼šå¯¼è‡´å¤±è´¥çš„é”™è¯¯è°ƒç”¨ï¼\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"url\": \"https://example.com\" // é”™è¯¯ï¼'url' å¿…é¡»åœ¨ 'parameters' å†…éƒ¨\r\n}\r\n```\r\n\r\n### âœ… æ­£ç¡®çš„åŸºç¡€è°ƒç”¨æ¨¡å¼\r\n\r\n```json\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\"\r\n  }\r\n}\r\n```\r\n\r\n## ğŸš€ æ–°å¢æ™ºèƒ½åˆ†çº§ç³»ç»Ÿ\r\n\r\n**ç‰ˆæœ¬ 1.2 é‡è¦æ›´æ–°ï¼š** æ–°å¢æ™ºèƒ½åˆ†çº§æŠ“å–ç³»ç»Ÿï¼Œè‡ªåŠ¨é€‚åº”ä¸åŒç½‘ç«™ç±»å‹ï¼š\r\n\r\n| é…ç½®ç­‰çº§ | é€‚ç”¨åœºæ™¯ | ç‰¹ç‚¹ | æ€§èƒ½è¡¨ç° |\r\n|---------|---------|------|----------|\r\n| **æ ‡å‡†é…ç½®** | æ™®é€šé™æ€ç½‘ç«™ | é«˜æ€§èƒ½ï¼Œå¿«é€ŸæŠ“å– | âš¡ å¿«é€Ÿ (90ç§’è¶…æ—¶) |\r\n| **å¢å¼ºé…ç½®** | JSç½‘ç«™ã€åçˆ¬ç½‘ç«™ | åŠ å¼ºåçˆ¬ï¼Œå®Œæ•´æ¸²æŸ“ | ğŸ›¡ï¸ ç¨³å¥ (120ç§’è¶…æ—¶) |\r\n| **é™çº§é…ç½®** | æç«¯å¤æ‚ç½‘ç«™ | æœ€å¤§åŒ–å…¼å®¹æ€§ | ğŸ¢ ä¿å®ˆ (180ç§’è¶…æ—¶) |\r\n\r\n**æ™ºèƒ½è¯†åˆ«èƒ½åŠ›ï¼š**\r\n- âœ… è‡ªåŠ¨æ£€æµ‹ JavaScript é©±åŠ¨ç½‘ç«™ï¼ˆReactã€Vueã€Angularç­‰ï¼‰\r\n- âœ… è¯†åˆ«åçˆ¬æœºåˆ¶ï¼ˆCloudflareã€reCAPTCHAç­‰ï¼‰\r\n- âœ… ç¼“å­˜æˆåŠŸé…ç½®ï¼ŒåŠ é€Ÿåç»­æŠ“å–\r\n- âœ… å¤±è´¥è‡ªåŠ¨é™çº§ï¼Œæé«˜æˆåŠŸç‡\r\n\r\n## ğŸ“‹ å¯ç”¨æ¨¡å¼å¿«é€Ÿé€‰æ‹©æŒ‡å—\r\n\r\n| æ¨¡å¼ | åŠŸèƒ½æè¿° | ä¸»è¦ç”¨é€” | å¤æ‚åº¦ | æ¨èåœºæ™¯ |\r\n|------|----------|----------|---------|----------|\r\n| `scrape` | æŠ“å–å•ä¸ªç½‘é¡µ | è·å–é¡µé¢å†…å®¹ã€æˆªå›¾ã€PDF | â­â­ | å•é¡µé¢å†…å®¹è·å– |\r\n| `deep_crawl` | æ·±åº¦æ™ºèƒ½çˆ¬å– | ä½¿ç”¨ç­–ç•¥æ·±åº¦çˆ¬å–ç½‘ç«™ | â­â­â­â­ | ç½‘ç«™å†…å®¹æ¢ç´¢ |\r\n| `batch_crawl` | æ‰¹é‡ URL å¤„ç† | åŒæ—¶å¤„ç†å¤šä¸ª URL | â­â­ | æ‰¹é‡æ•°æ®æ”¶é›† |\r\n| `extract` | ç»“æ„åŒ–æ•°æ®æå– | åŸºäº CSS æˆ– LLM æå–æ•°æ® | â­â­â­ | ç‰¹å®šæ•°æ®æå– |\r\n| `pdf_export` | PDF å¯¼å‡º | å°†ç½‘é¡µå¯¼å‡ºä¸º PDF | â­ | æ–‡æ¡£ä¿å­˜ |\r\n| `screenshot` | æˆªå›¾æ•è· | æ•è·ç½‘é¡µæˆªå›¾ | â­ | è§†è§‰è¯æ®ä¿å­˜ |\r\n\r\n**æ³¨æ„ï¼š** `crawl` æ¨¡å¼å·²åœ¨æœ€æ–°ç‰ˆæœ¬ä¸­ç§»é™¤ï¼Œè¯·ä½¿ç”¨ `scrape` æˆ– `deep_crawl` æ›¿ä»£ã€‚\r\n\r\n## ğŸ¯ ä½¿ç”¨åœºæ™¯å¿«é€ŸæŒ‡å—\r\n\r\n### åœºæ™¯1ï¼šå¿«é€Ÿè·å–é¡µé¢å†…å®¹\r\n```json\r\n{\r\n  \"mode\": \"scrape\", \r\n  \"parameters\": {\r\n    \"url\": \"https://example.com/article\",\r\n    \"format\": \"markdown\",\r\n    \"word_count_threshold\": 10,\r\n    \"include_links\": true,\r\n    \"include_images\": true\r\n  }\r\n}\r\n```\r\n\r\n### åœºæ™¯2ï¼šæ‰¹é‡æ”¶é›†äº§å“ä¿¡æ¯\r\n```json\r\n{\r\n  \"mode\": \"batch_crawl\",\r\n  \"parameters\": {\r\n    \"urls\": [\r\n      \"https://example.com/product1\",\r\n      \"https://example.com/product2\",\r\n      \"https://example.com/product3\"\r\n    ],\r\n    \"concurrent_limit\": 4\r\n  }\r\n}\r\n```\r\n\r\n### åœºæ™¯3ï¼šæ·±åº¦ç ”ç©¶æŸä¸ªç½‘ç«™\r\n```json\r\n{\r\n  \"mode\": \"deep_crawl\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com/docs\",\r\n    \"max_depth\": 3,\r\n    \"max_pages\": 80,\r\n    \"keywords\": [\"æ•™ç¨‹\", \"æŒ‡å—\", \"API\"],\r\n    \"strategy\": \"best_first\"\r\n  }\r\n}\r\n```\r\n\r\n### åœºæ™¯4ï¼šæå–ç»“æ„åŒ–æ•°æ®\r\n```json\r\n{\r\n  \"mode\": \"extract\",\r\n  \"parameters\": {\r\n    \"url\": \"https://news.example.com/article\",\r\n    \"schema_definition\": {\r\n      \"name\": \"Article\",\r\n      \"baseSelector\": \".article-content\",\r\n      \"fields\": [\r\n        {\r\n          \"name\": \"title\",\r\n          \"selector\": \"h1\",\r\n          \"type\": \"text\"\r\n        },\r\n        {\r\n          \"name\": \"author\",\r\n          \"selector\": \".author\",\r\n          \"type\": \"text\"\r\n        },\r\n        {\r\n          \"name\": \"content\",\r\n          \"selector\": \".content\",\r\n          \"type\": \"text\"\r\n        }\r\n      ]\r\n    },\r\n    \"extraction_type\": \"css\"\r\n  }\r\n}\r\n```\r\n\r\n### åœºæ™¯5ï¼šä¿å­˜ç½‘é¡µè¯æ®\r\n```json\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\",\r\n    \"return_screenshot\": true,\r\n    \"return_pdf\": true,\r\n    \"screenshot_quality\": 90,\r\n    \"screenshot_max_width\": 1200\r\n  }\r\n}\r\n```\r\n\r\n## ğŸš€ è¯¦ç»†æ¨¡å¼è¯´æ˜\r\n\r\n### 1. æŠ“å–å•ä¸ªç½‘é¡µ (`scrape`)\r\n\r\n**æ™ºèƒ½åˆ†çº§æŠ“å–**ï¼šæ–°ç‰ˆå·¥å…·è‡ªåŠ¨æ ¹æ®ç½‘ç«™ç±»å‹é€‰æ‹©æœ€ä½³é…ç½®æ–¹æ¡ˆã€‚\r\n\r\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\r\n```json\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\",\r\n    \"format\": \"markdown\",\r\n    \"css_selector\": \".article-content\",\r\n    \"include_links\": true,\r\n    \"include_images\": true,\r\n    \"return_screenshot\": true,\r\n    \"return_pdf\": false,\r\n    \"screenshot_quality\": 80,\r\n    \"screenshot_max_width\": 1200,\r\n    \"word_count_threshold\": 10,\r\n    \"exclude_external_links\": true\r\n  }\r\n}\r\n```\r\n\r\n**âŒ é”™è¯¯ç¤ºä¾‹ï¼ˆå‚æ•°æœªåµŒå¥—ï¼‰:**\r\n```json\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"url\": \"https://example.com\", // é”™è¯¯ï¼ç¼ºå°‘parametersåŒ…è£…\r\n  \"format\": \"markdown\"\r\n}\r\n```\r\n\r\n**æ™ºèƒ½åˆ†çº§æŠ“å–åŸç†ï¼š**\r\n1. **è‡ªåŠ¨è¯†åˆ«**ï¼šå·¥å…·ä¼šåˆ†æURLç‰¹å¾ï¼Œåˆ¤æ–­ç½‘ç«™ç±»å‹\r\n2. **åˆ†çº§é…ç½®**ï¼š\r\n   - **æ ‡å‡†é…ç½®**ï¼šæ™®é€šç½‘ç«™ï¼Œ90ç§’è¶…æ—¶ï¼Œé«˜æ€§èƒ½\r\n   - **å¢å¼ºé…ç½®**ï¼šJSç½‘ç«™/åçˆ¬ç½‘ç«™ï¼Œ120ç§’è¶…æ—¶ï¼Œå®Œæ•´æ¸²æŸ“\r\n   - **é™çº§é…ç½®**ï¼šæç«¯å¤æ‚ç½‘ç«™ï¼Œ180ç§’è¶…æ—¶ï¼Œæœ€å¤§åŒ–å…¼å®¹\r\n3. **å¤±è´¥é™çº§**ï¼šå¦‚æœå½“å‰é…ç½®å¤±è´¥ï¼Œè‡ªåŠ¨å°è¯•æ›´å…¼å®¹çš„é…ç½®\r\n4. **é…ç½®ç¼“å­˜**ï¼šæˆåŠŸé…ç½®ä¼šç¼“å­˜ï¼Œåç»­åŒåŸŸåè¯·æ±‚æ›´å¿«\r\n\r\n**å‚æ•°è¯´æ˜:**\r\n- `url` (å¿…éœ€): è¦æŠ“å–çš„ç½‘é¡µ URLï¼Œå¿…é¡»ä»¥ `http://` æˆ– `https://` å¼€å¤´\r\n- `format`: è¾“å‡ºæ ¼å¼ï¼Œ`markdown`(é»˜è®¤)/`html`/`text`\r\n- `css_selector`: æå–ç‰¹å®šå†…å®¹çš„ CSS é€‰æ‹©å™¨\r\n- `include_links`: æ˜¯å¦åœ¨è¾“å‡ºä¸­åŒ…å«é“¾æ¥ï¼Œé»˜è®¤ true\r\n- `include_images`: æ˜¯å¦åœ¨è¾“å‡ºä¸­åŒ…å«å›¾ç‰‡ï¼Œé»˜è®¤ true\r\n- `return_screenshot`: æ˜¯å¦è¿”å›æˆªå›¾(base64)ï¼Œé»˜è®¤ false\r\n- `return_pdf`: æ˜¯å¦è¿”å› PDF(base64)ï¼Œé»˜è®¤ false\r\n- `screenshot_quality`: æˆªå›¾è´¨é‡(10-100)ï¼Œé»˜è®¤ 70\r\n- `screenshot_max_width`: æˆªå›¾æœ€å¤§å®½åº¦ï¼Œé»˜è®¤ 1920\r\n- `word_count_threshold`: å†…å®¹å—æœ€å°å•è¯æ•°ï¼Œé»˜è®¤ 10\r\n- `exclude_external_links`: æ˜¯å¦æ’é™¤å¤–éƒ¨é“¾æ¥ï¼Œé»˜è®¤ true\r\n\r\n### 2. æ·±åº¦ç½‘ç«™çˆ¬å– (`deep_crawl`)\r\n\r\nä½¿ç”¨æ™ºèƒ½ç­–ç•¥æ·±åº¦çˆ¬å–æ•´ä¸ªç½‘ç«™ï¼Œæ”¯æŒå…³é”®è¯è¯„åˆ†å’Œ URL è¿‡æ»¤ã€‚\r\n\r\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\r\n```json\r\n{\r\n  \"mode\": \"deep_crawl\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\",\r\n    \"max_depth\": 3,\r\n    \"max_pages\": 80,\r\n    \"strategy\": \"best_first\",\r\n    \"include_external\": false,\r\n    \"keywords\": [\"äº§å“\", \"ä»·æ ¼\", \"è§„æ ¼\"],\r\n    \"url_patterns\": [\"/products/\", \"/docs/\"],\r\n    \"stream\": false\r\n  }\r\n}\r\n```\r\n\r\n**å¢å¼ºåŠŸèƒ½ï¼š**\r\n- âœ… **Contexté”™è¯¯è‡ªåŠ¨æ¢å¤**ï¼šæ•è·å¹¶å¤„ç†Crawl4AIå†…éƒ¨é”™è¯¯\r\n- âœ… **æ™ºèƒ½è¶…æ—¶å¤„ç†**ï¼š400ç§’è¶…æ—¶ä¿æŠ¤ï¼Œè¿”å›éƒ¨åˆ†ç»“æœ\r\n- âœ… **æµå¼å¿ƒè·³**ï¼šå®šæœŸè¿›åº¦æ›´æ–°ï¼Œé¿å…Cloudflare 524é”™è¯¯\r\n\r\n**å‚æ•°è¯´æ˜:**\r\n- `url` (å¿…éœ€): èµ·å§‹ URL\r\n- `max_depth`: æœ€å¤§çˆ¬å–æ·±åº¦ï¼Œé»˜è®¤ 3\r\n- `max_pages`: æœ€å¤§é¡µé¢æ•°ï¼Œé»˜è®¤ 80\r\n- `strategy`: çˆ¬å–ç­–ç•¥ï¼Œ`bfs`(é»˜è®¤)/`dfs`/`best_first`\r\n- `include_external`: æ˜¯å¦è·Ÿè¸ªå¤–éƒ¨é“¾æ¥ï¼Œé»˜è®¤ false\r\n- `keywords`: ç”¨äºç›¸å…³æ€§è¯„åˆ†çš„å…³é”®è¯åˆ—è¡¨\r\n- `url_patterns`: URL æ¨¡å¼è¿‡æ»¤åˆ—è¡¨\r\n- `stream`: æ˜¯å¦æµå¼è¿”å›ç»“æœï¼Œé»˜è®¤ false\r\n\r\n### 3. æ‰¹é‡ URL å¤„ç† (`batch_crawl`)\r\n\r\nåŒæ—¶å¤„ç†å¤šä¸ª URLï¼Œé€‚ç”¨äºæ‰¹é‡æ•°æ®æ”¶é›†ã€‚\r\n\r\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\r\n```json\r\n{\r\n  \"mode\": \"batch_crawl\",\r\n  \"parameters\": {\r\n    \"urls\": [\r\n      \"https://example.com/page1\",\r\n      \"https://example.com/page2\",\r\n      \"https://example.com/page3\"\r\n    ],\r\n    \"stream\": false,\r\n    \"concurrent_limit\": 4\r\n  }\r\n}\r\n```\r\n\r\n**âŒ é”™è¯¯ç¤ºä¾‹ï¼ˆurlsä¸æ˜¯æ•°ç»„ï¼‰:**\r\n```json\r\n{\r\n  \"mode\": \"batch_crawl\",\r\n  \"parameters\": {\r\n    \"urls\": \"https://example.com/page1\" // é”™è¯¯ï¼urlså¿…é¡»æ˜¯æ•°ç»„\r\n  }\r\n}\r\n```\r\n\r\n**å®‰å…¨é™åˆ¶ï¼š**\r\n- âš¡ å¹¶å‘æ•°ç¡¬æ€§ä¸Šé™ï¼š4ï¼ˆæ— è®ºå‚æ•°è®¾ç½®å¤šé«˜ï¼‰\r\n- â° æ•´ä½“è¶…æ—¶ï¼š300ç§’ï¼ˆ5åˆ†é’Ÿï¼‰\r\n- ğŸ“Š æœ€å¤§é¡µé¢æ•°ï¼š20ï¼ˆé˜²æ­¢è¿‡åº¦è¯·æ±‚ï¼‰\r\n\r\n**å‚æ•°è¯´æ˜:**\r\n- `urls` (å¿…éœ€): URL åˆ—è¡¨ï¼Œå¿…é¡»æ˜¯æ•°ç»„æ ¼å¼\r\n- `stream`: æ˜¯å¦æµå¼è¿”å›ï¼Œé»˜è®¤ false\r\n- `concurrent_limit`: æœ€å¤§å¹¶å‘æ•°ï¼Œé»˜è®¤ 4 (å·²æ ¹æ®æœåŠ¡å™¨å†…å­˜å‡çº§è°ƒæ•´ï¼Œå¹¶è®¾ç½®äº†ç¡¬æ€§ä¸Šé™)\r\n\r\n### 4. ç»“æ„åŒ–æ•°æ®æå– (`extract`)\r\n\r\nä»ç½‘é¡µä¸­æå–ç»“æ„åŒ–æ•°æ®ï¼Œ**ä¸»è¦ä¾èµ–ç²¾ç¡®çš„ CSS é€‰æ‹©å™¨**ã€‚\r\n\r\n**âœ… æ­£ç¡®ç¤ºä¾‹ (CSS æå–):**\r\n```json\r\n{\r\n  \"mode\": \"extract\",\r\n  \"parameters\": {\r\n    \"url\": \"https://news.example.com/article\",\r\n    \"schema_definition\": {\r\n      \"name\": \"Article\",\r\n      \"baseSelector\": \".article-content\",\r\n      \"fields\": [\r\n        {\r\n          \"name\": \"title\",\r\n          \"selector\": \"h1\",\r\n          \"type\": \"text\"\r\n        },\r\n        {\r\n          \"name\": \"author\",\r\n          \"selector\": \".author\",\r\n          \"type\": \"text\"\r\n        },\r\n        {\r\n          \"name\": \"publish_date\",\r\n          \"selector\": \".date\",\r\n          \"type\": \"text\"\r\n        },\r\n        {\r\n          \"name\": \"content\",\r\n          \"selector\": \".content\",\r\n          \"type\": \"text\"\r\n        }\r\n      ]\r\n    },\r\n    \"css_selector\": \".article-content\",\r\n    \"extraction_type\": \"css\"\r\n  }\r\n}\r\n```\r\n\r\n**âš ï¸ å…³é”®é™åˆ¶ä¸æœ€ä½³å®è·µï¼š**\r\n- **AI æ™ºèƒ½æå–é™åˆ¶**: `crawl4ai` çš„ `extraction_type: \"llm\"` æ¨¡å¼**å°šæœªéƒ¨ç½²**æœ‰æ•ˆçš„ LLM å®ä¾‹ã€‚\r\n- **æ³¨æ„äº‹é¡¹åšæ³•**: **`crawl4ai` æå–åœºæ™¯**: ä»…é€‚ç”¨äºæ‚¨èƒ½æä¾›**ç²¾ç¡® CSS é€‰æ‹©å™¨**çš„ç®€å•ã€ç»“æ„å›ºå®šçš„é¡µé¢ã€‚æ— æ³•å®ç°**ä»…å‡­è‡ªç„¶è¯­è¨€æè¿°å’Œ JSON Schema**ï¼ˆå³ä¸æä¾› CSS é€‰æ‹©å™¨ï¼‰ä»å¤æ‚é¡µé¢ï¼ˆå¦‚è¡¨æ ¼ã€åˆ—è¡¨ï¼‰ä¸­æ™ºèƒ½æå–æ•°æ®ã€‚\r\n\r\n**ğŸ›¡ï¸ è‡ªåŠ¨ä¿®å¤æœºåˆ¶ï¼š**\r\næˆ‘ä»¬çš„å·¥å…·ä¼šè‡ªåŠ¨ä¿®å¤å¸¸è§çš„ schema æ ¼å¼é—®é¢˜ï¼š\r\n- å¦‚æœç¼ºå°‘ `baseSelector`ï¼Œè‡ªåŠ¨è®¾ç½®ä¸º `css_selector` æˆ– `'body'`\r\n- å¦‚æœç¼ºå°‘ `fields`ï¼Œè‡ªåŠ¨åˆ›å»ºé»˜è®¤å­—æ®µé…ç½®  \r\n- å¦‚æœç¼ºå°‘ `name`ï¼Œè‡ªåŠ¨è®¾ç½®ä¸º `\"ExtractedData\"`\r\n- å¦‚æœ schema æ˜¯æ•°ç»„æ ¼å¼ï¼Œè‡ªåŠ¨è½¬æ¢ä¸ºå¯¹è±¡æ ¼å¼\r\n\r\n**ğŸ’¡ æœ€ä½³å®è·µï¼š** è™½ç„¶å·¥å…·ä¼šè‡ªåŠ¨ä¿®å¤ï¼Œä½†æä¾›å®Œæ•´çš„ schema å¯ä»¥è·å¾—æ›´ç²¾ç¡®çš„æå–ç»“æœã€‚\r\n\r\n**å‚æ•°è¯´æ˜:**\r\n- `url` (å¿…éœ€): è¦æå–çš„ç½‘é¡µ URL\r\n- `schema_definition` (å¿…éœ€): å®šä¹‰è¾“å‡ºç»“æ„çš„ JSON schema\r\n- `css_selector`: åŸºç¡€ CSS é€‰æ‹©å™¨ï¼ˆCSS æå–æ—¶ä½¿ç”¨ï¼‰\r\n- `extraction_type`: æå–ç±»å‹ï¼Œ`css`(é»˜è®¤)/`llm`\r\n- `prompt`: LLM æå–çš„æç¤ºè¯­\r\n\r\n## ğŸ“‹ Schema Definition ç»“æ„è¯´æ˜\r\n\r\n### CSS æå–æ¨¡å¼å¿…éœ€çš„ schema ç»“æ„ï¼š\r\n```json\r\n{\r\n  \"name\": \"YourSchemaName\",           // å¿…éœ€ï¼šschema åç§°\r\n  \"baseSelector\": \"css-selector\",     // å¿…éœ€ï¼šåŸºç¡€ CSS é€‰æ‹©å™¨\r\n  \"fields\": [                         // å¿…éœ€ï¼šå­—æ®µå®šä¹‰æ•°ç»„\r\n    {\r\n      \"name\": \"field_name\",           // å¿…éœ€ï¼šå­—æ®µåç§°\r\n      \"selector\": \"css-selector\",     // å¿…éœ€ï¼šå­—æ®µé€‰æ‹©å™¨\r\n      \"type\": \"text\",                 // å¿…éœ€ï¼šå­—æ®µç±»å‹\r\n      \"multiple\": true                // å¯é€‰ï¼šæ˜¯å¦å…è®¸å¤šä¸ªå€¼\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n### LLM æå–æ¨¡å¼ schema ç»“æ„ï¼š\r\n```json\r\n{\r\n  \"type\": \"object\",\r\n  \"properties\": {\r\n    \"field1\": {\"type\": \"string\"},\r\n    \"field2\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\r\n  }\r\n}\r\n```\r\n\r\n### 5. PDF å¯¼å‡º (`pdf_export`)\r\n\r\nå°†ç½‘é¡µå¯¼å‡ºä¸º PDF æ ¼å¼ã€‚\r\n\r\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\r\n```json\r\n{\r\n  \"mode\": \"pdf_export\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com/document\",\r\n    \"return_as_base64\": true\r\n  }\r\n}\r\n```\r\n\r\n**å‚æ•°è¯´æ˜:**\r\n- `url` (å¿…éœ€): è¦å¯¼å‡ºä¸º PDF çš„ç½‘é¡µ URL\r\n- `return_as_base64`: æ˜¯å¦è¿”å› base64 ç¼–ç ï¼Œé»˜è®¤ true\r\n\r\n### 6. æˆªå›¾æ•è· (`screenshot`)\r\n\r\næ•è·ç½‘é¡µæˆªå›¾ï¼Œæ”¯æŒè´¨é‡å‹ç¼©å’Œå°ºå¯¸è°ƒæ•´ã€‚\r\n\r\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\r\n```json\r\n{\r\n  \"mode\": \"screenshot\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\",\r\n    \"full_page\": true,\r\n    \"return_as_base64\": true,\r\n    \"quality\": 80,\r\n    \"max_width\": 1200,\r\n    \"max_height\": 3000\r\n  }\r\n}\r\n```\r\n\r\n**å‚æ•°è¯´æ˜:**\r\n- `url` (å¿…éœ€): è¦æˆªå›¾çš„ç½‘é¡µ URL\r\n- `full_page`: æ˜¯å¦æˆªå–æ•´ä¸ªé¡µé¢ï¼Œé»˜è®¤ true\r\n- `return_as_base64`: æ˜¯å¦è¿”å› base64 ç¼–ç ï¼Œé»˜è®¤ true\r\n- `quality`: æˆªå›¾è´¨é‡(10-100)ï¼Œé»˜è®¤ 70\r\n- `max_width`: æœ€å¤§å®½åº¦ï¼Œé»˜è®¤ 1920\r\n- `max_height`: æœ€å¤§é«˜åº¦ï¼Œé»˜è®¤ 5000\r\n\r\n## ğŸ›¡ï¸ å¢å¼ºçš„åçˆ¬èƒ½åŠ›\r\n\r\n**ç‰ˆæœ¬ 1.2 æ–°å¢åçˆ¬ç‰¹æ€§ï¼š**\r\n\r\n### æµè§ˆå™¨æŒ‡çº¹æ··æ·†\r\n- âœ… éšæœº User-Agent è½®æ¢\r\n- âœ… éšæœºè§†å£å°ºå¯¸ (1280-1920 Ã— 720-1080)\r\n- âœ… éšèº«æ¨¡å¼ï¼ˆIncognitoï¼‰å‡å°‘æŒ‡çº¹è¿½è¸ª\r\n\r\n### é«˜çº§æµè§ˆå™¨å‚æ•°\r\n- âœ… ç¦ç”¨è‡ªåŠ¨åŒ–æ§åˆ¶ç‰¹å¾ (`--disable-blink-features=AutomationControlled`)\r\n- âœ… ç¦ç”¨ Web å®‰å…¨å’Œç«™ç‚¹éš”ç¦»\r\n- âœ… ç¦ç”¨æ‰©å±•å’Œé€šçŸ¥\r\n- âœ… ç¦ç”¨ IP æ³›æ´ªä¿æŠ¤\r\n\r\n### æ™ºèƒ½ç­‰å¾…ç­–ç•¥\r\n- âœ… æ ‡å‡†é…ç½®ï¼šçŸ­ç­‰å¾… (1.0ç§’)ï¼Œé«˜æ€§èƒ½\r\n- âœ… å¢å¼ºé…ç½®ï¼šä¸­ç­‰ç­‰å¾… (2.0ç§’)ï¼Œå®Œæ•´æ¸²æŸ“\r\n- âœ… é™çº§é…ç½®ï¼šé•¿ç­‰å¾… (3.0ç§’)ï¼Œæœ€å¤§åŒ–å…¼å®¹\r\n\r\n## ğŸ”„ å¸¸è§å·¥ä½œæµ\r\n\r\n### æ–°é—»æ–‡ç« é‡‡é›†å·¥ä½œæµ\r\n**ç›®æ ‡**: è‡ªåŠ¨æ”¶é›†å’Œåˆ†ææ–°é—»å†…å®¹\r\n1. **å‘ç°é˜¶æ®µ**: ä½¿ç”¨ `deep_crawl` å‘ç°ç›¸å…³æ–‡ç« é“¾æ¥\r\n2. **é‡‡é›†é˜¶æ®µ**: ä½¿ç”¨ `batch_crawl` æ‰¹é‡è·å–å†…å®¹  \r\n3. **æå–é˜¶æ®µ**: ä½¿ç”¨ `extract` ç»“æ„åŒ–æå–å…³é”®ä¿¡æ¯\r\n\r\n### ç«å“åˆ†æå·¥ä½œæµ\r\n**ç›®æ ‡**: ç³»ç»ŸåŒ–åˆ†æç«äº‰å¯¹æ‰‹ç½‘ç«™\r\n1. **è¯æ®æ”¶é›†**: ä½¿ç”¨ `screenshot` æ•è·ç«å“é¡µé¢\r\n2. **å†…å®¹åˆ†æ**: ä½¿ç”¨ `scrape` è·å–è¯¦ç»†å†…å®¹\r\n3. **æ–‡æ¡£ä¿å­˜**: ä½¿ç”¨ `pdf_export` ä¿å­˜è¯æ®\r\n\r\n### äº§å“ç›®å½•çˆ¬å–å·¥ä½œæµ  \r\n**ç›®æ ‡**: å»ºç«‹å®Œæ•´çš„äº§å“æ•°æ®åº“\r\n1. **ç›®å½•æ¢ç´¢**: ä½¿ç”¨ `deep_crawl` å‘ç°æ‰€æœ‰äº§å“é¡µé¢\r\n2. **æ•°æ®æå–**: ä½¿ç”¨ `extract` æå–äº§å“ä¿¡æ¯\r\n\r\n## ğŸ› ï¸ æ•…éšœæ’é™¤\r\n\r\n### å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ\r\n\r\n#### æ€§èƒ½é—®é¢˜\r\n- **è¶…æ—¶é—®é¢˜**: å·¥å…·å·²å†…ç½®æ™ºèƒ½è¶…æ—¶ï¼Œæ ‡å‡†é…ç½®90ç§’ï¼Œå¢å¼ºé…ç½®120ç§’ï¼Œé™çº§é…ç½®180ç§’\r\n- **å†…å­˜é—®é¢˜**: å¯ç”¨ `stream: true`ï¼Œå‡å°‘æ‰¹é‡å¤„ç†çš„ URL æ•°é‡ã€‚åç«¯å·²è®¾ç½®å†…å­˜ä¸Šé™ï¼ˆ6000MBï¼‰ï¼Œé«˜å¹¶å‘å¯èƒ½è§¦å‘æµè§ˆå™¨é‡å¯\r\n\r\n#### å†…å®¹è´¨é‡é—®é¢˜  \r\n- **å†…å®¹ç¼ºå¤±**: è°ƒæ•´ `word_count_threshold`ï¼Œæ£€æŸ¥ `css_selector`\r\n- **æˆªå›¾ä¸å®Œæ•´**: å¢åŠ  `max_height` å€¼ï¼Œç¡®ä¿ `full_page: true`\r\n\r\n#### ç½‘ç»œé—®é¢˜\r\n- **è¿æ¥å¤±è´¥**: æ£€æŸ¥ URL æ ¼å¼ï¼ŒéªŒè¯ç½‘ç»œè¿æ¥\r\n- **è¢«ç½‘ç«™å±è”½**: æ™ºèƒ½åˆ†çº§ç³»ç»Ÿä¼šè‡ªåŠ¨é™çº§é…ç½®ï¼Œæé«˜æˆåŠŸç‡\r\n\r\n#### Extract æ¨¡å¼ç‰¹å®šé—®é¢˜\r\n- **ç©ºç»“æœ**: æ£€æŸ¥ `fields` æ•°ç»„ä¸­çš„ `selector` æ˜¯å¦å‡†ç¡®åŒ¹é…é¡µé¢å…ƒç´ \r\n- **å­—æ®µç¼ºå¤±**: ç¡®ä¿ `schema_definition` åŒ…å«å®Œæ•´çš„ `name`ã€`baseSelector`ã€`fields` ç»“æ„\r\n- **è‡ªåŠ¨ä¿®å¤**: å·¥å…·ä¼šè‡ªåŠ¨è¡¥å…¨ç¼ºå¤±å­—æ®µï¼Œä½†æ‰‹åŠ¨æä¾›å®Œæ•´ schema æ•ˆæœæ›´å¥½\r\n\r\n#### æ™ºèƒ½åˆ†çº§ç³»ç»Ÿé—®é¢˜\r\n- **é…ç½®é€‰æ‹©ä¸å½“**: å·¥å…·ä¼šè‡ªåŠ¨å­¦ä¹ ï¼Œå¤±è´¥æ¬¡æ•°å¤šçš„URLä¼šè‡ªåŠ¨ä½¿ç”¨æ›´å…¼å®¹çš„é…ç½®\r\n- **ç¼“å­˜é—®é¢˜**: é…ç½®ç¼“å­˜æŒ‰åŸŸåå­˜å‚¨ï¼Œæ¸…é™¤æµè§ˆå™¨å®ä¾‹ä¼šé‡ç½®ç¼“å­˜\r\n\r\n### è°ƒè¯•æŠ€å·§\r\n\r\n1. **ä»ç®€å•å¼€å§‹**: å…ˆç”¨ `scrape` æ¨¡å¼æµ‹è¯•å•ä¸ªé¡µé¢\r\n2. **é€æ­¥å¢åŠ å¤æ‚åº¦**: ç¡®è®¤åŸºç¡€åŠŸèƒ½æ­£å¸¸åå†ä½¿ç”¨é«˜çº§æ¨¡å¼  \r\n3. **æ£€æŸ¥å‚æ•°**: ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½æ­£ç¡®åµŒå¥—åœ¨ `parameters` å¯¹è±¡å†…\r\n4. **éªŒè¯è¾“å‡º**: å…ˆæµ‹è¯•å°è§„æ¨¡æ•°æ®ï¼Œç¡®è®¤è¾“å‡ºæ ¼å¼ç¬¦åˆé¢„æœŸ\r\n\r\n## âš ï¸ é‡è¦æç¤º\r\n\r\n### âœ… æ­£ç¡®åšæ³•\r\n- **å‚æ•°åµŒå¥—**: æ‰€æœ‰å‚æ•°å¿…é¡»æ”¾åœ¨ `parameters` å¯¹è±¡å†…\r\n- **URL æ ¼å¼**: å¿…é¡»ä»¥ `http://` æˆ– `https://` å¼€å¤´  \r\n- **æ¨¡å¼é€‰æ‹©**: æ ¹æ®éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ¨¡å¼\r\n- **å†…å­˜ç®¡ç†**: å¤§é‡æ•°æ®æ—¶ä½¿ç”¨æµå¼å¤„ç† (`stream: true`)\r\n- **Schema å®Œæ•´æ€§**: ä¸º CSS æå–æä¾›å®Œæ•´çš„ `name`ã€`baseSelector`ã€`fields` ç»“æ„\r\n\r\n### âŒ å¸¸è§é”™è¯¯\r\n\r\n**é”™è¯¯ 1: ç¼ºå°‘åµŒå¥—å‚æ•°**\r\n```json\r\n// âŒ é”™è¯¯\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"url\": \"https://example.com\"\r\n}\r\n\r\n// âœ… æ­£ç¡®\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\"\r\n  }\r\n}\r\n```\r\n\r\n**é”™è¯¯ 2: URL ç¼ºå°‘åè®®**\r\n```json\r\n// âŒ é”™è¯¯\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"parameters\": {\r\n    \"url\": \"example.com\"\r\n  }\r\n}\r\n\r\n// âœ… æ­£ç¡®\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\"\r\n  }\r\n}\r\n```\r\n\r\n**é”™è¯¯ 3: é”™è¯¯çš„å‚æ•°ç±»å‹**\r\n```json\r\n// âŒ é”™è¯¯ - urls åº”è¯¥æ˜¯æ•°ç»„\r\n{\r\n  \"mode\": \"batch_crawl\",\r\n  \"parameters\": {\r\n    \"urls\": \"https://example.com\"\r\n  }\r\n}\r\n\r\n// âœ… æ­£ç¡®\r\n{\r\n  \"mode\": \"batch_crawl\",\r\n  \"parameters\": {\r\n    \"urls\": [\"https://example.com\"]\r\n  }\r\n}\r\n```\r\n\r\n**é”™è¯¯ 4: extractæ¨¡å¼ä½¿ç”¨é”™è¯¯çš„å‚æ•°å**\r\n```json\r\n// âŒ é”™è¯¯ - åº”è¯¥ä½¿ç”¨ schema_definition\r\n{\r\n  \"mode\": \"extract\", \r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\",\r\n    \"schema\": { // é”™è¯¯ï¼åº”è¯¥æ˜¯ schema_definition\r\n      \"title\": \"string\"\r\n    }\r\n  }\r\n}\r\n\r\n// âœ… æ­£ç¡®\r\n{\r\n  \"mode\": \"extract\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\", \r\n    \"schema_definition\": {\r\n      \"name\": \"Article\",\r\n      \"baseSelector\": \".content\",\r\n      \"fields\": [\r\n        {\r\n          \"name\": \"title\",\r\n          \"selector\": \"h1\",\r\n          \"type\": \"text\"\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n## ğŸª é«˜çº§ä½¿ç”¨æŠ€å·§\r\n\r\n### 1. ç»„åˆä½¿ç”¨åª’ä½“æ•è·\r\n```json\r\n{\r\n  \"mode\": \"scrape\",\r\n  \"parameters\": {\r\n    \"url\": \"https://example.com\",\r\n    \"include_links\": true,\r\n    \"include_images\": true,\r\n    \"return_screenshot\": true,\r\n    \"return_pdf\": true,\r\n    \"screenshot_quality\": 90,\r\n    \"screenshot_max_width\": 1200,\r\n    \"word_count_threshold\": 15\r\n  }\r\n}\r\n```\r\n\r\n### 2. æ™ºèƒ½æ·±åº¦çˆ¬å–\r\n```json\r\n{\r\n  \"mode\": \"deep_crawl\",\r\n  \"parameters\": {\r\n    \"url\": \"https://docs.example.com\",\r\n    \"strategy\": \"best_first\",\r\n    \"keywords\": [\"API\", \"æ•™ç¨‹\", \"ç¤ºä¾‹\"],\r\n    \"max_depth\": 3,\r\n    \"max_pages\": 80\r\n  }\r\n}\r\n```\r\n\r\n### 3. æ‰¹é‡å¤„ç†é‡è¦é¡µé¢\r\n```json\r\n{\r\n  \"mode\": \"batch_crawl\",\r\n  \"parameters\": {\r\n    \"urls\": [\r\n      \"https://example.com/home\",\r\n      \"https://example.com/about\", \r\n      \"https://example.com/contact\",\r\n      \"https://example.com/products\"\r\n    ],\r\n    \"concurrent_limit\": 4\r\n  }\r\n}\r\n```\r\n\r\n### 4. æ™ºèƒ½å†…å®¹æå–\r\n```json\r\n{\r\n  \"mode\": \"extract\",\r\n  \"parameters\": {\r\n    \"url\": \"https://news.example.com/article\",\r\n    \"schema_definition\": {\r\n      \"name\": \"NewsArticle\",\r\n      \"baseSelector\": \".article-container\",\r\n      \"fields\": [\r\n        {\r\n          \"name\": \"headline\",\r\n          \"selector\": \"h1.news-title\",\r\n          \"type\": \"text\"\r\n        },\r\n        {\r\n          \"name\": \"author\",\r\n          \"selector\": \".author-name\",\r\n          \"type\": \"text\"\r\n        },\r\n        {\r\n          \"name\": \"publish_date\",\r\n          \"selector\": \".publish-date\",\r\n          \"type\": \"text\"\r\n        },\r\n        {\r\n          \"name\": \"main_content\",\r\n          \"selector\": \".article-body\",\r\n          \"type\": \"text\"\r\n        },\r\n        {\r\n          \"name\": \"tags\",\r\n          \"selector\": \".tag\",\r\n          \"type\": \"text\",\r\n          \"multiple\": true\r\n        }\r\n      ]\r\n    },\r\n    \"extraction_type\": \"css\"\r\n  }\r\n}\r\n```\r\n\r\n## ğŸ“ æœ€ä½³å®è·µæ€»ç»“\r\n\r\n1. **é€‰æ‹©åˆé€‚çš„æ¨¡å¼**: æ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©æœ€ç®€å•æœ‰æ•ˆçš„æ¨¡å¼\r\n2. **æ¸è¿›å¼æµ‹è¯•**: ä»å°è§„æ¨¡å¼€å§‹æµ‹è¯•ï¼Œé€æ­¥æ‰©å¤§èŒƒå›´  \r\n3. **èµ„æºç®¡ç†**: æ³¨æ„å¹¶å‘æ•°å’Œå†…å­˜ä½¿ç”¨ï¼Œé¿å…è¿‡åº¦è¯·æ±‚\r\n4. **é”™è¯¯å¤„ç†**: å‡†å¤‡å¥½å¤„ç†ç½‘ç»œé”™è¯¯å’Œå†…å®¹è§£æå¤±è´¥çš„æƒ…å†µ\r\n5. **åˆæ³•ä½¿ç”¨**: éµå®ˆç½‘ç«™çš„ robots.txt å’ŒæœåŠ¡æ¡æ¬¾\r\n6. **æ ¼å¼æ£€æŸ¥**: æ¯æ¬¡è°ƒç”¨å‰ç¡®è®¤å‚æ•°æ­£ç¡®åµŒå¥—åœ¨ `parameters` å¯¹è±¡å†…\r\n7. **å‚æ•°éªŒè¯**: ç¡®ä¿ URL åŒ…å«åè®®ï¼Œæ•°ç»„å‚æ•°æ­£ç¡®æ ¼å¼\r\n8. **å‘½åè§„èŒƒ**: extractæ¨¡å¼å¿…é¡»ä½¿ç”¨ `schema_definition` å‚æ•°å\r\n9. **å†…å®¹æ§åˆ¶**: ä½¿ç”¨ `include_links` å’Œ `include_images` æ§åˆ¶è¾“å‡ºå†…å®¹\r\n10. **è´¨é‡ä¼˜åŒ–**: ä½¿ç”¨ `word_count_threshold` è¿‡æ»¤ä½è´¨é‡å†…å®¹å—\r\n11. **Schema å®Œæ•´æ€§**: ä¸º CSS æå–æä¾›å®Œæ•´çš„ schema ç»“æ„ä»¥è·å¾—æœ€ä½³ç»“æœ\r\n12. **ä¿¡ä»»æ™ºèƒ½åˆ†çº§**: æ— éœ€æ‰‹åŠ¨è°ƒæ•´é…ç½®ï¼Œå·¥å…·ä¼šè‡ªåŠ¨é€‰æ‹©æœ€ä½³æ–¹æ¡ˆ\r\n13. **æ³¨æ„æ¨¡å¼å˜æ›´**: å·²ç§»é™¤ `crawl` æ¨¡å¼ï¼Œè¯·ä½¿ç”¨ `scrape` æˆ– `deep_crawl` æ›¿ä»£\r\n\r\n## ğŸ”„ ç‰ˆæœ¬æ›´æ–°è¯´æ˜\r\n\r\n### ç‰ˆæœ¬ 1.2 ä¸»è¦æ›´æ–°\r\n1. **æ–°å¢æ™ºèƒ½åˆ†çº§ç³»ç»Ÿ**: è‡ªåŠ¨é€‚åº”ä¸åŒç½‘ç«™ç±»å‹çš„é…ç½®ç­–ç•¥\r\n2. **å¢å¼ºåçˆ¬èƒ½åŠ›**: æ”¹è¿›æµè§ˆå™¨æŒ‡çº¹æ··æ·†å’Œåæ£€æµ‹æœºåˆ¶\r\n3. **å†…å­˜ç®¡ç†ä¼˜åŒ–**: æå‡å†…å­˜ä½¿ç”¨æ•ˆç‡å’Œç¨³å®šæ€§\r\n4. **é”™è¯¯å¤„ç†æ”¹è¿›**: å¢å¼ºå¯¹ Context é”™è¯¯çš„æ•è·å’Œæ¢å¤\r\n5. **ç§»é™¤ `crawl` æ¨¡å¼**: ç®€åŒ–æ¨¡å¼é€‰æ‹©ï¼Œèšç„¦æ ¸å¿ƒåŠŸèƒ½\r\n6. **å‚æ•°è‡ªåŠ¨ä¿®å¤**: å¢å¼ºå¯¹è¾“å…¥å‚æ•°çš„å®¹é”™èƒ½åŠ›\r\n\r\n### å‘åå…¼å®¹æ€§\r\n- âœ… æ‰€æœ‰ç°æœ‰è°ƒç”¨æ ¼å¼ä¿æŒå…¼å®¹\r\n- âœ… æ™ºèƒ½åˆ†çº§ç³»ç»Ÿå¯¹ Agent å®Œå…¨é€æ˜\r\n- âœ… æ–°å¢åŠŸèƒ½ä¸å½±å“ç°æœ‰åŠŸèƒ½\r\n- âŒ ä¸å†æ”¯æŒ `mode: \"crawl\"`ï¼Œè¯·ä½¿ç”¨æ›¿ä»£æ¨¡å¼",
    "resources": {},
    "filePath": "D:\\Github_10110531\\gemini_chat\\src\\skills\\crawl4ai",
    "lastUpdated": "2025-12-25T09:04:36.432Z"
  },
  "firecrawl": {
    "metadata": {
      "name": "firecrawl",
      "description": "å¤šåŠŸèƒ½ç½‘é¡µæŠ“å–å’Œæ•°æ®æå–å·¥å…·ï¼Œæ”¯æŒåŒæ­¥æŠ“å–ã€æœç´¢ã€ç½‘ç«™åœ°å›¾è·å–å’Œå¼‚æ­¥çˆ¬å–",
      "tool_name": "firecrawl",
      "category": "web-crawling",
      "priority": 7,
      "tags": [
        "web-scraping",
        "data-extraction",
        "crawling",
        "automation",
        "firecrawl"
      ],
      "version": 1
    },
    "content": "# å·¥å…·è°ƒç”¨ç¤ºä¾‹ï¼ˆFirecrawlï¼‰\n\n`firecrawl` æ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½ç½‘é¡µæŠ“å–å’Œæ•°æ®æå–å·¥å…·ï¼Œé€šè¿‡ `mode` å‚æ•°è°ƒç”¨ä¸åŒåŠŸèƒ½ã€‚å…¶ `parameters` ç»“æ„æ˜¯åµŒå¥—çš„ã€‚\n\n**âœ… æ­£ç¡®çš„è°ƒç”¨ç»“æ„:**\n```json\n{\"mode\": \"<åŠŸèƒ½æ¨¡å¼>\", \"parameters\": {\"<å‚æ•°å>\": \"<å‚æ•°å€¼>\"}}\n```\n\n**ğŸ’¡ é‡è¦æç¤º:**\n- `scrape`ã€`search`ã€`map` æ˜¯åŒæ­¥æ“ä½œï¼Œç«‹å³è¿”å›ç»“æœ\n- `crawl`ã€`extract` æ˜¯å¼‚æ­¥æ“ä½œï¼Œè¿”å› `job_id` ç”¨äºåç»­çŠ¶æ€æ£€æŸ¥\n- æ‰€æœ‰å‚æ•°éƒ½å¿…é¡»åœ¨ `parameters` å¯¹è±¡å†…ï¼Œä¸è¦æ”¾åœ¨é¡¶å±‚\n- URL å¿…é¡»ä»¥ `http://` æˆ– `https://` å¼€å¤´\n\n## åŠŸèƒ½æ¨¡å¼è¯¦è§£\n\n### â¡ï¸ ç¤ºä¾‹ 1: æŠ“å–å•ä¸ªç½‘é¡µ (`scrape`)\n\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\n```json\n{\n  \"mode\": \"scrape\", \n  \"parameters\": {\n    \"url\": \"https://docs.firecrawl.dev/\",\n    \"formats\": [\"markdown\"]  // å¯é€‰ï¼š[\"markdown\", \"html\"]ï¼Œé»˜è®¤ markdown\n  }\n}\n```\n\n### â¡ï¸ ç¤ºä¾‹ 2: ç½‘é¡µæœç´¢ (`search`)\n\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\n```json\n{\n  \"mode\": \"search\", \n  \"parameters\": {\n    \"query\": \"äººå·¥æ™ºèƒ½æœ€æ–°å‘å±•\",\n    \"limit\": 5\n  }\n}\n```\n\n### â¡ï¸ ç¤ºä¾‹ 3: è·å–ç½‘ç«™åœ°å›¾ (`map`)\n\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\n```json\n{\n  \"mode\": \"map\", \n  \"parameters\": {\n    \"url\": \"https://example.com\"\n  }\n}\n```\n\n### â¡ï¸ ç¤ºä¾‹ 4: å¼‚æ­¥çˆ¬å–ç½‘ç«™ (`crawl`)\n\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\n```json\n{\n  \"mode\": \"crawl\", \n  \"parameters\": {\n    \"url\": \"https://firecrawl.dev\", \n    \"limit\": 5\n  }\n}\n```\n*æ­¤è°ƒç”¨ä¼šè¿”å›ä¸€ä¸ª `job_id`ï¼Œç”¨äºåç»­æŸ¥è¯¢ã€‚*\n\n### â¡ï¸ ç¤ºä¾‹ 5: ç»“æ„åŒ–æ•°æ®æå– (`extract`)\n\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\n```json\n{\n  \"mode\": \"extract\", \n  \"parameters\": {\n    \"urls\": [\"https://news.example.com/article\"],\n    \"prompt\": \"æå–æ–‡ç« æ ‡é¢˜ã€ä½œè€…å’Œå‘å¸ƒæ—¶é—´\",\n    \"schema\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"title\": {\"type\": \"string\"},\n        \"author\": {\"type\": \"string\"}, \n        \"publish_time\": {\"type\": \"string\"}\n      }\n    }\n  }\n}\n```\n\n### â¡ï¸ ç¤ºä¾‹ 6: æ£€æŸ¥å¼‚æ­¥ä»»åŠ¡çŠ¶æ€ (`check_status`)\n\n**âœ… æ­£ç¡®ç¤ºä¾‹:**\n```json\n{\n  \"mode\": \"check_status\", \n  \"parameters\": {\n    \"job_id\": \"some-unique-job-identifier\"\n  }\n}\n```\n\n## âŒ é”™è¯¯ç¤ºä¾‹ (è¯·é¿å…ä»¥ä¸‹å¸¸è§é”™è¯¯)\n\n- **ç¼ºå°‘ `mode` å‚æ•°:** `{\"parameters\": {\"url\": \"...\"}}`\n- **ç¼ºå°‘åµŒå¥—çš„ `parameters` å¯¹è±¡:** `{\"mode\": \"scrape\", \"url\": \"...\"}`\n- **å°†å‚æ•°æ”¾åœ¨é¡¶å±‚:** `{\"url\": \"...\"}` \n- **ä½¿ç”¨æ— æ•ˆçš„ URL æ ¼å¼:** `{\"mode\": \"scrape\", \"parameters\": {\"url\": \"example.com\"}}` (ç¼ºå°‘åè®®)\n- **é”™è¯¯çš„å‚æ•°ç±»å‹:** `{\"mode\": \"extract\", \"parameters\": {\"urls\": \"https://example.com\"}}` (urls åº”è¯¥æ˜¯æ•°ç»„)",
    "resources": {},
    "filePath": "D:\\Github_10110531\\gemini_chat\\src\\skills\\firecrawl",
    "lastUpdated": "2025-12-25T09:04:36.437Z"
  },
  "glm4v_analyze_image": {
    "metadata": {
      "name": "glm4v_analyze_image",
      "description": "æ™ºè°±AIçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œç”¨äºå›¾åƒåˆ†æã€å†…å®¹è¯†åˆ«å’Œè§†è§‰é—®ç­”",
      "tool_name": "glm4v_analyze_image",
      "category": "vision",
      "priority": 7,
      "tags": [
        "image-analysis",
        "vision",
        "recognition",
        "visual-qa",
        "multimodal"
      ],
      "version": 1
    },
    "content": "# GLM-4Vå›¾åƒåˆ†æå·¥å…·æŒ‡å—\r\n\r\n## æ ¸å¿ƒèƒ½åŠ›\r\n- å›¾åƒå†…å®¹è¯†åˆ«å’Œæè¿°\r\n- è§†è§‰é—®ç­”å’Œæ¨ç†\r\n- å›¾åƒç»†èŠ‚åˆ†æ\r\n- å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆ\r\n\r\n## è°ƒç”¨è§„èŒƒ\r\n```json\r\n{\r\n  \"tool_name\": \"glm4v_analyze_image\",\r\n  \"parameters\": {\r\n    \"model\": \"glm-4v-flash\",\r\n    \"image_url\": \"å›¾ç‰‡URL\",\r\n    \"prompt\": \"åˆ†ææç¤ºè¯­\"\r\n  }\r\n}\r\n```\r\n\r\nä»¥ä¸‹æ˜¯è°ƒç”¨ `glm4v_analyze_image` å·¥å…·çš„**æ­£ç¡®**å’Œ**é”™è¯¯**ç¤ºä¾‹ã€‚è¯·åŠ¡å¿…éµå¾ªæ­£ç¡®æ ¼å¼ã€‚\r\n\r\n## âœ… æ­£ç¡®ç¤ºä¾‹\r\n```json\r\n{\"model\": \"glm-4v-flash\", \"image_url\": \"https://path/to/image.jpg\", \"prompt\": \"Describe this image.\"}\r\n```\r\n\r\n## âŒ é”™è¯¯ç¤ºä¾‹ (è¯·é¿å…ä»¥ä¸‹å¸¸è§é”™è¯¯)\r\n\r\n- **ç¼ºå°‘å¼•å·æˆ–é€—å·:** \r\n  ```json\r\n  {\"model\": \"glm-4v-flash\", \"image_url\": \"https://path/to/image.jpg\", \"prompt\": \"Describe this image.\"}\r\n  ```\r\n  (ç¼ºå°‘ `}`)\r\n\r\n- **å‚æ•°åé”™è¯¯:** \r\n  ```json\r\n  {\"img_url\": \"https://path/to/image.jpg\"}\r\n  ```\r\n  (åº”ä¸º \"image_url\" è€Œé \"img_url\")\r\n\r\n- **æ¨¡å‹åç§°é”™è¯¯:** \r\n  ```json\r\n  {\"model\": \"glm4v-flash\", \"image_url\": \"https://path/to/image.jpg\", \"prompt\": \"Describe this image.\"}\r\n  ```\r\n  (åº”ä¸º \"glm-4v-flash\")\r\n  \r\n## å…³é”®æŒ‡ä»¤\r\n1. **æ¨¡å‹é€‰æ‹©**: ä½¿ç”¨ `glm-4v-flash` æ¨¡å‹\r\n2. **å›¾ç‰‡æ ¼å¼**: æ”¯æŒå¸¸è§å›¾ç‰‡æ ¼å¼ï¼ˆJPEG, PNG, WebPç­‰ï¼‰\r\n3. **æç¤ºè¯­è®¾è®¡**: æ¸…æ™°å…·ä½“çš„åˆ†ææŒ‡ä»¤\r\n4. **URLæœ‰æ•ˆæ€§**: ç¡®ä¿å›¾ç‰‡URLå¯å…¬å¼€è®¿é—®\r\n\r\n## ä½¿ç”¨åœºæ™¯\r\n\r\n### å›¾åƒæè¿°\r\n```json\r\n{\r\n  \"tool_name\": \"glm4v_analyze_image\",\r\n  \"parameters\": {\r\n    \"model\": \"glm-4v-flash\", \r\n    \"image_url\": \"https://example.com/image.jpg\",\r\n    \"prompt\": \"è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹\"\r\n  }\r\n}\r\n```\r\n\r\n### è§†è§‰é—®ç­”\r\n```json\r\n{\r\n  \"tool_name\": \"glm4v_analyze_image\",\r\n  \"parameters\": {\r\n    \"model\": \"glm-4v-flash\",\r\n    \"image_url\": \"https://example.com/image.jpg\", \r\n    \"prompt\": \"å›¾ç‰‡ä¸­æœ‰å¤šå°‘äººï¼Ÿä»–ä»¬åœ¨åšä»€ä¹ˆï¼Ÿ\"\r\n  }\r\n}\r\n```\r\n\r\n### ç»†èŠ‚åˆ†æ\r\n```json\r\n{\r\n  \"tool_name\": \"glm4v_analyze_image\",\r\n  \"parameters\": {\r\n    \"model\": \"glm-4v-flash\",\r\n    \"image_url\": \"https://example.com/image.jpg\",\r\n    \"prompt\": \"åˆ†æå›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹å’ŒæŠ€æœ¯ç»†èŠ‚\"\r\n  }\r\n}\r\n```\r\n\r\n## æœ€ä½³å®è·µ\r\n\r\n### æç¤ºè¯­è®¾è®¡\r\n- **å…·ä½“æ˜ç¡®**: \"æè¿°å›¾ç‰‡ä¸­äººç‰©çš„åŠ¨ä½œå’Œè¡¨æƒ…\"\r\n- **ä»»åŠ¡å¯¼å‘**: \"è¯†åˆ«å›¾ç‰‡ä¸­çš„æ‰€æœ‰ç‰©ä½“å¹¶åˆ†ç±»\"\r\n- **ç»†èŠ‚è¦æ±‚**: \"æ³¨æ„é¢œè‰²ã€å½¢çŠ¶ã€ç©ºé—´å…³ç³»ç­‰ç»†èŠ‚\"\r\n\r\n### é”™è¯¯å¤„ç†\r\n- æ£€æŸ¥å›¾ç‰‡URLæ˜¯å¦æœ‰æ•ˆ\r\n- ç¡®è®¤å›¾ç‰‡æ ¼å¼æ”¯æŒ\r\n- å¤„ç†ç½‘ç»œè¶…æ—¶æƒ…å†µ\r\n\r\n## èƒ½åŠ›èŒƒå›´\r\n- âœ… ç‰©ä½“è¯†åˆ«å’Œåˆ†ç±»\r\n- âœ… åœºæ™¯ç†è§£å’Œæè¿°  \r\n- âœ… æ–‡å­—è¯†åˆ«ï¼ˆOCRï¼‰\r\n- âœ… æƒ…æ„Ÿå’Œæ°›å›´åˆ†æ\r\n- âœ… æŠ€æœ¯ç»†èŠ‚æå–\r\n\r\n## é™åˆ¶è¯´æ˜\r\n- âŒ ä¸èƒ½å¤„ç†æ•æ„Ÿæˆ–ä¸å½“å†…å®¹\r\n- âŒ å›¾ç‰‡å¤§å°å’Œåˆ†è¾¨ç‡æœ‰é™åˆ¶\r\n- âŒ å®æ—¶è§†é¢‘æµä¸æ”¯æŒ\r\n- âŒ 3Dæ¨¡å‹åˆ†æä¸æ”¯æŒ\r\n\r\n## æ€§èƒ½ä¼˜åŒ–\r\n- ä½¿ç”¨åˆé€‚çš„å›¾ç‰‡å°ºå¯¸\r\n- æä¾›å…·ä½“çš„åˆ†æéœ€æ±‚\r\n- åˆ†æ­¥éª¤è¿›è¡Œå¤æ‚åˆ†æ\r\n- ç»“åˆå…¶ä»–å·¥å…·è¿›è¡ŒéªŒè¯",
    "resources": {},
    "filePath": "D:\\Github_10110531\\gemini_chat\\src\\skills\\glm4v_analyze_image",
    "lastUpdated": "2025-12-25T09:04:36.438Z"
  },
  "python_sandbox": {
    "metadata": {
      "name": "python_sandbox",
      "description": "åœ¨æ²™ç›’ç¯å¢ƒä¸­æ‰§è¡ŒPythonä»£ç ï¼Œç”¨äºæ•°æ®åˆ†æã€å¯è§†åŒ–å’Œç”ŸæˆExcelã€Wordã€PDFç­‰æ–‡ä»¶ã€‚æ”¯æŒæ•°æ®æ¸…æ´—ã€ç»Ÿè®¡åˆ†æã€æœºå™¨å­¦ä¹ ã€å›¾è¡¨ç”Ÿæˆã€æ–‡æ¡£è‡ªåŠ¨åŒ–ç­‰å¤æ‚å·¥ä½œæµã€‚",
      "tool_name": "python_sandbox",
      "category": "code",
      "priority": 10,
      "tags": [
        "python",
        "code",
        "visualization",
        "data-analysis",
        "chart",
        "document",
        "automation",
        "machine-learning",
        "reporting",
        "excel",
        "word",
        "pdf",
        "ppt"
      ],
      "version": 2.5,
      "references": [
        "matplotlib_cookbook.md",
        "pandas_cheatsheet.md",
        "report_generator_workflow.md",
        "ml_workflow.md",
        "sympy_cookbook.md",
        "scipy_cookbook.md",
        "text_analysis_cookbook.md"
      ]
    },
    "content": "# Pythonæ²™ç›’å·¥å…·ä½¿ç”¨æŒ‡å— v2.5 (ä¸åç«¯å®Œå…¨åŒ¹é…ç‰ˆ)\r\n\r\n## ğŸ¯ **æ ¸å¿ƒèƒ½åŠ›æ¦‚è§ˆ**\r\n\r\nPythonæ²™ç›’æ˜¯ä¸€ä¸ª**å¤šåŠŸèƒ½çš„ä»£ç æ‰§è¡Œç¯å¢ƒ**ï¼Œæ”¯æŒï¼š\r\n\r\n| åŠŸèƒ½é¢†åŸŸ | ä¸»è¦ç”¨é€” | å…³é”®åº“ |\r\n|---------|---------|-------|\r\n| **æ•°æ®åˆ†æ** | æ•°æ®æ¸…æ´—ã€è½¬æ¢ã€èšåˆ | Pandas, Polars |\r\n| **é«˜æ€§èƒ½è®¡ç®—** | å†…å­˜SQLã€è¡¨è¾¾å¼åŠ é€Ÿ | DuckDB, Numexpr, Bottleneck |\r\n| **å¯è§†åŒ–** | å›¾è¡¨ç”Ÿæˆä¸è‡ªåŠ¨æ•è· | Matplotlib, Seaborn |\r\n| **æ–‡æ¡£è‡ªåŠ¨åŒ–** | Excel/Word/PDF/PPTç”Ÿæˆ | python-docx, reportlab, openpyxl |\r\n| **æœºå™¨å­¦ä¹ ** | æ¨¡å‹è®­ç»ƒä¸è¯„ä¼° | scikit-learn, LightGBM |\r\n| **ç¬¦å·æ•°å­¦** | å…¬å¼è¯æ˜ã€æ–¹ç¨‹æ±‚è§£ | SymPy |\r\n| **ç§‘å­¦è®¡ç®—** | ä¼˜åŒ–ã€ç§¯åˆ†ã€ä¿¡å·å¤„ç† | SciPy |\r\n| **æµç¨‹å›¾ç”Ÿæˆ** | æ¶æ„å›¾ã€æµç¨‹å›¾ | Graphviz, NetworkX |\r\n| **æ–‡æœ¬åˆ†æ** | HTMLè§£æã€æ•°æ®æå– | BeautifulSoup4, lxml |\r\n| **æ€§èƒ½ä¼˜åŒ–** | æœºæ¢°ç¡¬ç›˜ä¼˜åŒ–ã€å¼‚æ­¥IO | aiofiles, joblib |\r\n\r\n\r\n---\r\n\r\n## ğŸ“ **æ–‡ä»¶å¤„ç†æŒ‡å— - ä¸¤ç§æ¨¡å¼å¿…é¡»åˆ†æ¸…**\r\n\r\n### **æ¨¡å¼A: å·¥ä½œåŒºæ–‡ä»¶ (`/data` ç›®å½•)**\r\n**ç”¨é€”**: æ•°æ®åˆ†æã€å¤„ç†ã€æŒä¹…åŒ–å­˜å‚¨  \r\n**æ”¯æŒæ ¼å¼**: `.csv`, `.xlsx`, `.xls`, `.parquet`, `.json`, `.txt`, `.feather`  \r\n**è®¿é—®æ–¹å¼**: ç»å¯¹è·¯å¾„ `/data/æ–‡ä»¶å`  \r\n```python\r\nimport pandas as pd\r\ndf = pd.read_csv('/data/sales.csv')  # âœ… æ­£ç¡®\r\n```\r\n\r\n### **æ¨¡å¼B: ä¸Šä¸‹æ–‡æ–‡ä»¶ (Base64åµŒå…¥)**\r\n**ç”¨é€”**: å›¾ç‰‡è¯†åˆ«ã€PDFå†…å®¹æå–  \r\n**æ”¯æŒæ ¼å¼**: `.png`, `.jpg`, `.jpeg`, `.pdf`, `.txt`(å°æ–‡ä»¶)  \r\n**ç‰¹ç‚¹**: æ–‡ä»¶å†…å®¹ç›´æ¥åµŒå…¥å¯¹è¯ï¼Œ**ä¸åœ¨ `/data` ç›®å½•**\r\n```python\r\n# âŒ é”™è¯¯ï¼šæ— æ³•ä»/dataè¯»å–ä¸Šä¼ çš„å›¾ç‰‡\r\n# img = Image.open('/data/uploaded_image.png')  # ä¼šå¤±è´¥\r\n```\r\n\r\n---\r\n\r\n## ğŸš€ **è¾“å‡ºè§„èŒƒ - åç«¯å®é™…æ”¯æŒçš„æ ¼å¼**\r\n\r\n### **1. å›¾è¡¨è¾“å‡º - ç³»ç»Ÿè‡ªåŠ¨æ•è·**\r\n```python\r\nimport matplotlib.pyplot as plt\r\nplt.plot([1,2,3], [4,5,6])\r\nplt.title('ç¤ºä¾‹å›¾è¡¨')\r\nplt.show()  # ğŸ¯ å…³é”®ï¼šè‡ªåŠ¨æ•è·ï¼Œæ— éœ€æ‰‹åŠ¨å¤„ç†\r\n\r\n# æ”¯æŒä»¥ä¸‹å›¾è¡¨åº“çš„è‡ªåŠ¨æ•è·ï¼š\r\n# - Matplotlib (ä½¿ç”¨ plt.show() è§¦å‘)\r\n# - Graphviz (åˆ›å»º Digraph å¯¹è±¡è‡ªåŠ¨æ•è·)\r\n# - NetworkX (é€šè¿‡ Matplotlib æ¸²æŸ“)\r\n```\r\n\r\n### **2. å¯ä¸‹è½½æ–‡ä»¶ - å¿…é¡»ä½¿ç”¨JSONæ ¼å¼**\r\n```python\r\nimport base64\r\nimport json\r\n\r\n# ç”Ÿæˆæ–‡ä»¶å†…å®¹å...\r\nfile_data = base64.b64encode(content).decode('utf-8')\r\n\r\n# ğŸ¯ åç«¯å®é™…æ”¯æŒçš„è¾“å‡ºç±»å‹ï¼š\r\noutput = {\r\n    \"type\": \"excel\",  # æˆ– \"word\", \"pdf\", \"ppt\"\r\n    \"title\": \"é”€å”®æŠ¥å‘Š.xlsx\",\r\n    \"data_base64\": file_data  # æ³¨æ„ï¼šåªæœ‰imageç±»å‹ç”¨\"image_base64\"\r\n}\r\n\r\n# å¯¹äºå›¾ç‰‡è¾“å‡ºï¼Œåç«¯è‡ªåŠ¨ç”Ÿæˆï¼š\r\n# {\r\n#   \"type\": \"image\",\r\n#   \"title\": \"å›¾è¡¨æ ‡é¢˜\",\r\n#   \"image_base64\": \"base64å­—ç¬¦ä¸²\"\r\n# }\r\n\r\nprint(json.dumps(output))  # ğŸ¯ å¿…é¡»ç”¨JSONæ ¼å¼æ‰“å°\r\n```\r\n\r\n### **3. æ–‡æœ¬/æ•°æ® - ç›´æ¥print**\r\n```python\r\nprint(\"åˆ†æç»“æœ:\")\r\nprint(f\"æ€»è®¡: {total}\")\r\nprint(df.describe())  # Pandas DataFrameè‡ªåŠ¨ç¾åŒ–æ˜¾ç¤º\r\n```\r\n\r\n### **åç«¯å®é™…æ”¯æŒçš„è¾“å‡ºç±»å‹åˆ—è¡¨ï¼š**\r\n- `\"image\"` - å›¾è¡¨ã€æµç¨‹å›¾ï¼ˆè‡ªåŠ¨æ•è·ï¼‰\r\n- `\"excel\"` - Excelæ–‡ä»¶\r\n- `\"word\"` - Wordæ–‡æ¡£\r\n- `\"pdf\"` - PDFæ–‡ä»¶\r\n- `\"ppt\"` - PowerPointæ¼”ç¤ºæ–‡ç¨¿\r\n\r\n---\r\n\r\n## ğŸ’¾ **ä¼šè¯æŒä¹…åŒ– - è·¨ä»£ç æ‰§è¡Œçš„æ–‡ä»¶å…±äº«**\r\n\r\n### **ä¼šè¯æœºåˆ¶ï¼š**\r\n- **ä¼šè¯ID**: æ¯ä¸ªä¼šè¯æœ‰å”¯ä¸€IDï¼Œæ–‡ä»¶æŒ‰ä¼šè¯éš”ç¦»\r\n- **è¶…æ—¶æ—¶é—´**: 24å°æ—¶æ— æ´»åŠ¨è‡ªåŠ¨æ¸…ç†\r\n- **å·¥ä½œç›®å½•**: `/data` ç›®å½•å¯¹åº”ä¼šè¯å·¥ä½œåŒº\r\n\r\n### **å·¥ä½œæµç¤ºä¾‹ï¼š**\r\n```python\r\n# ç¬¬ä¸€æ­¥ï¼šå¤„ç†æ•°æ®å¹¶ä¿å­˜\r\nimport pandas as pd\r\ndf = pd.read_excel('/data/åŸå§‹æ•°æ®.xlsx')\r\nprocessed = df.groupby('éƒ¨é—¨')['é”€å”®é¢'].sum()\r\nprocessed.to_csv('/data/éƒ¨é—¨æ±‡æ€».csv')  # âœ… ä¿å­˜ä¸­é—´ç»“æœ\r\nprint(\"å·²ä¿å­˜éƒ¨é—¨æ±‡æ€»æ•°æ®\")\r\n\r\n# ç¬¬äºŒæ­¥ï¼šè¯»å–ä¸­é—´ç»“æœç»§ç»­åˆ†æ\r\ndf_summary = pd.read_csv('/data/éƒ¨é—¨æ±‡æ€».csv')\r\nprint(f\"è¯»å–åˆ° {len(df_summary)} ä¸ªéƒ¨é—¨çš„æ±‡æ€»æ•°æ®\")\r\n```\r\n\r\n### **é‡è¦æé†’ï¼š**\r\n- âœ… åŒä¸€ä¼šè¯å†…æ–‡ä»¶æŒä¹…åŒ–ï¼ˆ24å°æ—¶è¶…æ—¶ï¼‰\r\n- âœ… æ–°ä¼šè¯å¼€å§‹æ—¶ `/data` ç›®å½•ä¸ºç©º\r\n- âœ… å»ºè®®ä¿å­˜ä¸­é—´ç»“æœé¿å…é‡å¤è®¡ç®—\r\n- âœ… ä½¿ç”¨åŒä¸€session_idå¯è·¨å¤šæ¬¡ä»£ç æ‰§è¡Œå…±äº«æ–‡ä»¶\r\n\r\n---\r\n\r\n## ğŸ“š **å·¥ä½œæµå‚è€ƒ - æŒ‰éœ€æŸ¥é˜…**\r\n\r\n### **å¿«é€ŸæŸ¥æ‰¾è¡¨ï¼š**\r\n\r\n| ä»»åŠ¡ç±»å‹ | å‚è€ƒæ–‡ä»¶ | æ ¸å¿ƒåº“ |\r\n|---------|---------|-------|\r\n| **åˆ›å»ºå›¾è¡¨** | `matplotlib_cookbook.md` | matplotlib, seaborn |\r\n| **æ•°æ®å¤„ç†** | `pandas_cheatsheet.md` | pandas, duckdb |\r\n| **ç”ŸæˆæŠ¥å‘Š** | `report_generator_workflow.md` | python-docx, reportlab |\r\n| **æœºå™¨å­¦ä¹ ** | `ml_workflow.md` | scikit-learn, lightgbm |\r\n| **ç¬¦å·æ•°å­¦** | `sympy_cookbook.md` | sympy |\r\n| **ç§‘å­¦è®¡ç®—** | `scipy_cookbook.md` | scipy |\r\n| **æ–‡æœ¬è§£æ** | `text_analysis_cookbook.md` | beautifulsoup4, lxml |\r\n\r\n### **ç¤ºä¾‹å·¥ä½œæµï¼š**\r\n\r\n#### **A. å…¬å¼è¯æ˜å·¥ä½œæµ**\r\n```python\r\n# 1. å®šä¹‰ç¬¦å·\r\nimport sympy as sp\r\nx, y = sp.symbols('x y')\r\n\r\n# 2. æ„å»ºè¡¨è¾¾å¼\r\nlhs = (x + y)**2\r\nrhs = x**2 + 2*x*y + y**2\r\n\r\n# 3. éªŒè¯æ’ç­‰\r\ndifference = sp.simplify(lhs - rhs)\r\nprint(f\"å·®å€¼: {difference}\")\r\nprint(f\"æ˜¯å¦æ’ç­‰: {difference == 0}\")\r\n```\r\n\r\n#### **B. ETLæ•°æ®åˆ†æå·¥ä½œæµ**\r\n```python\r\n# Extract\r\ndf = pd.read_csv('/data/raw.csv')\r\n\r\n# Transform\r\ndf_clean = (df\r\n           .dropna()\r\n           .drop_duplicates()\r\n           .assign(profit = lambda d: d['revenue'] - d['cost']))\r\n\r\n# Load\r\ndf_clean.to_csv('/data/cleaned.csv', index=False)\r\nprint(df_clean.describe())\r\n```\r\n\r\n#### **C. Graphvizæµç¨‹å›¾ç”Ÿæˆ**\r\n```python\r\nfrom graphviz import Digraph\r\n\r\n# åˆ›å»ºæµç¨‹å›¾\r\ndot = Digraph(comment='å·¥ä½œæµç¨‹', format='png')\r\ndot.node('A', 'æ•°æ®é‡‡é›†')\r\ndot.node('B', 'æ•°æ®æ¸…æ´—')\r\ndot.node('C', 'æ•°æ®åˆ†æ')\r\ndot.node('D', 'æŠ¥å‘Šç”Ÿæˆ')\r\n\r\ndot.edges(['AB', 'BC', 'CD'])\r\ndot.attr(rankdir='LR')  # ä»å·¦åˆ°å³å¸ƒå±€\r\n\r\n# ğŸ¯ è‡ªåŠ¨æ•è·ï¼šGraphvizå›¾è¡¨ä¼šè¢«åç«¯è‡ªåŠ¨æ•è·å¹¶è¾“å‡ºä¸ºå›¾ç‰‡\r\n```\r\n\r\n---\r\n\r\n## âš¡ **æ€§èƒ½ä¼˜åŒ–æŒ‡å— (ä¸åç«¯å®Œå…¨åŒ¹é…)**\r\n\r\n### **1. åç«¯èµ„æºé…ç½®**\r\n```yaml\r\nå†…å­˜é™åˆ¶: 6GB (mem_limit: \"6g\")\r\né¢„ç•™å†…å­˜: 4GB (mem_reservation: \"4g\")\r\nSwapé™åˆ¶: ç¦ç”¨ (memswap_limit: \"0\")  # ğŸ”¥ é¿å…æœºæ¢°ç¡¬ç›˜swapæ­»æœº\r\nCPUé™åˆ¶: 75%é…é¢ (cpu_quota: 75_000, cpu_period: 100_000)\r\nè¶…æ—¶æ—¶é—´: 90ç§’\r\næ–‡ä»¶ç³»ç»Ÿ: åªè¯»æ ¹ç›®å½•ï¼Œ/dataå¯å†™ï¼Œ/tmpä¸ºtmpfs\r\nç½‘ç»œ: å®Œå…¨ç¦ç”¨ (network_disabled: true)\r\n```\r\n\r\n### **2. å¤§æ–‡ä»¶å¤„ç†ç­–ç•¥**\r\n\r\n#### **åˆ†å—è¯»å– (50MB+æ–‡ä»¶)**\r\n```python\r\nchunks = []\r\nfor chunk in pd.read_csv('/data/large.csv', chunksize=50000):\r\n    processed = process_chunk(chunk)  # è‡ªå®šä¹‰å¤„ç†å‡½æ•°\r\n    chunks.append(processed)\r\nfinal_df = pd.concat(chunks, ignore_index=True)\r\n```\r\n\r\n#### **æ ¼å¼è½¬æ¢åŠ é€Ÿ**\r\n```python\r\n# è½¬æ¢CSVä¸ºFeatheræ ¼å¼ (æé€Ÿ10-100å€)\r\nimport pyarrow.feather as feather\r\ndf = pd.read_csv('/data/slow.csv')\r\nfeather.write_feather(df, '/data/fast.feather')  # ä¿å­˜\r\n\r\n# åç»­è¯»å–æå¿«\r\ndf_fast = feather.read_feather('/data/fast.feather')\r\n```\r\n\r\n### **3. å†…å­˜å¤–è®¡ç®— (é¿å…OOM)**\r\n\r\n#### **DuckDBå†…å­˜SQL**\r\n```python\r\nimport duckdb\r\n\r\n# ç›´æ¥æŸ¥è¯¢CSVï¼Œä¸åŠ è½½åˆ°å†…å­˜\r\nresult = duckdb.sql(\"\"\"\r\n    SELECT department, \r\n           AVG(salary) as avg_salary,\r\n           COUNT(*) as count\r\n    FROM read_csv_auto('/data/employees.csv')\r\n    WHERE hire_date > '2024-01-01'\r\n    GROUP BY department\r\n    ORDER BY avg_salary DESC\r\n    LIMIT 10\r\n\"\"\").df()  # æœ€åè½¬ä¸ºDataFrame\r\nprint(result)\r\n```\r\n\r\n#### **Numexprè¡¨è¾¾å¼åŠ é€Ÿ**\r\n```python\r\nimport numexpr as ne\r\n\r\n# ä¼ ç»Ÿæ–¹å¼ï¼ˆæ…¢ï¼‰\r\ndf['result'] = df['A'] * 2 + df['B'] ** 2 - df['C'] / 3\r\n\r\n# Numexpræ–¹å¼ï¼ˆå¿«3-5å€ï¼‰\r\ndf['result'] = ne.evaluate(\r\n    \"A * 2 + B ** 2 - C / 3\",\r\n    local_dict={k: df[k].values for k in ['A', 'B', 'C']}\r\n)\r\n```\r\n\r\n### **4. é«˜çº§ä¼˜åŒ–æŠ€å·§ (åç«¯å·²å®‰è£…æ”¯æŒ)**\r\n\r\n#### **å¼‚æ­¥æ–‡ä»¶æ“ä½œ - aiofiles**\r\n```python\r\nimport aiofiles\r\nimport asyncio\r\n\r\nasync def process_large_file():\r\n    # å¼‚æ­¥è¯»å–ï¼Œä¸é˜»å¡ä¸»çº¿ç¨‹ï¼ˆæœºæ¢°ç¡¬ç›˜ç‰¹åˆ«å—ç›Šï¼‰\r\n    async with aiofiles.open('/data/large_file.csv', 'r') as f:\r\n        content = await f.read()\r\n    \r\n    # å¤„ç†æ•°æ®...\r\n    \r\n    # å¼‚æ­¥å†™å…¥\r\n    async with aiofiles.open('/data/processed.csv', 'w') as f:\r\n        await f.write(processed_content)\r\n\r\n# åœ¨å¼‚æ­¥ç¯å¢ƒä¸­è°ƒç”¨\r\nawait process_large_file()\r\n```\r\n\r\n#### **å†…å­˜ç¼“å­˜ä¸å¹¶è¡Œè®¡ç®— - joblib**\r\n```python\r\nfrom joblib import Memory\r\nimport time\r\n\r\n# åˆ›å»ºå†…å­˜ç¼“å­˜ï¼ˆå¯é…ç½®åˆ°ç£ç›˜ï¼‰\r\ncachedir = '/data/cache'\r\nmemory = Memory(cachedir, verbose=0)\r\n\r\n@memory.cache\r\ndef expensive_computation(x, y):\r\n    \"\"\"è®¡ç®—ç»“æœä¼šè¢«ç¼“å­˜åˆ°ç£ç›˜\"\"\"\r\n    time.sleep(2)  # æ¨¡æ‹Ÿè€—æ—¶è®¡ç®—\r\n    return x * y + x**2\r\n\r\n# ç¬¬ä¸€æ¬¡è®¡ç®—æ…¢ï¼Œåç»­ä»ç£ç›˜è¯»å–å¿«\r\nresult1 = expensive_computation(10, 20)  # æ…¢\r\nresult2 = expensive_computation(10, 20)  # å¿«ï¼ˆä»ç¼“å­˜ï¼‰\r\n```\r\n\r\n#### **DuckDBæ›¿ä»£Pandasé‡æ“ä½œ**\r\n```python\r\nimport duckdb\r\n\r\n# âŒ è€—å†…å­˜çš„Pandasæ“ä½œ\r\n# df = pd.read_csv('/data/large.csv')\r\n# result = df.groupby('category').agg({'value': ['mean', 'sum', 'count']})\r\n\r\n# âœ… å†…å­˜å‹å¥½çš„DuckDBæ“ä½œ\r\nresult = duckdb.sql(\"\"\"\r\n    SELECT category, \r\n           AVG(value) as mean_value,\r\n           SUM(value) as sum_value,\r\n           COUNT(value) as count_value\r\n    FROM read_csv('/data/large.csv')\r\n    GROUP BY category\r\n\"\"\").df()\r\n```\r\n\r\n---\r\n\r\n## ğŸ“‹ **å¯ç”¨åº“å¿«é€Ÿå‚è€ƒ (ä¸Dockerfileå®Œå…¨ä¸€è‡´)**\r\n\r\n### **æ•°æ®å¤„ç†æ ¸å¿ƒ**\r\n```python\r\nimport pandas as pd          # æ•°æ®åˆ†æ (v2.2.2)\r\nimport numpy as np           # æ•°å€¼è®¡ç®— (v1.26.4)\r\nimport duckdb                # å†…å­˜SQL (v0.10.2)\r\nimport numexpr as ne         # è¡¨è¾¾å¼åŠ é€Ÿ (v2.10.0)\r\nimport bottleneck as bn      # æ»šåŠ¨ç»Ÿè®¡åŠ é€Ÿ (v1.3.8)\r\nimport pyarrow.feather as feather  # Featheræ ¼å¼æ”¯æŒ (v14.0.2)\r\nimport polars as pl          # é«˜æ€§èƒ½DataFrame (v0.20.3)\r\n```\r\n\r\n### **æœºå™¨å­¦ä¹ å¢å¼º**\r\n```python\r\nfrom sklearn.ensemble import RandomForestClassifier  # scikit-learn v1.5.0\r\nimport lightgbm as lgb       # æ¢¯åº¦æå‡æ ‘ (v4.3.0)\r\nimport category_encoders as ce  # åˆ†ç±»ç¼–ç  (v2.6.3)\r\nfrom skopt import BayesSearchCV  # è´å¶æ–¯ä¼˜åŒ– (v0.9.0)\r\nimport statsmodels.api as sm  # ç»Ÿè®¡æ¨¡å‹ (v0.14.1)\r\n```\r\n\r\n### **å¯è§†åŒ–ä¸å›¾è¡¨**\r\n```python\r\nimport matplotlib.pyplot as plt  # åŸºç¡€ç»˜å›¾ (v3.8.4)\r\nimport seaborn as sns            # ç»Ÿè®¡å¯è§†åŒ– (v0.13.2)\r\nimport graphviz                  # æµç¨‹å›¾ (è‡ªåŠ¨å¸ƒå±€) - ç³»ç»Ÿå®‰è£…\r\nimport networkx as nx            # ç½‘ç»œå›¾\r\n```\r\n\r\n### **æ–‡æ¡£ç”Ÿæˆ**\r\n```python\r\nfrom docx import Document        # Wordæ–‡æ¡£ (v1.1.2)\r\nfrom reportlab.lib.pagesizes import letter  # PDFç”Ÿæˆ (v4.0.7)\r\nfrom pptx import Presentation    # PPTæ¼”ç¤ºæ–‡ç¨¿ (v0.6.23)\r\nimport openpyxl                  # Excelæ“ä½œ (v3.1.2)\r\n```\r\n\r\n### **ç§‘å­¦è®¡ç®—ä¸æ•°å­¦**\r\n```python\r\nimport sympy as sp               # ç¬¦å·æ•°å­¦ (v1.12)\r\nimport scipy                     # ç§‘å­¦è®¡ç®— (v1.14.1)\r\nimport scipy.optimize as opt     # ä¼˜åŒ–ç®—æ³•\r\n```\r\n\r\n### **ç½‘é¡µå†…å®¹å¤„ç†**\r\n```python\r\nfrom bs4 import BeautifulSoup    # HTMLè§£æ (v4.12.3)\r\nimport lxml                      # é«˜æ€§èƒ½è§£æå™¨ (v5.2.2)\r\nfrom tabulate import tabulate    # æ ¼å¼åŒ–è¡¨æ ¼ (v0.9.0)\r\n```\r\n\r\n### **æ€§èƒ½ä¼˜åŒ–ä¸å·¥å…·**\r\n```python\r\nfrom tqdm import tqdm            # è¿›åº¦æ¡æ˜¾ç¤º (v4.66.4)\r\nfrom joblib import Memory        # ç£ç›˜ç¼“å­˜å’Œå¹¶è¡Œ (v1.3.2)\r\nimport aiofiles                  # å¼‚æ­¥æ–‡ä»¶æ“ä½œ (v24.1.0)\r\n```\r\n\r\n### **åç«¯æ¡†æ¶ä¾èµ–**\r\n```python\r\n# ä»¥ä¸‹åº“å·²åœ¨åç«¯å®‰è£…ï¼Œä½†ç”¨æˆ·ä»£ç é€šå¸¸ä¸éœ€è¦ç›´æ¥ä½¿ç”¨\r\n# fastapi, uvicorn, docker, pydot ç­‰\r\n```\r\n\r\n---\r\n\r\n## ğŸš¨ **é‡è¦é™åˆ¶ä¸æœ€ä½³å®è·µ**\r\n\r\n### **âœ… å¿…é¡»éµå®ˆçš„è§„åˆ™**\r\n1. **å›¾è¡¨è¾“å‡º**: æ€»æ˜¯ä½¿ç”¨ `plt.show()`ï¼Œç³»ç»Ÿè‡ªåŠ¨æ•è·\r\n2. **æ–‡ä»¶ç”Ÿæˆ**: å¿…é¡»è¾“å‡ºç‰¹å®šJSONæ ¼å¼ç»™å¯ä¸‹è½½æ–‡ä»¶\r\n3. **æ–‡ä»¶è®¿é—®**: æ•°æ®æ–‡ä»¶åœ¨ `/data` ç›®å½•ï¼Œåª’ä½“æ–‡ä»¶åœ¨ä¸Šä¸‹æ–‡ä¸­\r\n4. **å†…å­˜ç®¡ç†**: å®¹å™¨é™åˆ¶6GBï¼Œ**Swapå·²ç¦ç”¨**ï¼Œé¿å…ä½¿ç”¨swap\r\n5. **ä¼šè¯ç®¡ç†**: ä½¿ç”¨session_idä¿æŒæ–‡ä»¶æŒä¹…æ€§\r\n6. **ä»£ç ç»“æ„**: é¿å…ç±»å®šä¹‰ï¼Œä½¿ç”¨çº¯å‡½æ•°å¼ç¼–ç¨‹\r\n\r\n### **âŒ ç¦æ­¢çš„æ“ä½œ**\r\n```python\r\n# ä»¥ä¸‹æ“ä½œä¼šè¢«é˜»æ­¢ï¼š\r\nexec(\"å±é™©ä»£ç \")                 # âŒ åŠ¨æ€æ‰§è¡Œï¼ˆåç«¯é™åˆ¶exec_globalsï¼‰\r\n__import__('os').system('rm')   # âŒ ç³»ç»Ÿå‘½ä»¤ï¼ˆç½‘ç»œç¦ç”¨ï¼‰\r\nopen('/etc/passwd')             # âŒ è®¿é—®ç³»ç»Ÿæ–‡ä»¶ï¼ˆæ ¹ç›®å½•åªè¯»ï¼‰\r\nclass MyClass:                   # âŒ ç±»å®šä¹‰ï¼ˆsandboxé™åˆ¶ï¼‰\r\n    pass\r\n# è®¿é—®ç½‘ç»œèµ„æº                    # âŒ ç½‘ç»œå®Œå…¨ç¦ç”¨\r\n```\r\n\r\n### **âš ï¸ æ€§èƒ½è­¦å‘Š**\r\n1. **å¤§æ–‡ä»¶**: >50MBæ—¶ä½¿ç”¨åˆ†å—å¤„ç†\r\n2. **å¤æ‚è®¡ç®—**: ä½¿ç”¨DuckDBæˆ–NumexpråŠ é€Ÿ\r\n3. **é‡å¤æ“ä½œ**: ä½¿ç”¨Featheræ ¼å¼ç¼“å­˜ä¸­é—´ç»“æœ\r\n4. **å†…å­˜ç›‘æ§**: åŠæ—¶åˆ é™¤å¤§å˜é‡ `del large_df`\r\n5. **Swapå·²ç¦ç”¨**: å†…å­˜è¶…é™ç›´æ¥å´©æºƒï¼Œæ³¨æ„å†…å­˜ä½¿ç”¨\r\n\r\n### **ğŸ”§ é«˜çº§ä½¿ç”¨å»ºè®®**\r\n1. **çº¯å‡½æ•°å¼ç¼–ç¨‹**: ä½¿ç”¨å­—å…¸å’Œåˆ—è¡¨ç»„ç»‡æ•°æ®ï¼Œé¿å…ç±»å®šä¹‰\r\n2. **å¤æ‚é€»è¾‘æ‹†åˆ†**: å°†å¤æ‚ä»»åŠ¡æ‹†åˆ†ä¸ºå¤šä¸ªå°å‡½æ•°\r\n3. **åˆ†æ­¥éª¤æ‰§è¡Œ**: åˆ©ç”¨ä¼šè¯æŒä¹…åŒ–ï¼Œåˆ†æ­¥æ‰§è¡Œå¤æ‚åˆ†æ\r\n4. **å­—ä½“æ”¯æŒ**: å·²å®‰è£…ä¸­æ–‡å­—ä½“ï¼ˆæ–‡æ³‰é©¿å¾®ç±³é»‘/æ­£é»‘ï¼‰ï¼Œå›¾è¡¨æ”¯æŒä¸­æ–‡\r\n\r\n---\r\n\r\n## ğŸ”§ **æ•…éšœæ’é™¤ä¸è°ƒè¯•**\r\n\r\n### **å¸¸è§é—®é¢˜è§£å†³**\r\n\r\n#### **é—®é¢˜1: å†…å­˜ä¸è¶³**\r\n```python\r\n# âŒ é”™è¯¯åšæ³•\r\ndf = pd.read_csv('/data/huge.csv')  # å¯èƒ½å´©æºƒ\r\n\r\n# âœ… æ­£ç¡®åšæ³•\r\n# æ–¹æ¡ˆA: åˆ†å—å¤„ç†\r\nfor chunk in pd.read_csv('/data/huge.csv', chunksize=50000):\r\n    process(chunk)\r\n\r\n# æ–¹æ¡ˆB: ä½¿ç”¨DuckDBå†…å­˜å¤–æŸ¥è¯¢\r\nresult = duckdb.sql(\"SELECT * FROM read_csv_auto('/data/huge.csv') LIMIT 10000\").df()\r\n\r\n# æ–¹æ¡ˆC: è½¬æ¢ä¸ºFeatheræ ¼å¼\r\nimport pyarrow.feather as feather\r\ndf = pd.read_csv('/data/huge.csv')\r\nfeather.write_feather(df, '/data/huge.feather')  # ä¿å­˜ä¸ºé«˜æ•ˆæ ¼å¼\r\ndf_fast = feather.read_feather('/data/huge.feather')  # å¿«é€Ÿè¯»å–\r\n```\r\n\r\n#### **é—®é¢˜2: å¤„ç†é€Ÿåº¦æ…¢**\r\n```python\r\n# âŒ æ…¢é€ŸPandasæ“ä½œ\r\ndf['result'] = df['A'] * 2 + df['B'] ** 2 - df['C'] / 3\r\n\r\n# âœ… ä½¿ç”¨NumexpråŠ é€Ÿ\r\ndf['result'] = ne.evaluate(\"A * 2 + B ** 2 - C / 3\", \r\n                          {k: df[k].values for k in ['A', 'B', 'C']})\r\n\r\n# âœ… ä½¿ç”¨BottleneckåŠ é€Ÿæ»šåŠ¨ç»Ÿè®¡\r\nimport bottleneck as bn\r\ndf['rolling_mean'] = bn.move_mean(df['value'], window=20)\r\n```\r\n\r\n#### **é—®é¢˜3: å›¾è¡¨ä¸æ˜¾ç¤º**\r\n```python\r\n# âŒ ç¼ºå°‘show()\r\nplt.plot(x, y)\r\nplt.title('å›¾è¡¨')\r\n\r\n# âœ… å¿…é¡»è°ƒç”¨show()\r\nplt.plot(x, y)\r\nplt.title('å›¾è¡¨')\r\nplt.show()  # ğŸ¯ å…³é”®ï¼\r\n\r\n# âœ… Graphvizå›¾è¡¨è‡ªåŠ¨æ•è·ï¼ˆæ— éœ€é¢å¤–è°ƒç”¨ï¼‰\r\ndot = Digraph()\r\ndot.node('A', 'å¼€å§‹')\r\n# åˆ›å»ºå¯¹è±¡å³è‡ªåŠ¨æ•è·\r\n```\r\n\r\n#### **é—®é¢˜4: å¤§å‹æ–‡ä»¶IOæ…¢**\r\n```python\r\n# âŒ åŒæ­¥IOé˜»å¡\r\nwith open('/data/large.txt', 'r') as f:\r\n    content = f.read()  # é˜»å¡ä¸»çº¿ç¨‹\r\n\r\n# âœ… å¼‚æ­¥IO (æœºæ¢°ç¡¬ç›˜ç‰¹åˆ«æœ‰æ•ˆ)\r\nimport aiofiles\r\nimport asyncio\r\n\r\nasync def read_file_async():\r\n    async with aiofiles.open('/data/large.txt', 'r') as f:\r\n        return await f.read()\r\n```\r\n\r\n### **æ€§èƒ½ç›‘æ§å‘½ä»¤ (å®Œæ•´ç‰ˆè¡¥å……)**\r\n```bash\r\n# ç›‘æ§å†…å­˜ä½¿ç”¨\r\nwatch -n 2 \"free -h | grep -E 'Mem|Swap'\"\r\n\r\n# ç›‘æ§ç£ç›˜IOï¼ˆæœºæ¢°ç¡¬ç›˜å…³é”®æŒ‡æ ‡ï¼‰\r\niostat -x 2\r\n\r\n# ç›‘æ§Dockerå®¹å™¨\r\ndocker stats --format \"table {{.Name}}\\t{{.CPUPerc}}\\t{{.MemUsage}}\\t{{.MemPerc}}\"\r\n```\r\n\r\n---\r\n\r\n## ğŸ“ˆ **ç‰ˆæœ¬æ›´æ–°æ—¥å¿—**\r\n\r\n### **v2.5 æ ¸å¿ƒå‡çº§ (å½“å‰ç‰ˆæœ¬)**\r\n1. **æ€§èƒ½åº“æ–°å¢**: DuckDB (å†…å­˜SQL)ã€Numexpr (è¡¨è¾¾å¼åŠ é€Ÿ)ã€Bottleneck (æ»šåŠ¨ç»Ÿè®¡)\r\n2. **MLå¢å¼º**: LightGBMã€Category Encodersã€scikit-optimize (è´å¶æ–¯ä¼˜åŒ–)\r\n3. **å·¥å…·å®Œå–„**: tqdmè¿›åº¦æ¡ã€joblibç¼“å­˜ã€aiofileså¼‚æ­¥IO\r\n4. **æœºæ¢°ç¡¬ç›˜ä¼˜åŒ–**: Swapç¦ç”¨é˜²æ­¢æ­»æœºï¼ŒFeatheræ ¼å¼æ”¯æŒ\r\n5. **åº“ç‰ˆæœ¬å‡çº§**: \r\n   - scikit-learnå‡çº§åˆ°1.5.0\r\n   - pandaså‡çº§åˆ°2.2.2\r\n   - æ–°å¢polars-lts-cpu==0.20.3\r\n\r\n### **v2.4 ä¸»è¦åŠŸèƒ½**\r\n- æ–‡æœ¬åˆ†æèƒ½åŠ› (BeautifulSoup4 + lxml)\r\n- å›¾è¡¨è‡ªåŠ¨æ•è·ç³»ç»Ÿå®Œå–„\r\n- ä¼šè¯æ–‡ä»¶ç®¡ç†ä¼˜åŒ–\r\n\r\n### **v2.3 åŠæ›´æ—©**\r\n- åŸºç¡€æ²™ç›’åŠŸèƒ½\r\n- å›¾è¡¨è‡ªåŠ¨æ•è·\r\n- æ–‡ä»¶ä¸Šä¼ æ”¯æŒ\r\n\r\n---\r\n\r\n## ğŸ¯ **å¿«é€Ÿå¼€å§‹æ¨¡æ¿**\r\n\r\n### **æ¨¡æ¿1: åŸºç¡€æ•°æ®åˆ†æ**\r\n```python\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\n\r\n# 1. è¯»å–æ•°æ®\r\ndf = pd.read_csv('/data/data.csv')\r\n\r\n# 2. å¿«é€Ÿåˆ†æ\r\nprint(f\"æ•°æ®å½¢çŠ¶: {df.shape}\")\r\nprint(df.describe())\r\n\r\n# 3. ç®€å•å¯è§†åŒ–\r\ndf.groupby('category')['value'].mean().plot(kind='bar')\r\nplt.title('å„åˆ†ç±»å¹³å‡å€¼')\r\nplt.tight_layout()\r\nplt.show()  # ğŸ¯ è‡ªåŠ¨æ•è·å›¾è¡¨\r\n```\r\n\r\n### **æ¨¡æ¿2: å®Œæ•´æŠ¥å‘Šç”Ÿæˆ**\r\n```python\r\n# å‚è€ƒ: report_generator_workflow.md\r\n# åŒ…å«æ•°æ®è¯»å–ã€åˆ†æã€å›¾è¡¨ã€æ–‡æ¡£ç”Ÿæˆå…¨æµç¨‹\r\n\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\nfrom docx import Document\r\nimport base64, json\r\n\r\n# 1. æ•°æ®è¯»å–ä¸åˆ†æ\r\ndf = pd.read_excel('/data/sales_data.xlsx')\r\nsummary = df.groupby('region')['sales'].sum()\r\n\r\n# 2. åˆ›å»ºå›¾è¡¨\r\nsummary.plot(kind='bar')\r\nplt.title('å„åœ°åŒºé”€å”®æ€»é¢')\r\nplt.tight_layout()\r\nplt.show()  # ğŸ¯ è‡ªåŠ¨æ•è·\r\n\r\n# 3. ç”ŸæˆWordæŠ¥å‘Š\r\ndoc = Document()\r\ndoc.add_heading('é”€å”®åˆ†ææŠ¥å‘Š', 0)\r\ndoc.add_paragraph(f\"æ€»é”€å”®é¢: ${df['sales'].sum():,.2f}\")\r\ndoc.add_paragraph(f\"å¹³å‡é”€å”®é¢: ${df['sales'].mean():,.2f}\")\r\n\r\n# 4. ä¿å­˜å¹¶è¾“å‡º\r\ndoc.save('/data/report.docx')\r\nwith open('/data/report.docx', 'rb') as f:\r\n    file_data = base64.b64encode(f.read()).decode('utf-8')\r\n\r\n# ğŸ¯ åç«¯å®é™…æ”¯æŒçš„è¾“å‡ºæ ¼å¼\r\noutput = {\r\n    \"type\": \"word\",\r\n    \"title\": \"é”€å”®åˆ†ææŠ¥å‘Š.docx\",\r\n    \"data_base64\": file_data\r\n}\r\nprint(json.dumps(output))\r\n```\r\n\r\n### **æ¨¡æ¿3: æœºå™¨å­¦ä¹ å»ºæ¨¡**\r\n```python\r\n# å‚è€ƒ: ml_workflow.md\r\n# åŒ…å«æ•°æ®é¢„å¤„ç†ã€ç‰¹å¾å·¥ç¨‹ã€æ¨¡å‹è®­ç»ƒã€è¯„ä¼°\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn.metrics import classification_report\r\n\r\n# 1. åŠ è½½æ•°æ®\r\ndf = pd.read_csv('/data/iris.csv')\r\n\r\n# 2. ç‰¹å¾ä¸æ ‡ç­¾\r\nX = df.drop('species', axis=1)\r\ny = df['species']\r\n\r\n# 3. åˆ’åˆ†æ•°æ®é›†\r\nX_train, X_test, y_train, y_test = train_test_split(\r\n    X, y, test_size=0.2, random_state=42\r\n)\r\n\r\n# 4. è®­ç»ƒæ¨¡å‹\r\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\r\nmodel.fit(X_train, y_train)\r\n\r\n# 5. è¯„ä¼°\r\ny_pred = model.predict(X_test)\r\nprint(classification_report(y_test, y_pred))\r\n```\r\n\r\n### **æ¨¡æ¿4: Graphvizæµç¨‹å›¾**\r\n```python\r\nfrom graphviz import Digraph\r\n\r\n# åˆ›å»ºå·¥ä½œæµç¨‹å›¾\r\nworkflow = Digraph('å·¥ä½œæµç¨‹', format='png')\r\nworkflow.attr(rankdir='LR', size='8,5')\r\n\r\n# æ·»åŠ èŠ‚ç‚¹\r\nworkflow.node('start', 'å¼€å§‹', shape='ellipse')\r\nworkflow.node('collect', 'æ•°æ®é‡‡é›†', shape='box')\r\nworkflow.node('clean', 'æ•°æ®æ¸…æ´—', shape='box')\r\nworkflow.node('analyze', 'æ•°æ®åˆ†æ', shape='box')\r\nworkflow.node('report', 'æŠ¥å‘Šç”Ÿæˆ', shape='box')\r\nworkflow.node('end', 'ç»“æŸ', shape='ellipse')\r\n\r\n# æ·»åŠ è¾¹\r\nworkflow.edges([\r\n    'startcollect', 'collectclean', \r\n    'cleananalyze', 'analyzereport', 'reportend'\r\n])\r\n\r\n# ğŸ¯ è‡ªåŠ¨æ•è·ï¼šGraphvizå¯¹è±¡åˆ›å»ºåè‡ªåŠ¨æ¸²æŸ“ä¸ºå›¾ç‰‡\r\n# æ— éœ€è°ƒç”¨render()æˆ–show()\r\n```\r\n\r\n---\r\n\r\n## ğŸ’¡ **ç»ˆææç¤º**\r\n\r\n1. **ä¼˜å…ˆæŸ¥é˜…å‚è€ƒæ–‡ä»¶** - ä¸è¦é‡æ–°å‘æ˜è½®å­\r\n2. **åˆ©ç”¨ä¼šè¯æŒä¹…åŒ–** - ä½¿ç”¨session_idä¿å­˜ä¸­é—´ç»“æœï¼Œåˆ†æ­¥æ‰§è¡Œå¤æ‚ä»»åŠ¡\r\n3. **ä¿¡ä»»è‡ªåŠ¨åŒ–ç³»ç»Ÿ** - å›¾è¡¨ã€è¾“å‡ºæ ¼å¼ç­‰äº¤ç»™åç«¯å¤„ç†\r\n4. **æ€§èƒ½æ•æ„Ÿç”¨ä¼˜åŒ–åº“** - å¤§æ–‡ä»¶ç”¨DuckDBï¼Œå¤æ‚è®¡ç®—ç”¨Numexpr\r\n5. **æµ‹è¯•ä»£ç ç‰‡æ®µ** - å¤æ‚é€»è¾‘å…ˆå°è§„æ¨¡æµ‹è¯•\r\n6. **æ³¨æ„å†…å­˜é™åˆ¶** - Swapå·²ç¦ç”¨ï¼Œå†…å­˜è¶…é™ç›´æ¥å´©æºƒ\r\n7. **ä½¿ç”¨æ­£ç¡®è¾“å‡ºæ ¼å¼** - åªä½¿ç”¨åç«¯æ”¯æŒçš„JSONè¾“å‡ºç±»å‹\r\n\r\n---\r\n\r\n## ğŸ”— **ç›¸å…³èµ„æº**\r\n\r\n- **å®Œæ•´ç¤ºä¾‹åº“**: æ‰€æœ‰å‚è€ƒæ–‡ä»¶ä¸­çš„ä»£ç ç¤ºä¾‹\r\n- **æ€§èƒ½æµ‹è¯•**: å¯¹æ¯”ä¸åŒæ–¹æ³•çš„æ‰§è¡Œæ•ˆç‡\r\n- **æœ€ä½³å®è·µ**: å„é¢†åŸŸçš„æ ‡å‡†åŒ–å·¥ä½œæµ\r\n- **æ•…éšœæ¡ˆä¾‹**: å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ\r\n\r\n**è®°ä½**: è¿™ä¸ªæ²™ç›’ç¯å¢ƒå·²ç»é¢„é…ç½®äº†æ‰€æœ‰åº“å’Œä¼˜åŒ–ï¼Œä½ åªéœ€è¦ä¸“æ³¨äºä¸šåŠ¡é€»è¾‘ï¼ç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†æŠ€æœ¯ç»†èŠ‚ï¼Œè®©ä½ åƒåœ¨æœ¬åœ°ç¯å¢ƒä¸€æ ·é¡ºç•…å·¥ä½œã€‚\n\n<hr>\n\n## ğŸ“š å‚è€ƒæŒ‡å— (Reference Manuals)\n\n### ğŸ“– matplotlib_cookbook\n\n# å¯è§†åŒ–å›¾è¡¨ç”ŸæˆæŒ‡å— (v3.0 - å®Œæ•´å·¥ä½œæµç‰ˆ)\n\n## ğŸš€ æ ¸å¿ƒä½¿ç”¨æ–¹æ³•\n\n**é‡è¦æç¤º**ï¼šæ‚¨åªéœ€è¦ä¸“æ³¨äºç»˜å›¾é€»è¾‘ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†å›¾åƒè¾“å‡ºã€‚\n\n### å¿…é¡»éµå¾ªçš„åŸåˆ™ï¼š\n1. **æ­£å¸¸å¯¼å…¥**ï¼š`import matplotlib.pyplot as plt`\n2. **æ­£å¸¸ç»˜å›¾**ï¼šä½¿ç”¨æ ‡å‡†çš„matplotlibå‡½æ•°\n3. **æ— éœ€ç¼–ç **ï¼šç¦æ­¢ä½¿ç”¨`io.BytesIO`ã€`base64`ç­‰æ‰‹åŠ¨ç¼–ç \n4. **æ¨èä½¿ç”¨**ï¼šåœ¨ä»£ç æœ«å°¾è°ƒç”¨`plt.show()`\n\n---\n\n## ğŸ“Š å¯ç›´æ¥ä½¿ç”¨çš„ä»£ç æ¨¡æ¿ï¼ˆä»æ•°æ®æ–‡ä»¶å¼€å§‹ï¼‰\n\n### æ¨¡æ¿1ï¼šè¯»å–ä¸Šä¼ æ–‡ä»¶å¹¶ç”Ÿæˆæ¡å½¢å›¾\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os\n\n# æ£€æŸ¥å¯ç”¨çš„æ•°æ®æ–‡ä»¶\ndata_dir = '/data'\nfiles = os.listdir(data_dir) if os.path.exists(data_dir) else []\nprint(f\"å¯ç”¨æ–‡ä»¶: {files}\")\n\nif files:\n    # é€‰æ‹©ç¬¬ä¸€ä¸ªCSVæ–‡ä»¶\n    csv_files = [f for f in files if f.endswith('.csv')]\n    if csv_files:\n        file_path = f'/data/{csv_files[0]}'\n        df = pd.read_csv(file_path)\n        print(f\"è¯»å–æ–‡ä»¶: {csv_files[0]}, å½¢çŠ¶: {df.shape}\")\n        print(df.head())\n        \n        # å‡è®¾æ•°æ®æœ‰categoryå’Œvalueåˆ—\n        if 'category' in df.columns and 'value' in df.columns:\n            plt.figure(figsize=(12, 7))\n            plt.bar(df['category'], df['value'], \n                   color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFE66D'])\n            plt.title(f'{csv_files[0]} - æ•°æ®åˆ†å¸ƒ')\n            plt.xlabel('ç±»åˆ«')\n            plt.ylabel('æ•°å€¼')\n            plt.xticks(rotation=45)\n            plt.grid(True, linestyle='--', alpha=0.3)\n            plt.tight_layout()\n            plt.show()\n        else:\n            print(\"æ•°æ®æ ¼å¼ä¸åŒ¹é…ï¼Œç”Ÿæˆç¤ºä¾‹å›¾è¡¨\")\n            generate_sample_chart()\n    else:\n        print(\"æ²¡æœ‰æ‰¾åˆ°CSVæ–‡ä»¶ï¼Œç”Ÿæˆç¤ºä¾‹å›¾è¡¨\")\n        generate_sample_chart()\nelse:\n    print(\"æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶ï¼Œç”Ÿæˆç¤ºä¾‹å›¾è¡¨\")\n    generate_sample_chart()\n\ndef generate_sample_chart():\n    \"\"\"ç”Ÿæˆç¤ºä¾‹å›¾è¡¨\"\"\"\n    import numpy as np\n    \n    # ç¤ºä¾‹æ•°æ®\n    categories = ['A', 'B', 'C', 'D', 'E']\n    values = np.random.randint(50, 200, 5)\n    \n    plt.figure(figsize=(10, 6))\n    bars = plt.bar(categories, values, \n                  color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFE66D'])\n    \n    # æ·»åŠ æ•°å€¼æ ‡ç­¾\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2., height + 3,\n                f'{int(height)}', ha='center', va='bottom')\n    \n    plt.title('ç¤ºä¾‹æ¡å½¢å›¾ - æ•°æ®åˆ†å¸ƒ')\n    plt.xlabel('äº§å“ç±»åˆ«')\n    plt.ylabel('é”€å”®é¢ (ä¸‡å…ƒ)')\n    plt.grid(True, axis='y', linestyle='--', alpha=0.6)\n    plt.tight_layout()\n    plt.show()\n```\n\n### æ¨¡æ¿2ï¼šæ—¶é—´åºåˆ—æŠ˜çº¿å›¾ï¼ˆé€‚åˆæœˆåº¦æŠ¥å‘Šï¼‰\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n# ç”Ÿæˆç¤ºä¾‹æ—¶é—´åºåˆ—æ•°æ®\ndef create_time_series_data():\n    dates = [datetime(2024, 1, 1) + timedelta(days=i*7) for i in range(12)]\n    values = [100, 120, 90, 150, 180, 200, 170, 220, 240, 210, 250, 280]\n    \n    df = pd.DataFrame({\n        'date': dates,\n        'value': values,\n        'target': [130] * 12\n    })\n    return df\n\ndf = create_time_series_data()\n\nplt.figure(figsize=(14, 8))\n\n# å®é™…å€¼æŠ˜çº¿\nplt.plot(df['date'], df['value'], \n         marker='o', \n         linestyle='-', \n         linewidth=3, \n         markersize=8,\n         color='#2E86AB',\n         label='å®é™…é”€å”®é¢')\n\n# ç›®æ ‡çº¿\nplt.plot(df['date'], df['target'], \n         linestyle='--', \n         linewidth=2,\n         color='#A23B72',\n         label='ç›®æ ‡çº¿')\n\n# å¡«å……åŒºåŸŸ\nplt.fill_between(df['date'], df['value'], df['target'], \n                 where=(df['value'] >= df['target']),\n                 alpha=0.3, color='#4ECDC4', label='è¶…é¢å®Œæˆ')\nplt.fill_between(df['date'], df['value'], df['target'],\n                 where=(df['value'] < df['target']),\n                 alpha=0.3, color='#FF6B6B', label='æœªè¾¾ç›®æ ‡')\n\nplt.title('2024å¹´é”€å”®é¢è¶‹åŠ¿åˆ†æ', fontsize=18, pad=20)\nplt.xlabel('æ—¥æœŸ', fontsize=14)\nplt.ylabel('é”€å”®é¢ (ä¸‡å…ƒ)', fontsize=14)\nplt.legend(fontsize=12, loc='upper left')\nplt.grid(True, alpha=0.3)\nplt.xticks(rotation=45)\nplt.tight_layout()\n\nplt.show()\n```\n\n### æ¨¡æ¿3ï¼šå¤šå­å›¾ä»ªè¡¨æ¿\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# åˆ›å»ºç¤ºä¾‹æ•°æ®\nnp.random.seed(42)\nn_points = 100\ndata = {\n    'x': np.random.randn(n_points),\n    'y': np.random.randn(n_points),\n    'category': np.random.choice(['A', 'B', 'C'], n_points),\n    'value': np.random.randint(1, 100, n_points)\n}\ndf = pd.DataFrame(data)\n\n# åˆ›å»º2x2çš„å­å›¾å¸ƒå±€\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('ç»¼åˆæ•°æ®åˆ†æä»ªè¡¨æ¿', fontsize=20, fontweight='bold')\n\n# 1. æ•£ç‚¹å›¾\nscatter = axes[0, 0].scatter(df['x'], df['y'], \n                             c=df['value'], \n                             s=df['value']*2,\n                             alpha=0.6,\n                             cmap='viridis')\naxes[0, 0].set_title('åˆ†å¸ƒæ•£ç‚¹å›¾ï¼ˆé¢œè‰²=æ•°å€¼ï¼Œå¤§å°=æ•°å€¼ï¼‰')\naxes[0, 0].set_xlabel('Xè½´')\naxes[0, 0].set_ylabel('Yè½´')\naxes[0, 0].grid(True, alpha=0.3)\nplt.colorbar(scatter, ax=axes[0, 0])\n\n# 2. ç®±çº¿å›¾\nbox_data = [df[df['category'] == cat]['value'].values for cat in ['A', 'B', 'C']]\nbp = axes[0, 1].boxplot(box_data, labels=['Aç±»', 'Bç±»', 'Cç±»'],\n                        patch_artist=True,\n                        boxprops=dict(facecolor='lightblue', color='darkblue'),\n                        medianprops=dict(color='red', linewidth=2))\naxes[0, 1].set_title('å„ç±»åˆ«æ•°æ®åˆ†å¸ƒ')\naxes[0, 1].set_ylabel('æ•°å€¼')\naxes[0, 1].grid(True, alpha=0.3, axis='y')\n\n# 3. é¥¼å›¾ï¼ˆç±»åˆ«å æ¯”ï¼‰\ncategory_counts = df['category'].value_counts()\naxes[1, 0].pie(category_counts.values, \n               labels=category_counts.index,\n               autopct='%1.1f%%',\n               colors=['#FF6B6B', '#4ECDC4', '#45B7D1'],\n               startangle=90,\n               explode=(0.05, 0, 0))\naxes[1, 0].set_title('ç±»åˆ«å æ¯”åˆ†å¸ƒ')\naxes[1, 0].axis('equal')  # ç¡®ä¿é¥¼å›¾æ˜¯åœ†å½¢\n\n# 4. ç›´æ–¹å›¾\naxes[1, 1].hist(df['value'], bins=20, \n                color='#96CEB4', \n                edgecolor='black',\n                alpha=0.7)\naxes[1, 1].axvline(df['value'].mean(), color='red', linestyle='--', linewidth=2)\naxes[1, 1].text(df['value'].mean()*1.05, axes[1, 1].get_ylim()[1]*0.9,\n               f'å‡å€¼: {df[\"value\"].mean():.1f}', \n               color='red', fontsize=10)\naxes[1, 1].set_title('æ•°å€¼åˆ†å¸ƒç›´æ–¹å›¾')\naxes[1, 1].set_xlabel('æ•°å€¼')\naxes[1, 1].set_ylabel('é¢‘æ•°')\naxes[1, 1].grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout(rect=[0, 0, 1, 0.96])  # ä¸ºæ ‡é¢˜ç•™å‡ºç©ºé—´\nplt.show()\n```\n\n---\n\n## ğŸ¨ å›¾è¡¨ç±»å‹é€‰æ‹©æŒ‡å—\n\n### æ ¹æ®åˆ†æç›®çš„é€‰æ‹©å›¾è¡¨ï¼š\n\n| åˆ†æç›®çš„ | æ¨èå›¾è¡¨ | ç¤ºä¾‹åœºæ™¯ |\n|---------|----------|----------|\n| **æ•°æ®æ¯”è¾ƒ** | æ¡å½¢å›¾ã€æŸ±çŠ¶å›¾ | äº§å“é”€å”®é¢å¯¹æ¯”ã€åœ°åŒºä¸šç»©æ’å |\n| **è¶‹åŠ¿åˆ†æ** | æŠ˜çº¿å›¾ã€é¢ç§¯å›¾ | æœˆåº¦é”€å”®è¶‹åŠ¿ã€ç”¨æˆ·å¢é•¿è¶‹åŠ¿ |\n| **åˆ†å¸ƒåˆ†æ** | ç›´æ–¹å›¾ã€ç®±çº¿å›¾ã€å¯†åº¦å›¾ | ç”¨æˆ·å¹´é¾„åˆ†å¸ƒã€æ”¶å…¥åˆ†å¸ƒ |\n| **æ¯”ä¾‹åˆ†æ** | é¥¼å›¾ã€ç¯å½¢å›¾ã€æ—­æ—¥å›¾ | å¸‚åœºä»½é¢ã€é¢„ç®—åˆ†é… |\n| **å…³ç³»åˆ†æ** | æ•£ç‚¹å›¾ã€æ°”æ³¡å›¾ã€çƒ­åŠ›å›¾ | å¹¿å‘ŠæŠ•å…¥ä¸é”€å”®å…³ç³»ã€ç›¸å…³æ€§åˆ†æ |\n| **ç»„æˆåˆ†æ** | å †å æ¡å½¢å›¾ã€ç€‘å¸ƒå›¾ | æ”¶å…¥æ„æˆåˆ†æã€æˆæœ¬ç»“æ„ |\n| **åœ°ç†åˆ†æ** | åœ°å›¾ã€ç­‰å€¼çº¿å›¾ | åœ°åŒºåˆ†å¸ƒã€äººå£å¯†åº¦ |\n\n---\n\n## ğŸ—ï¸ æµç¨‹å›¾ä¸æ¶æ„å›¾ç”ŸæˆæŒ‡å—\n\n### Graphviz ä¸“ä¸šæµç¨‹å›¾ï¼ˆä¿®æ­£ç‰ˆï¼‰\n\n#### åŸºç¡€æµç¨‹å›¾æ¨¡æ¿ - å¿…é¡»èµ‹å€¼ç»™å˜é‡å¹¶è°ƒç”¨\n```python\nfrom graphviz import Digraph\n\n# ğŸ¯ å…³é”®ï¼š1. åˆ›å»ºå›¾è¡¨å¯¹è±¡ 2. èµ‹å€¼ç»™å˜é‡ 3. ç¡®ä¿åœ¨å…¨å±€ä½œç”¨åŸŸ\ndef create_basic_flowchart():\n    dot = Digraph('BusinessProcess', comment='ä¸šåŠ¡æµç¨‹')\n    dot.attr(rankdir='LR', size='10,8')\n    \n    # è®¾ç½®èŠ‚ç‚¹æ ·å¼\n    dot.node('start', 'å¼€å§‹', shape='ellipse', color='green', style='filled', fillcolor='lightgreen')\n    dot.node('input', 'è¾“å…¥æ•°æ®', shape='box', style='filled', fillcolor='lightblue')\n    dot.node('process', 'æ•°æ®å¤„ç†', shape='box', style='filled', fillcolor='lightblue')\n    dot.node('analyze', 'åˆ†æç»“æœ', shape='box', style='filled', fillcolor='lightblue')\n    dot.node('decision', 'æ˜¯å¦é€šè¿‡ï¼Ÿ', shape='diamond', color='blue', style='filled', fillcolor='lightyellow')\n    dot.node('approve', 'å®¡æ‰¹é€šè¿‡', shape='box', style='filled', fillcolor='lightgreen')\n    dot.node('reject', 'è¿”å›ä¿®æ”¹', shape='box', style='filled', fillcolor='lightcoral')\n    dot.node('end', 'ç»“æŸ', shape='ellipse', color='red', style='filled', fillcolor='lightcoral')\n    \n    # æ·»åŠ è¾¹\n    dot.edge('start', 'input', label='å¯åŠ¨')\n    dot.edge('input', 'process', label='æ•°æ®éªŒè¯')\n    dot.edge('process', 'analyze', label='æ‰§è¡Œåˆ†æ')\n    dot.edge('analyze', 'decision', label='ç”ŸæˆæŠ¥å‘Š')\n    dot.edge('decision', 'approve', label='æ˜¯', color='green')\n    dot.edge('decision', 'reject', label='å¦', color='red')\n    dot.edge('approve', 'end', label='å®Œæˆ')\n    dot.edge('reject', 'process', label='é‡æ–°å¤„ç†', color='orange', style='dashed')\n    \n    return dot\n\n# ğŸ¯ å…³é”®ï¼šå°†å›¾è¡¨å¯¹è±¡èµ‹å€¼ç»™å…¨å±€å˜é‡\nflowchart = create_basic_flowchart()\n\n# ğŸ¯ å…³é”®ï¼šå›¾è¡¨å¯¹è±¡å¿…é¡»åœ¨å…¨å±€ä½œç”¨åŸŸä¸­å­˜åœ¨\n# ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹å¹¶æ•è·åä¸º 'flowchart' çš„Digraphå¯¹è±¡\n```\n\n#### ç³»ç»Ÿæ¶æ„å›¾æ¨¡æ¿\n```python\nfrom graphviz import Digraph\n\ndef create_system_architecture():\n    dot = Digraph('SystemArchitecture', format='png')\n    dot.attr(rankdir='TB', size='14,10', compound='true')\n    \n    # å‰ç«¯å±‚é›†ç¾¤\n    with dot.subgraph(name='cluster_frontend') as c:\n        c.attr(label='å‰ç«¯å±‚', style='filled', color='lightgrey', fontsize='16')\n        c.node('web_app', 'Webåº”ç”¨', shape='box3d', style='filled', fillcolor='lightblue')\n        c.node('mobile_app', 'ç§»åŠ¨ç«¯', shape='box3d', style='filled', fillcolor='lightblue')\n        c.node('api_gateway', 'APIç½‘å…³', shape='pentagon', style='filled', fillcolor='lightyellow')\n        \n    # åç«¯æœåŠ¡é›†ç¾¤\n    with dot.subgraph(name='cluster_backend') as c:\n        c.attr(label='åç«¯æœåŠ¡å±‚', style='filled', color='lightblue', fontsize='16')\n        c.node('auth_service', 'è®¤è¯æœåŠ¡', shape='component', style='filled', fillcolor='lightgreen')\n        c.node('user_service', 'ç”¨æˆ·æœåŠ¡', shape='component', style='filled', fillcolor='lightgreen')\n        c.node('product_service', 'äº§å“æœåŠ¡', shape='component', style='filled', fillcolor='lightgreen')\n        c.node('order_service', 'è®¢å•æœåŠ¡', shape='component', style='filled', fillcolor='lightgreen')\n        \n    # æ•°æ®å±‚é›†ç¾¤\n    with dot.subgraph(name='cluster_data') as c:\n        c.attr(label='æ•°æ®å­˜å‚¨å±‚', style='filled', color='lightgreen', fontsize='16')\n        c.node('main_db', 'ä¸»æ•°æ®åº“\\n(PostgreSQL)', shape='cylinder', style='filled', fillcolor='lightyellow')\n        c.node('cache', 'ç¼“å­˜\\n(Redis)', shape='cylinder', style='filled', fillcolor='lightcoral')\n        c.node('search_engine', 'æœç´¢å¼•æ“\\n(Elasticsearch)', shape='cylinder', style='filled', fillcolor='lightskyblue')\n        \n    # è¿æ¥å…³ç³»\n    dot.edge('web_app', 'api_gateway', label='HTTPS')\n    dot.edge('mobile_app', 'api_gateway', label='REST API')\n    dot.edge('api_gateway', 'auth_service', label='éªŒè¯è¯·æ±‚')\n    dot.edge('api_gateway', 'user_service', label='ç”¨æˆ·æ•°æ®')\n    dot.edge('api_gateway', 'product_service', label='äº§å“æ•°æ®')\n    dot.edge('api_gateway', 'order_service', label='è®¢å•å¤„ç†')\n    \n    dot.edge('user_service', 'main_db', label='CRUD')\n    dot.edge('product_service', 'main_db', label='æŸ¥è¯¢')\n    dot.edge('order_service', 'main_db', label='äº‹åŠ¡')\n    dot.edge('user_service', 'cache', label='ä¼šè¯ç¼“å­˜')\n    dot.edge('product_service', 'search_engine', label='å…¨æ–‡æœç´¢')\n    \n    return dot\n\n# åˆ›å»ºå¹¶èµ‹å€¼ç»™å…¨å±€å˜é‡\nsystem_arch = create_system_architecture()\n```\n\n### NetworkX ç½‘ç»œå…³ç³»å›¾ï¼ˆé€šè¿‡Matplotlibæ˜¾ç¤ºï¼‰\n\n#### å®Œæ•´çš„æ•°æ®æµæ°´çº¿ç½‘ç»œå›¾\n```python\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef create_data_pipeline_diagram():\n    # åˆ›å»ºæœ‰å‘å›¾\n    G = nx.DiGraph()\n    \n    # æ·»åŠ èŠ‚ç‚¹ï¼ˆæ•°æ®æµæ°´çº¿å„é˜¶æ®µï¼‰\n    nodes = {\n        'æ•°æ®æº': {'type': 'source', 'color': 'lightgreen'},\n        'æ•°æ®é‡‡é›†': {'type': 'process', 'color': 'lightblue'},\n        'æ•°æ®æ¸…æ´—': {'type': 'process', 'color': 'lightblue'},\n        'æ•°æ®è½¬æ¢': {'type': 'process', 'color': 'lightblue'},\n        'æ•°æ®å­˜å‚¨': {'type': 'storage', 'color': 'lightyellow'},\n        'æ•°æ®åˆ†æ': {'type': 'analysis', 'color': 'lightcoral'},\n        'æ•°æ®å¯è§†åŒ–': {'type': 'visualization', 'color': 'lightskyblue'},\n        'ä¸šåŠ¡å†³ç­–': {'type': 'decision', 'color': 'lightpink'}\n    }\n    \n    for node, attrs in nodes.items():\n        G.add_node(node, **attrs)\n    \n    # æ·»åŠ è¾¹ï¼ˆæ•°æ®æµå‘ï¼‰\n    edges = [\n        ('æ•°æ®æº', 'æ•°æ®é‡‡é›†', 'åŸå§‹æ•°æ®'),\n        ('æ•°æ®é‡‡é›†', 'æ•°æ®æ¸…æ´—', 'é¢„å¤„ç†'),\n        ('æ•°æ®æ¸…æ´—', 'æ•°æ®è½¬æ¢', 'æ ¼å¼åŒ–'),\n        ('æ•°æ®è½¬æ¢', 'æ•°æ®å­˜å‚¨', 'æŒä¹…åŒ–'),\n        ('æ•°æ®å­˜å‚¨', 'æ•°æ®åˆ†æ', 'æŸ¥è¯¢'),\n        ('æ•°æ®åˆ†æ', 'æ•°æ®å¯è§†åŒ–', 'ç»“æœ'),\n        ('æ•°æ®å¯è§†åŒ–', 'ä¸šåŠ¡å†³ç­–', 'æ´å¯Ÿ')\n    ]\n    \n    for src, dst, label in edges:\n        G.add_edge(src, dst, label=label)\n    \n    # å¸ƒå±€ç®—æ³•\n    pos = nx.spring_layout(G, k=2, iterations=50, seed=42)\n    \n    # ç»˜å›¾\n    plt.figure(figsize=(16, 10))\n    \n    # æŒ‰ç±»å‹ç€è‰²èŠ‚ç‚¹\n    node_colors = [nodes[node]['color'] for node in G.nodes()]\n    node_sizes = [3000 if nodes[node]['type'] in ['source', 'decision'] else 2000 for node in G.nodes()]\n    \n    nx.draw_networkx_nodes(G, pos, \n                          node_color=node_colors,\n                          node_size=node_sizes,\n                          edgecolors='black',\n                          linewidths=2,\n                          alpha=0.9)\n    \n    # ç»˜åˆ¶è¾¹\n    nx.draw_networkx_edges(G, pos, \n                          edge_color='gray',\n                          arrows=True,\n                          arrowsize=20,\n                          width=2,\n                          alpha=0.7,\n                          connectionstyle=\"arc3,rad=0.1\")\n    \n    # ç»˜åˆ¶èŠ‚ç‚¹æ ‡ç­¾\n    nx.draw_networkx_labels(G, pos, \n                           font_size=12,\n                           font_weight='bold',\n                           font_family='WenQuanYi Micro Hei')\n    \n    # ç»˜åˆ¶è¾¹æ ‡ç­¾\n    edge_labels = nx.get_edge_attributes(G, 'label')\n    nx.draw_networkx_edge_labels(G, pos, \n                                edge_labels=edge_labels,\n                                font_size=10,\n                                label_pos=0.5,\n                                font_family='WenQuanYi Micro Hei')\n    \n    # è®¾ç½®æ ‡é¢˜å’Œç½‘æ ¼\n    plt.title('æ•°æ®æµæ°´çº¿æ¶æ„å›¾', fontsize=20, pad=30, fontweight='bold')\n    plt.axis('off')\n    plt.tight_layout()\n    \n    # ğŸ¯ å…³é”®ï¼šè§¦å‘Matplotlibè‡ªåŠ¨æ•è·\n    plt.show()\n\n# è°ƒç”¨å‡½æ•°ç”Ÿæˆå›¾è¡¨\ncreate_data_pipeline_diagram()\n```\n\n---\n\n## âš™ï¸ æ ·å¼é…ç½®ä¸å­—ä½“è®¾ç½®ï¼ˆé‡è¦ï¼‰\n\n### ä¸­æ–‡å­—ä½“è‡ªåŠ¨é…ç½®ï¼ˆç³»ç»Ÿå·²å¤„ç†ï¼‰\n```python\nimport matplotlib.pyplot as plt\n\n# ç³»ç»Ÿå·²è‡ªåŠ¨é…ç½®ä¸­æ–‡å­—ä½“ï¼Œæ— éœ€æ‰‹åŠ¨è®¾ç½®\n# å¦‚æœé‡åˆ°å­—ä½“é—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹é…ç½®ï¼š\n\nplt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei', 'WenQuanYi Zen Hei']\nplt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜\n\n# å¯é€‰ï¼šè®¾ç½®å…¨å±€æ ·å¼\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams.update({\n    'figure.figsize': (12, 8),\n    'font.size': 12,\n    'axes.titlesize': 16,\n    'axes.labelsize': 14,\n    'xtick.labelsize': 11,\n    'ytick.labelsize': 11,\n    'legend.fontsize': 11,\n    'grid.alpha': 0.3\n})\n\nprint(\"å­—ä½“é…ç½®å®Œæˆï¼Œå¯ä»¥å¼€å§‹ç»˜å›¾\")\n```\n\n---\n\n## ğŸ“ˆ è¿›é˜¶åŠŸèƒ½ï¼šäº¤äº’å¼å›¾è¡¨ä¸åŠ¨ç”»\n\n### ç®€å•åŠ¨ç”»ç¤ºä¾‹\n```python\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport numpy as np\n\nfig, ax = plt.subplots(figsize=(10, 6))\nx = np.linspace(0, 2*np.pi, 100)\nline, = ax.plot(x, np.sin(x))\n\ndef animate(i):\n    line.set_ydata(np.sin(x + i/10.0))\n    return line,\n\nani = animation.FuncAnimation(fig, animate, interval=50, blit=True)\nplt.title('æ­£å¼¦æ³¢åŠ¨ç”»æ¼”ç¤º')\nplt.xlabel('Xè½´')\nplt.ylabel('Yè½´')\nplt.grid(True)\nplt.show()\n```\n\n---\n\n## âš ï¸ é‡è¦æ³¨æ„äº‹é¡¹\n\n### âœ… å¿…é¡»åŒ…å«ï¼š\n1. `import matplotlib.pyplot as plt`\n2. æœ‰æ„ä¹‰çš„å›¾è¡¨æ ‡é¢˜`plt.title()`\n3. `plt.show()`ï¼ˆMatplotlibå’ŒNetworkXå¿…é¡»è°ƒç”¨ï¼‰\n\n### âŒ ç¦æ­¢æ“ä½œï¼š\n1. ä¸è¦ä½¿ç”¨`base64.b64encode()`æ‰‹åŠ¨ç¼–ç å›¾ç‰‡\n2. ä¸è¦åˆ›å»º`io.BytesIO()`å¯¹è±¡\n3. ä¸è¦æ‰‹åŠ¨æ„å»ºJSONè¾“å‡ºï¼ˆç³»ç»Ÿè‡ªåŠ¨å¤„ç†ï¼‰\n4. **Graphvizå›¾è¡¨å¿…é¡»èµ‹å€¼ç»™å…¨å±€å˜é‡**\n\n### ğŸ”§ æœ€ä½³å®è·µï¼š\n1. **æ–‡ä»¶è¯»å–ä¼˜å…ˆ**ï¼šå…ˆä»`/data`ç›®å½•è¯»å–ç”¨æˆ·ä¸Šä¼ çš„æ–‡ä»¶\n2. **æä¾›å¤‡ç”¨æ–¹æ¡ˆ**ï¼šå¦‚æœæ²¡æœ‰æ–‡ä»¶ï¼Œç”Ÿæˆç¤ºä¾‹å›¾è¡¨\n3. **æ¸…æ™°çš„æ ‡ç­¾**ï¼šä¸ºå›¾è¡¨æ·»åŠ æ¸…æ™°çš„æ ‡é¢˜å’Œåæ ‡è½´æ ‡ç­¾\n4. **åˆç†çš„å°ºå¯¸**ï¼š`figsize`å»ºè®®(12, 8)æˆ–(10, 6)\n5. **å¸ƒå±€ä¼˜åŒ–**ï¼šä½¿ç”¨`plt.tight_layout()`é˜²æ­¢æ ‡ç­¾é‡å \n\n---\n\n## ğŸ¯ ç°åœ¨å®Œå…¨åŒ¹é…åç«¯ï¼\n\n### ç»Ÿä¸€çš„è‡ªåŠ¨æ•è·æœºåˆ¶ï¼š\n\n| å›¾è¡¨ç±»å‹ | æ­£ç¡®ä½¿ç”¨æ–¹æ³• | ç¤ºä¾‹ä»£ç  |\n|---------|-------------|----------|\n| **Matplotlib** | `plt.show()` | `plt.plot(); plt.show()` |\n| **Graphviz** | åˆ›å»ºå¹¶èµ‹å€¼ç»™å…¨å±€å˜é‡ | `dot = Digraph(); ...` |\n| **NetworkX** | `plt.show()` | `nx.draw(); plt.show()` |\n\n### ç»ˆæå·¥ä½œæµæ¨¡æ¿ï¼š\n```python\n# 1. æ£€æŸ¥æ•°æ®æ–‡ä»¶\nimport os, pandas as pd\nfiles = os.listdir('/data') if os.path.exists('/data') else []\n\n# 2. è¯»å–æ•°æ®ï¼ˆå¦‚æœæœ‰æ–‡ä»¶ï¼‰\nif files and 'data.csv' in files:\n    df = pd.read_csv('/data/data.csv')\n    # ä½¿ç”¨çœŸå®æ•°æ®ç»˜å›¾\nelse:\n    # ç”Ÿæˆç¤ºä¾‹æ•°æ®ç»˜å›¾\n    pass\n\n# 3. ç”Ÿæˆå›¾è¡¨ï¼ˆé€‰æ‹©ä¸€ç§ç±»å‹ï¼‰\n# Matplotlib: plt.plot(); plt.show()\n# Graphviz: dot = Digraph(); (è‡ªåŠ¨æ•è·)\n# NetworkX: nx.draw(); plt.show()\n\n# 4. å›¾è¡¨ä¼šè¢«è‡ªåŠ¨æ•è·å¹¶æ˜¾ç¤ºç»™ç”¨æˆ·\n```\n\n### æ•…éšœæ’é™¤ï¼š\n1. **å›¾è¡¨æœªæ˜¾ç¤º**ï¼š\n   - æ£€æŸ¥æ˜¯å¦è°ƒç”¨äº†`plt.show()`\n   - æ£€æŸ¥Graphvizå¯¹è±¡æ˜¯å¦èµ‹å€¼ç»™å…¨å±€å˜é‡\n   - æŸ¥çœ‹ç³»ç»Ÿé”™è¯¯è¾“å‡º\n\n2. **ä¸­æ–‡ä¹±ç **ï¼š\n   - ç³»ç»Ÿå·²å†…ç½®å­—ä½“ä¿®å¤\n   - å¯æ‰‹åŠ¨è®¾ç½®å­—ä½“é…ç½®\n\n3. **æ–‡ä»¶è¯»å–å¤±è´¥**ï¼š\n   - ç¡®ä¿æ–‡ä»¶å·²é€šè¿‡ä¸Šä¼ åŠŸèƒ½ä¸Šä¼ \n   - æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼š`/data/æ–‡ä»¶å`\n\n**è®°ä½**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨æ•è·æ‰€æœ‰å›¾è¡¨å¹¶è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼ï¼Œæ‚¨åªéœ€è¦ä¸“æ³¨äºç»˜å›¾é€»è¾‘å’Œæ•°æ®åˆ†æï¼\n\n\n### ğŸ“– ml_workflow\n\n# æœºå™¨å­¦ä¹ å·¥ä½œæµæŒ‡å— (v2.3)\r\n\r\n## ğŸ¯ å·¥å…·æ¦‚è¿°\r\n**åŠŸèƒ½**ï¼šæœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒã€è¯„ä¼°ã€ç»Ÿè®¡åˆ†æå’Œå¯è§†åŒ–  \r\n**è¾“å‡ºåŸåˆ™**ï¼šç›´æ¥æ‰“å°ç»“æœï¼Œç³»ç»Ÿè‡ªåŠ¨å¤„ç†è¾“å‡ºæ ¼å¼  \r\n\r\n**æ–°å¢åŠŸèƒ½**ï¼š\r\n- âœ… **XGBoost 1.7.6**ï¼šé«˜æ€§èƒ½æ¢¯åº¦æå‡æ ‘æ¨¡å‹\r\n- âœ… **pmdarima 2.0.4**ï¼šè‡ªåŠ¨åŒ–ARIMAæ—¶é—´åºåˆ—å»ºæ¨¡\r\n- âœ… å¢å¼ºçš„æ—¶é—´åºåˆ—åˆ†æèƒ½åŠ›\r\n- âœ… éçº¿æ€§æ¨¡å‹ä¸çº¿æ€§æ¨¡å‹çš„å¯¹æ¯”åˆ†æ\r\n\r\n## ğŸ“Š åŸºç¡€æœºå™¨å­¦ä¹ æ¨¡æ¿\r\n\r\n### æ•°æ®å‡†å¤‡ä¸é¢„å¤„ç†\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\n\r\ndef prepare_ml_data():\r\n    \"\"\"æœºå™¨å­¦ä¹ æ•°æ®å‡†å¤‡ç¤ºä¾‹\"\"\"\r\n    \r\n    # åˆ›å»ºç¤ºä¾‹æ•°æ®é›†\r\n    np.random.seed(42)\r\n    n_samples = 1000\r\n    \r\n    # å›å½’é—®é¢˜æ•°æ®\r\n    X_reg = np.random.normal(0, 1, (n_samples, 5))\r\n    y_reg = 2 * X_reg[:, 0] + 1.5 * X_reg[:, 1] - X_reg[:, 2] + np.random.normal(0, 0.5, n_samples)\r\n    \r\n    # åˆ†ç±»é—®é¢˜æ•°æ®\r\n    X_clf = np.random.normal(0, 1, (n_samples, 4))\r\n    y_clf = (X_clf[:, 0] + X_clf[:, 1] > 0).astype(int)\r\n    \r\n    print(\"=== æ•°æ®å‡†å¤‡å®Œæˆ ===\")\r\n    print(f\"æ ·æœ¬æ•°é‡: {n_samples}\")\r\n    print(f\"å›å½’ç‰¹å¾ç»´åº¦: {X_reg.shape[1]}\")\r\n    print(f\"åˆ†ç±»ç‰¹å¾ç»´åº¦: {X_clf.shape[1]}\")\r\n    print(f\"åˆ†ç±»æ ‡ç­¾åˆ†å¸ƒ: {np.unique(y_clf, return_counts=True)}\")\r\n    \r\n    return X_reg, y_reg, X_clf, y_clf\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\n# X_reg, y_reg, X_clf, y_clf = prepare_ml_data()\r\n```\r\n\r\n### æ ‡å‡†æœºå™¨å­¦ä¹ å·¥ä½œæµ\r\n```python\r\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\r\nfrom sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report\r\nfrom sklearn.model_selection import cross_val_score\r\n\r\ndef standard_ml_pipeline(X, y, problem_type='regression'):\r\n    \"\"\"æ ‡å‡†æœºå™¨å­¦ä¹ æµç¨‹\"\"\"\r\n    \r\n    print(f\"=== å¼€å§‹ {problem_type} æ¨¡å‹è®­ç»ƒ ===\")\r\n    \r\n    # æ•°æ®åˆ†å‰²\r\n    X_train, X_test, y_train, y_test = train_test_split(\r\n        X, y, test_size=0.2, random_state=42,\r\n        stratify=y if problem_type == 'classification' else None\r\n    )\r\n    \r\n    print(f\"è®­ç»ƒé›†å¤§å°: {X_train.shape}\")\r\n    print(f\"æµ‹è¯•é›†å¤§å°: {X_test.shape}\")\r\n    \r\n    # ç‰¹å¾æ ‡å‡†åŒ–\r\n    scaler = StandardScaler()\r\n    X_train_scaled = scaler.fit_transform(X_train)\r\n    X_test_scaled = scaler.transform(X_test)\r\n    \r\n    # é€‰æ‹©æ¨¡å‹\r\n    if problem_type == 'regression':\r\n        model = RandomForestRegressor(n_estimators=100, random_state=42)\r\n    else:\r\n        model = RandomForestClassifier(n_estimators=100, random_state=42)\r\n    \r\n    # è®­ç»ƒæ¨¡å‹\r\n    model.fit(X_train_scaled, y_train)\r\n    \r\n    # é¢„æµ‹\r\n    y_pred = model.predict(X_test_scaled)\r\n    \r\n    # æ¨¡å‹è¯„ä¼°\r\n    if problem_type == 'regression':\r\n        mse = mean_squared_error(y_test, y_pred)\r\n        rmse = np.sqrt(mse)\r\n        r2 = r2_score(y_test, y_pred)\r\n        \r\n        print(f\"å›å½’æ¨¡å‹æ€§èƒ½:\")\r\n        print(f\"  MSE: {mse:.4f}\")\r\n        print(f\"  RMSE: {rmse:.4f}\")\r\n        print(f\"  RÂ²: {r2:.4f}\")\r\n        \r\n        metrics = {'mse': mse, 'rmse': rmse, 'r2': r2}\r\n    else:\r\n        accuracy = accuracy_score(y_test, y_pred)\r\n        print(f\"åˆ†ç±»æ¨¡å‹æ€§èƒ½:\")\r\n        print(f\"  å‡†ç¡®ç‡: {accuracy:.4f}\")\r\n        print(\"\\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:\")\r\n        print(classification_report(y_test, y_pred))\r\n        \r\n        metrics = {'accuracy': accuracy}\r\n    \r\n    # äº¤å‰éªŒè¯\r\n    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, \r\n                               scoring='r2' if problem_type == 'regression' else 'accuracy')\r\n    print(f\"äº¤å‰éªŒè¯å¹³å‡å¾—åˆ†: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})\")\r\n    \r\n    return {\r\n        'model': model,\r\n        'metrics': metrics,\r\n        'X_test': X_test,\r\n        'y_test': y_test,\r\n        'y_pred': y_pred,\r\n        'cv_scores': cv_scores\r\n    }\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\n# X_reg, y_reg, X_clf, y_clf = prepare_ml_data()\r\n# regression_results = standard_ml_pipeline(X_reg, y_reg, 'regression')\r\n# classification_results = standard_ml_pipeline(X_clf, y_clf, 'classification')\r\n```\r\n\r\n## ğŸ“ˆ å›å½’åˆ†æå®Œæ•´å·¥ä½œæµ\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.ensemble import RandomForestRegressor\r\nfrom sklearn.metrics import mean_squared_error, r2_score\r\n\r\ndef complete_regression_analysis():\r\n    \"\"\"å®Œæ•´çš„å›å½’åˆ†æå·¥ä½œæµ\"\"\"\r\n    \r\n    print(\"=== å¼€å§‹å›å½’åˆ†æ ===\")\r\n    \r\n    # 1. æ•°æ®ç”Ÿæˆ\r\n    np.random.seed(42)\r\n    n_samples = 500\r\n    \r\n    # åˆ›å»ºæœ‰æ„ä¹‰çš„ç‰¹å¾\r\n    feature1 = np.random.normal(50, 15, n_samples)  # å¹´é¾„\r\n    feature2 = np.random.normal(100, 25, n_samples) # æ”¶å…¥\r\n    feature3 = np.random.normal(10, 3, n_samples)   # æ•™è‚²å¹´é™\r\n    feature4 = np.random.normal(0, 1, n_samples)    # å™ªå£°ç‰¹å¾\r\n    \r\n    # åˆ›å»ºç›®æ ‡å˜é‡ï¼ˆæ¨¡æ‹Ÿæˆ¿ä»·ï¼‰\r\n    target = (50 * feature1 + 80 * feature2 + 5000 * feature3 + \r\n              10 * feature1 * feature3 + np.random.normal(0, 10000, n_samples))\r\n    \r\n    df = pd.DataFrame({\r\n        'å¹´é¾„': feature1,\r\n        'æ”¶å…¥': feature2,\r\n        'æ•™è‚²å¹´é™': feature3,\r\n        'å™ªå£°ç‰¹å¾': feature4,\r\n        'æˆ¿ä»·': target\r\n    })\r\n    \r\n    print(\"æ•°æ®åŸºæœ¬ä¿¡æ¯:\")\r\n    print(f\"æ•°æ®é›†å½¢çŠ¶: {df.shape}\")\r\n    print(f\"ç‰¹å¾åˆ—è¡¨: {list(df.columns[:-1])}\")\r\n    print(f\"ç›®æ ‡å˜é‡: {df.columns[-1]}\")\r\n    \r\n    # 2. æ•°æ®æ¢ç´¢\r\n    print(\"\\n=== æ•°æ®æ¢ç´¢ ===\")\r\n    print(\"æ•°å€¼ç‰¹å¾ç»Ÿè®¡:\")\r\n    print(df.describe())\r\n    \r\n    # ç›¸å…³æ€§åˆ†æ\r\n    correlation = df.corr()['æˆ¿ä»·'].sort_values(ascending=False)\r\n    print(\"\\nç‰¹å¾ä¸ç›®æ ‡å˜é‡ç›¸å…³æ€§:\")\r\n    for feature, corr in correlation.items():\r\n        if feature != 'æˆ¿ä»·':\r\n            print(f\"  {feature}: {corr:.3f}\")\r\n    \r\n    # 3. æ¨¡å‹è®­ç»ƒ\r\n    X = df.drop('æˆ¿ä»·', axis=1)\r\n    y = df['æˆ¿ä»·']\r\n    \r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\r\n    \r\n    model = RandomForestRegressor(n_estimators=100, random_state=42)\r\n    model.fit(X_train, y_train)\r\n    \r\n    y_pred = model.predict(X_test)\r\n    \r\n    # 4. æ¨¡å‹è¯„ä¼°\r\n    mse = mean_squared_error(y_test, y_pred)\r\n    rmse = np.sqrt(mse)\r\n    r2 = r2_score(y_test, y_pred)\r\n    \r\n    print(f\"\\n=== æ¨¡å‹æ€§èƒ½ ===\")\r\n    print(f\"å‡æ–¹è¯¯å·® (MSE): {mse:,.2f}\")\r\n    print(f\"å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse:,.2f}\")\r\n    print(f\"å†³å®šç³»æ•° (RÂ²): {r2:.4f}\")\r\n    \r\n    # 5. ç‰¹å¾é‡è¦æ€§\r\n    feature_importance = pd.DataFrame({\r\n        'ç‰¹å¾': X.columns,\r\n        'é‡è¦æ€§': model.feature_importances_\r\n    }).sort_values('é‡è¦æ€§', ascending=False)\r\n    \r\n    print(f\"\\n=== ç‰¹å¾é‡è¦æ€§ ===\")\r\n    for _, row in feature_importance.iterrows():\r\n        print(f\"  {row['ç‰¹å¾']}: {row['é‡è¦æ€§']:.4f}\")\r\n    \r\n    # 6. å¯è§†åŒ–åˆ†æ\r\n    plt.figure(figsize=(15, 10))\r\n    \r\n    # å®é™…å€¼ vs é¢„æµ‹å€¼\r\n    plt.subplot(2, 3, 1)\r\n    plt.scatter(y_test, y_pred, alpha=0.6)\r\n    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\r\n    plt.xlabel('å®é™…å€¼')\r\n    plt.ylabel('é¢„æµ‹å€¼')\r\n    plt.title(f'é¢„æµ‹æ•ˆæœ (RÂ² = {r2:.3f})')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # æ®‹å·®åˆ†æ\r\n    plt.subplot(2, 3, 2)\r\n    residuals = y_test - y_pred\r\n    plt.scatter(y_pred, residuals, alpha=0.6)\r\n    plt.axhline(y=0, color='r', linestyle='--')\r\n    plt.xlabel('é¢„æµ‹å€¼')\r\n    plt.ylabel('æ®‹å·®')\r\n    plt.title('æ®‹å·®åˆ†æ')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–\r\n    plt.subplot(2, 3, 3)\r\n    top_features = feature_importance.head(5)\r\n    plt.barh(top_features['ç‰¹å¾'], top_features['é‡è¦æ€§'])\r\n    plt.xlabel('é‡è¦æ€§')\r\n    plt.title('Top 5 ç‰¹å¾é‡è¦æ€§')\r\n    plt.gca().invert_yaxis()\r\n    \r\n    # è¯¯å·®åˆ†å¸ƒ\r\n    plt.subplot(2, 3, 4)\r\n    plt.hist(residuals, bins=30, alpha=0.7, edgecolor='black')\r\n    plt.xlabel('æ®‹å·®')\r\n    plt.ylabel('é¢‘æ•°')\r\n    plt.title('è¯¯å·®åˆ†å¸ƒ')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # ç›¸å¯¹è¯¯å·®\r\n    plt.subplot(2, 3, 5)\r\n    relative_error = np.abs(residuals / y_test) * 100\r\n    plt.hist(relative_error, bins=30, alpha=0.7, edgecolor='black')\r\n    plt.xlabel('ç›¸å¯¹è¯¯å·® (%)')\r\n    plt.ylabel('é¢‘æ•°')\r\n    plt.title('ç›¸å¯¹è¯¯å·®åˆ†å¸ƒ')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # é¢„æµ‹è¯¯å·®ç®±çº¿å›¾\r\n    plt.subplot(2, 3, 6)\r\n    plt.boxplot(relative_error)\r\n    plt.ylabel('ç›¸å¯¹è¯¯å·® (%)')\r\n    plt.title('é¢„æµ‹è¯¯å·®åˆ†å¸ƒ')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    plt.tight_layout()\r\n    plt.show()\r\n    \r\n    # 7. æ¨¡å‹è§£é‡Š\r\n    print(f\"\\n=== æ¨¡å‹è§£é‡Š ===\")\r\n    print(f\"æ¨¡å‹æ€§èƒ½: {'ä¼˜ç§€' if r2 > 0.8 else 'è‰¯å¥½' if r2 > 0.6 else 'ä¸€èˆ¬'}\")\r\n    print(f\"æœ€é‡è¦çš„ç‰¹å¾: {feature_importance.iloc[0]['ç‰¹å¾']}\")\r\n    print(f\"å»ºè®®: å…³æ³¨{feature_importance.iloc[0]['ç‰¹å¾']}å’Œ{feature_importance.iloc[1]['ç‰¹å¾']}çš„ä¼˜åŒ–\")\r\n    \r\n    return {\r\n        'model': model,\r\n        'metrics': {'mse': mse, 'rmse': rmse, 'r2': r2},\r\n        'feature_importance': feature_importance,\r\n        'predictions': y_pred\r\n    }\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\n# regression_results = complete_regression_analysis()\r\n```\r\n\r\n## ğŸ” åˆ†ç±»åˆ†æå®Œæ•´å·¥ä½œæµ\r\n\r\n```python\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\r\nfrom sklearn.datasets import make_classification\r\n\r\ndef complete_classification_analysis():\r\n    \"\"\"å®Œæ•´çš„åˆ†ç±»åˆ†æå·¥ä½œæµ\"\"\"\r\n    \r\n    print(\"=== å¼€å§‹åˆ†ç±»åˆ†æ ===\")\r\n    \r\n    # 1. æ•°æ®ç”Ÿæˆ\r\n    X, y = make_classification(\r\n        n_samples=1000,\r\n        n_features=8,\r\n        n_informative=5,\r\n        n_redundant=2,\r\n        n_classes=3,\r\n        random_state=42\r\n    )\r\n    \r\n    feature_names = [f'ç‰¹å¾_{i+1}' for i in range(X.shape[1])]\r\n    df = pd.DataFrame(X, columns=feature_names)\r\n    df['ç±»åˆ«'] = y\r\n    \r\n    print(\"æ•°æ®åŸºæœ¬ä¿¡æ¯:\")\r\n    print(f\"æ•°æ®é›†å½¢çŠ¶: {df.shape}\")\r\n    print(f\"ç‰¹å¾æ•°é‡: {X.shape[1]}\")\r\n    print(f\"ç±»åˆ«æ•°é‡: {len(np.unique(y))}\")\r\n    print(f\"ç±»åˆ«åˆ†å¸ƒ: {np.unique(y, return_counts=True)}\")\r\n    \r\n    # 2. æ•°æ®æ¢ç´¢\r\n    print(\"\\n=== æ•°æ®æ¢ç´¢ ===\")\r\n    print(\"æ•°å€¼ç‰¹å¾ç»Ÿè®¡:\")\r\n    print(df.describe())\r\n    \r\n    # 3. æ¨¡å‹è®­ç»ƒ\r\n    X_data = df.drop('ç±»åˆ«', axis=1)\r\n    y_data = df['ç±»åˆ«']\r\n    \r\n    X_train, X_test, y_train, y_test = train_test_split(\r\n        X_data, y_data, test_size=0.2, random_state=42, stratify=y_data\r\n    )\r\n    \r\n    model = RandomForestClassifier(n_estimators=100, random_state=42)\r\n    model.fit(X_train, y_train)\r\n    \r\n    y_pred = model.predict(X_test)\r\n    \r\n    # 4. æ¨¡å‹è¯„ä¼°\r\n    accuracy = accuracy_score(y_test, y_pred)\r\n    \r\n    print(f\"\\n=== æ¨¡å‹æ€§èƒ½ ===\")\r\n    print(f\"å‡†ç¡®ç‡: {accuracy:.4f}\")\r\n    print(\"\\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:\")\r\n    print(classification_report(y_test, y_pred))\r\n    \r\n    # 5. ç‰¹å¾é‡è¦æ€§\r\n    feature_importance = pd.DataFrame({\r\n        'ç‰¹å¾': X_data.columns,\r\n        'é‡è¦æ€§': model.feature_importances_\r\n    }).sort_values('é‡è¦æ€§', ascending=False)\r\n    \r\n    print(f\"\\n=== ç‰¹å¾é‡è¦æ€§ ===\")\r\n    for _, row in feature_importance.iterrows():\r\n        print(f\"  {row['ç‰¹å¾']}: {row['é‡è¦æ€§']:.4f}\")\r\n    \r\n    # 6. å¯è§†åŒ–åˆ†æ\r\n    plt.figure(figsize=(15, 10))\r\n    \r\n    # æ··æ·†çŸ©é˜µ\r\n    plt.subplot(2, 3, 1)\r\n    cm = confusion_matrix(y_test, y_pred)\r\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\r\n    plt.xlabel('é¢„æµ‹æ ‡ç­¾')\r\n    plt.ylabel('çœŸå®æ ‡ç­¾')\r\n    plt.title('æ··æ·†çŸ©é˜µ')\r\n    \r\n    # ç‰¹å¾é‡è¦æ€§\r\n    plt.subplot(2, 3, 2)\r\n    top_features = feature_importance.head(8)\r\n    plt.barh(top_features['ç‰¹å¾'], top_features['é‡è¦æ€§'])\r\n    plt.xlabel('é‡è¦æ€§')\r\n    plt.title('ç‰¹å¾é‡è¦æ€§æ’å')\r\n    plt.gca().invert_yaxis()\r\n    \r\n    # ç±»åˆ«åˆ†å¸ƒ\r\n    plt.subplot(2, 3, 3)\r\n    unique, counts = np.unique(y, return_counts=True)\r\n    plt.pie(counts, labels=[f'ç±»åˆ« {cls}' for cls in unique], autopct='%1.1f%%')\r\n    plt.title('ç±»åˆ«åˆ†å¸ƒ')\r\n    \r\n    # åˆ†ç±»æŠ¥å‘Šçƒ­åŠ›å›¾\r\n    plt.subplot(2, 3, 4)\r\n    report_dict = classification_report(y_test, y_pred, output_dict=True)\r\n    report_df = pd.DataFrame(report_dict).transpose().iloc[:-3, :-1]\r\n    sns.heatmap(report_df, annot=True, cmap='YlOrRd', fmt='.3f')\r\n    plt.title('åˆ†ç±»æŒ‡æ ‡çƒ­åŠ›å›¾')\r\n    \r\n    # å­¦ä¹ æ›²çº¿ï¼ˆç®€åŒ–ç‰ˆï¼‰\r\n    plt.subplot(2, 3, 5)\r\n    train_sizes = np.linspace(0.1, 1.0, 10)\r\n    train_scores = []\r\n    test_scores = []\r\n    \r\n    for size in train_sizes:\r\n        n_train = int(size * len(X_train))\r\n        X_train_sub = X_train.iloc[:n_train]\r\n        y_train_sub = y_train.iloc[:n_train]\r\n        \r\n        model_temp = RandomForestClassifier(n_estimators=50, random_state=42)\r\n        model_temp.fit(X_train_sub, y_train_sub)\r\n        \r\n        train_score = model_temp.score(X_train_sub, y_train_sub)\r\n        test_score = model_temp.score(X_test, y_test)\r\n        \r\n        train_scores.append(train_score)\r\n        test_scores.append(test_score)\r\n    \r\n    plt.plot(train_sizes, train_scores, 'o-', label='è®­ç»ƒå¾—åˆ†')\r\n    plt.plot(train_sizes, test_scores, 'o-', label='æµ‹è¯•å¾—åˆ†')\r\n    plt.xlabel('è®­ç»ƒæ ·æœ¬æ¯”ä¾‹')\r\n    plt.ylabel('å‡†ç¡®ç‡')\r\n    plt.title('å­¦ä¹ æ›²çº¿')\r\n    plt.legend()\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # ç±»åˆ«é¢„æµ‹åˆ†å¸ƒ\r\n    plt.subplot(2, 3, 6)\r\n    pred_counts = pd.Series(y_pred).value_counts().sort_index()\r\n    true_counts = pd.Series(y_test).value_counts().sort_index()\r\n    \r\n    x = np.arange(len(true_counts))\r\n    width = 0.35\r\n    \r\n    plt.bar(x - width/2, true_counts, width, label='çœŸå®åˆ†å¸ƒ', alpha=0.7)\r\n    plt.bar(x + width/2, pred_counts, width, label='é¢„æµ‹åˆ†å¸ƒ', alpha=0.7)\r\n    plt.xlabel('ç±»åˆ«')\r\n    plt.ylabel('æ ·æœ¬æ•°')\r\n    plt.title('ç±»åˆ«åˆ†å¸ƒå¯¹æ¯”')\r\n    plt.legend()\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    plt.tight_layout()\r\n    plt.show()\r\n    \r\n    # 7. æ¨¡å‹è§£é‡Š\r\n    print(f\"\\n=== æ¨¡å‹è§£é‡Š ===\")\r\n    print(f\"æ¨¡å‹æ€§èƒ½: {'ä¼˜ç§€' if accuracy > 0.9 else 'è‰¯å¥½' if accuracy > 0.8 else 'ä¸€èˆ¬'}\")\r\n    print(f\"æœ€é‡è¦çš„ç‰¹å¾: {feature_importance.iloc[0]['ç‰¹å¾']}\")\r\n    print(f\"æœ€å®¹æ˜“æ··æ·†çš„ç±»åˆ«: æŸ¥çœ‹æ··æ·†çŸ©é˜µå¯¹è§’çº¿å¤–çš„æœ€å¤§å€¼\")\r\n    \r\n    return {\r\n        'model': model,\r\n        'metrics': {'accuracy': accuracy},\r\n        'feature_importance': feature_importance,\r\n        'predictions': y_pred\r\n    }\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\n# classification_results = complete_classification_analysis()\r\n```\r\n\r\n## ğŸ“Š ç»Ÿè®¡å»ºæ¨¡åˆ†æ\r\n\r\n```python\r\nimport statsmodels.api as sm\r\nimport statsmodels.formula.api as smf\r\n\r\ndef statistical_modeling_analysis():\r\n    \"\"\"ç»Ÿè®¡å»ºæ¨¡åˆ†æ\"\"\"\r\n    \r\n    print(\"=== å¼€å§‹ç»Ÿè®¡å»ºæ¨¡åˆ†æ ===\")\r\n    \r\n    # åˆ›å»ºç¤ºä¾‹æ•°æ®\r\n    np.random.seed(42)\r\n    n_samples = 200\r\n    \r\n    data = pd.DataFrame({\r\n        'å¹¿å‘ŠæŠ•å…¥': np.random.normal(1000, 300, n_samples),\r\n        'ä»·æ ¼': np.random.normal(50, 15, n_samples),\r\n        'ä¿ƒé”€æ´»åŠ¨': np.random.choice([0, 1], n_samples, p=[0.7, 0.3]),\r\n        'å­£èŠ‚æ€§': np.random.choice([0, 1], n_samples, p=[0.5, 0.5])\r\n    })\r\n    \r\n    # ç”Ÿæˆé”€å”®é¢ï¼ˆä¸ç‰¹å¾æœ‰çœŸå®å…³ç³»ï¼‰\r\n    data['é”€å”®é¢'] = (\r\n        500 + 0.8 * data['å¹¿å‘ŠæŠ•å…¥'] - 5 * data['ä»·æ ¼'] + \r\n        200 * data['ä¿ƒé”€æ´»åŠ¨'] + 150 * data['å­£èŠ‚æ€§'] + \r\n        np.random.normal(0, 100, n_samples)\r\n    )\r\n    \r\n    print(\"æ•°æ®åŸºæœ¬ä¿¡æ¯:\")\r\n    print(f\"æ ·æœ¬æ•°é‡: {len(data)}\")\r\n    print(f\"ç‰¹å¾: {list(data.columns[:-1])}\")\r\n    print(\"\\næ•°æ®æè¿°:\")\r\n    print(data.describe())\r\n    \r\n    # 1. OLS å›å½’åˆ†æ\r\n    print(\"\\n=== OLS å›å½’åˆ†æ ===\")\r\n    model = smf.ols('é”€å”®é¢ ~ å¹¿å‘ŠæŠ•å…¥ + ä»·æ ¼ + ä¿ƒé”€æ´»åŠ¨ + å­£èŠ‚æ€§', data=data).fit()\r\n    \r\n    print(\"å›å½’ç»“æœæ‘˜è¦:\")\r\n    print(model.summary())\r\n    \r\n    # 2. å…³é”®ç»Ÿè®¡æŒ‡æ ‡\r\n    print(f\"\\n=== å…³é”®ç»Ÿè®¡æŒ‡æ ‡ ===\")\r\n    print(f\"RÂ²: {model.rsquared:.4f}\")\r\n    print(f\"è°ƒæ•´RÂ²: {model.rsquared_adj:.4f}\")\r\n    print(f\"Fç»Ÿè®¡é‡: {model.fvalue:.2f}\")\r\n    print(f\"Fç»Ÿè®¡é‡på€¼: {model.f_pvalue:.4f}\")\r\n    \r\n    # 3. ç³»æ•°è§£é‡Š\r\n    print(f\"\\n=== ç³»æ•°è§£é‡Š ===\")\r\n    for feature, coef in model.params.items():\r\n        p_value = model.pvalues[feature]\r\n        significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"\"\r\n        print(f\"{feature}: {coef:.2f} {significance} (på€¼: {p_value:.4f})\")\r\n    \r\n    # 4. æ®‹å·®åˆ†æ\r\n    print(f\"\\n=== æ®‹å·®åˆ†æ ===\")\r\n    residuals = model.resid\r\n    print(f\"æ®‹å·®å‡å€¼: {residuals.mean():.4f}\")\r\n    print(f\"æ®‹å·®æ ‡å‡†å·®: {residuals.std():.4f}\")\r\n    \r\n    # 5. å¯è§†åŒ–åˆ†æ\r\n    plt.figure(figsize=(15, 10))\r\n    \r\n    # å®é™…å€¼ vs é¢„æµ‹å€¼\r\n    plt.subplot(2, 3, 1)\r\n    y_pred_ols = model.predict(data[['å¹¿å‘ŠæŠ•å…¥', 'ä»·æ ¼', 'ä¿ƒé”€æ´»åŠ¨', 'å­£èŠ‚æ€§']])\r\n    plt.scatter(data['é”€å”®é¢'], y_pred_ols, alpha=0.6)\r\n    plt.plot([data['é”€å”®é¢'].min(), data['é”€å”®é¢'].max()], \r\n             [data['é”€å”®é¢'].min(), data['é”€å”®é¢'].max()], 'r--', lw=2)\r\n    plt.xlabel('å®é™…é”€å”®é¢')\r\n    plt.ylabel('é¢„æµ‹é”€å”®é¢')\r\n    plt.title(f'OLSé¢„æµ‹æ•ˆæœ (RÂ² = {model.rsquared:.3f})')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # æ®‹å·®å›¾\r\n    plt.subplot(2, 3, 2)\r\n    plt.scatter(y_pred_ols, residuals, alpha=0.6)\r\n    plt.axhline(y=0, color='r', linestyle='--')\r\n    plt.xlabel('é¢„æµ‹å€¼')\r\n    plt.ylabel('æ®‹å·®')\r\n    plt.title('æ®‹å·®åˆ†æ')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # Q-Qå›¾\r\n    plt.subplot(2, 3, 3)\r\n    sm.qqplot(residuals, line='45', ax=plt.gca())\r\n    plt.title('Q-Qå›¾ï¼ˆæ®‹å·®æ­£æ€æ€§æ£€éªŒï¼‰')\r\n    \r\n    # ç‰¹å¾ä¸ç›®æ ‡å˜é‡å…³ç³»\r\n    plt.subplot(2, 3, 4)\r\n    plt.scatter(data['å¹¿å‘ŠæŠ•å…¥'], data['é”€å”®é¢'], alpha=0.6)\r\n    plt.xlabel('å¹¿å‘ŠæŠ•å…¥')\r\n    plt.ylabel('é”€å”®é¢')\r\n    plt.title('å¹¿å‘ŠæŠ•å…¥ vs é”€å”®é¢')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    plt.subplot(2, 3, 5)\r\n    plt.scatter(data['ä»·æ ¼'], data['é”€å”®é¢'], alpha=0.6)\r\n    plt.xlabel('ä»·æ ¼')\r\n    plt.ylabel('é”€å”®é¢')\r\n    plt.title('ä»·æ ¼ vs é”€å”®é¢')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # ç³»æ•°å¯è§†åŒ–\r\n    plt.subplot(2, 3, 6)\r\n    coefficients = model.params.iloc[1:]  # æ’é™¤æˆªè·é¡¹\r\n    colors = ['green' if p < 0.05 else 'red' for p in model.pvalues.iloc[1:]]\r\n    plt.barh(coefficients.index, coefficients.values, color=colors)\r\n    plt.axvline(x=0, color='black', linestyle='-')\r\n    plt.xlabel('ç³»æ•°å€¼')\r\n    plt.title('ç‰¹å¾ç³»æ•°ï¼ˆç»¿è‰²è¡¨ç¤ºæ˜¾è‘—ï¼‰')\r\n    \r\n    plt.tight_layout()\r\n    plt.show()\r\n    \r\n    # 6. ä¸šåŠ¡è§£é‡Š\r\n    print(f\"\\n=== ä¸šåŠ¡è§£é‡Š ===\")\r\n    print(f\"æ¨¡å‹è§£é‡ŠåŠ›: {'å¼º' if model.rsquared > 0.7 else 'ä¸­ç­‰' if model.rsquared > 0.5 else 'å¼±'}\")\r\n    \r\n    significant_features = []\r\n    for feature in model.params.index[1:]:  # æ’é™¤æˆªè·\r\n        if model.pvalues[feature] < 0.05:\r\n            significant_features.append(feature)\r\n    \r\n    if significant_features:\r\n        print(f\"æ˜¾è‘—å½±å“ç‰¹å¾: {', '.join(significant_features)}\")\r\n    else:\r\n        print(\"æ²¡æœ‰å‘ç°ç»Ÿè®¡æ˜¾è‘—çš„ç‰¹å¾\")\r\n    \r\n    return {\r\n        'model': model,\r\n        'rsquared': model.rsquared,\r\n        'significant_features': significant_features,\r\n        'residuals': residuals\r\n    }\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\n# stats_results = statistical_modeling_analysis()\r\n```\r\n\r\n## â° æ—¶é—´åºåˆ—åˆ†æï¼ˆv2.3æ–°å¢ï¼‰\r\n\r\n### ä½¿ç”¨pmdarimaè¿›è¡Œè‡ªåŠ¨åŒ–ARIMAå»ºæ¨¡\r\n\r\n```python\r\nfrom pmdarima import auto_arima\r\nimport xgboost as xgb\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\r\n\r\ndef time_series_arima_analysis(series, seasonal_period=7, forecast_steps=30):\r\n    \"\"\"è‡ªåŠ¨åŒ–ARIMAæ—¶é—´åºåˆ—åˆ†æ\"\"\"\r\n    \r\n    print(\"=== å¼€å§‹æ—¶é—´åºåˆ—ARIMAåˆ†æ ===\")\r\n    \r\n    # 1. æ•°æ®æ£€æŸ¥\r\n    print(f\"æ—¶é—´åºåˆ—é•¿åº¦: {len(series)}\")\r\n    print(f\"æ•°æ®ç±»å‹: {type(series)}\")\r\n    \r\n    # 2. è‡ªåŠ¨ARIMAå»ºæ¨¡\r\n    print(\"\\n=== è‡ªåŠ¨ARIMAå‚æ•°é€‰æ‹© ===\")\r\n    try:\r\n        model = auto_arima(\r\n            series,\r\n            seasonal=True,\r\n            m=seasonal_period,  # å­£èŠ‚æ€§å‘¨æœŸï¼ˆ7å¤©ä¸ºå‘¨å­£èŠ‚æ€§ï¼‰\r\n            stepwise=True,      # ä½¿ç”¨é€æ­¥æœç´¢ï¼ŒèŠ‚çœå†…å­˜\r\n            suppress_warnings=True,\r\n            error_action='ignore',\r\n            trace=True,         # æ˜¾ç¤ºæœç´¢è¿‡ç¨‹\r\n            random_state=42\r\n        )\r\n        \r\n        print(f\"æœ€ä½³ARIMAå‚æ•°: {model.order}\")\r\n        print(f\"æœ€ä½³å­£èŠ‚æ€§å‚æ•°: {model.seasonal_order}\")\r\n        print(f\"æ¨¡å‹AIC: {model.aic():.2f}\")\r\n        \r\n    except Exception as e:\r\n        print(f\"è‡ªåŠ¨ARIMAå¤±è´¥: {e}\")\r\n        return None\r\n    \r\n    # 3. æ¨¡å‹æ‘˜è¦\r\n    print(\"\\n=== æ¨¡å‹æ‘˜è¦ ===\")\r\n    print(model.summary())\r\n    \r\n    # 4. é¢„æµ‹\r\n    print(f\"\\n=== æœªæ¥{forecast_steps}æœŸé¢„æµ‹ ===\")\r\n    forecast, conf_int = model.predict(\r\n        n_periods=forecast_steps,\r\n        return_conf_int=True,\r\n        alpha=0.05  # 95%ç½®ä¿¡åŒºé—´\r\n    )\r\n    \r\n    # 5. æ¨¡å‹è¯„ä¼°ï¼ˆä½¿ç”¨è®­ç»ƒé›†æœ€åéƒ¨åˆ†ä½œä¸ºéªŒè¯ï¼‰\r\n    train_size = int(len(series) * 0.8)\r\n    train = series[:train_size]\r\n    test = series[train_size:]\r\n    \r\n    # åœ¨è®­ç»ƒé›†ä¸Šé‡æ–°æ‹Ÿåˆæ¨¡å‹\r\n    model.fit(train)\r\n    predictions = model.predict(n_periods=len(test))\r\n    \r\n    # è®¡ç®—æŒ‡æ ‡\r\n    mae = mean_absolute_error(test, predictions)\r\n    rmse = np.sqrt(mean_squared_error(test, predictions))\r\n    mape = np.mean(np.abs((test - predictions) / test)) * 100\r\n    \r\n    print(f\"\\n=== æ¨¡å‹æ€§èƒ½è¯„ä¼° ===\")\r\n    print(f\"MAE (å¹³å‡ç»å¯¹è¯¯å·®): {mae:.2f}\")\r\n    print(f\"RMSE (å‡æ–¹æ ¹è¯¯å·®): {rmse:.2f}\")\r\n    print(f\"MAPE (å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®): {mape:.2f}%\")\r\n    \r\n    # 6. å¯è§†åŒ–\r\n    plt.figure(figsize=(15, 10))\r\n    \r\n    # åŸå§‹åºåˆ—ä¸æ‹Ÿåˆå€¼\r\n    plt.subplot(2, 2, 1)\r\n    plt.plot(series.index, series, label='åŸå§‹åºåˆ—', alpha=0.7)\r\n    plt.plot(series.index, model.predict_in_sample(), label='æ‹Ÿåˆå€¼', alpha=0.7)\r\n    plt.xlabel('æ—¶é—´')\r\n    plt.ylabel('å€¼')\r\n    plt.title('åŸå§‹åºåˆ—ä¸æ¨¡å‹æ‹Ÿåˆ')\r\n    plt.legend()\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # æ®‹å·®åˆ†æ\r\n    plt.subplot(2, 2, 2)\r\n    residuals = series - model.predict_in_sample()\r\n    plt.plot(residuals.index, residuals, alpha=0.7)\r\n    plt.axhline(y=0, color='r', linestyle='--')\r\n    plt.xlabel('æ—¶é—´')\r\n    plt.ylabel('æ®‹å·®')\r\n    plt.title('æ¨¡å‹æ®‹å·®')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # é¢„æµ‹ç»“æœ\r\n    plt.subplot(2, 2, 3)\r\n    last_n = min(100, len(series))\r\n    plt.plot(series.index[-last_n:], series.values[-last_n:], label='å†å²æ•°æ®')\r\n    \r\n    # åˆ›å»ºæœªæ¥æ—¶é—´ç´¢å¼•\r\n    if hasattr(series.index, 'freq'):\r\n        future_index = pd.date_range(start=series.index[-1], periods=forecast_steps+1, freq=series.index.freq)[1:]\r\n    else:\r\n        future_index = range(len(series), len(series) + forecast_steps)\r\n    \r\n    plt.plot(future_index, forecast, label='é¢„æµ‹å€¼', color='red')\r\n    plt.fill_between(future_index, conf_int[:, 0], conf_int[:, 1], color='pink', alpha=0.3, label='95%ç½®ä¿¡åŒºé—´')\r\n    plt.xlabel('æ—¶é—´')\r\n    plt.ylabel('å€¼')\r\n    plt.title(f'æœªæ¥{forecast_steps}æœŸé¢„æµ‹')\r\n    plt.legend()\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # æ®‹å·®åˆ†å¸ƒ\r\n    plt.subplot(2, 2, 4)\r\n    plt.hist(residuals.dropna(), bins=30, alpha=0.7, edgecolor='black')\r\n    plt.xlabel('æ®‹å·®å€¼')\r\n    plt.ylabel('é¢‘æ•°')\r\n    plt.title('æ®‹å·®åˆ†å¸ƒ')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    plt.tight_layout()\r\n    plt.show()\r\n    \r\n    return {\r\n        'model': model,\r\n        'order': model.order,\r\n        'seasonal_order': model.seasonal_order,\r\n        'forecast': forecast,\r\n        'confidence_interval': conf_int,\r\n        'metrics': {'mae': mae, 'rmse': rmse, 'mape': mape},\r\n        'residuals': residuals\r\n    }\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\n# å‡è®¾dfæ˜¯ä¸€ä¸ªæ—¶é—´åºåˆ—DataFrameï¼Œindexä¸ºæ—¥æœŸï¼Œæœ‰ä¸€åˆ—'é”€å”®é¢'\r\n# results = time_series_arima_analysis(df['é”€å”®é¢'], seasonal_period=7, forecast_steps=30)\r\n```\r\n\r\n### ä½¿ç”¨XGBoostè¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹\r\n\r\n```python\r\ndef time_series_xgboost_analysis(df, target_col, lag_features=7, forecast_steps=30):\r\n    \"\"\"ä½¿ç”¨XGBoostè¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹\"\"\"\r\n    \r\n    print(\"=== å¼€å§‹æ—¶é—´åºåˆ—XGBooståˆ†æ ===\")\r\n    \r\n    # 1. å‡†å¤‡ç‰¹å¾\r\n    print(\"å‡†å¤‡æ—¶é—´åºåˆ—ç‰¹å¾...\")\r\n    features_df = pd.DataFrame(index=df.index)\r\n    \r\n    # æ»åç‰¹å¾\r\n    for lag in range(1, lag_features + 1):\r\n        features_df[f'lag_{lag}'] = df[target_col].shift(lag)\r\n    \r\n    # æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾\r\n    for window in [3, 7, 14, 30]:\r\n        features_df[f'ma_{window}'] = df[target_col].rolling(window).mean().shift(1)\r\n        features_df[f'std_{window}'] = df[target_col].rolling(window).std().shift(1)\r\n    \r\n    # æ—¥æœŸç‰¹å¾\r\n    if hasattr(df.index, 'month'):\r\n        features_df['month'] = df.index.month\r\n        features_df['dayofweek'] = df.index.dayofweek\r\n        features_df['dayofmonth'] = df.index.day\r\n        features_df['quarter'] = df.index.quarter\r\n    \r\n    # å¤–éƒ¨ç‰¹å¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰\r\n    external_features = ['Temperature', 'Promotion', 'Competitor_Price', 'Holiday']\r\n    for feat in external_features:\r\n        if feat in df.columns:\r\n            features_df[feat] = df[feat]\r\n    \r\n    # ç›®æ ‡å˜é‡\r\n    features_df['target'] = df[target_col]\r\n    \r\n    # ç§»é™¤ç¼ºå¤±å€¼\r\n    features_df = features_df.dropna()\r\n    \r\n    print(f\"ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {features_df.shape}\")\r\n    \r\n    # 2. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\r\n    X = features_df.drop('target', axis=1)\r\n    y = features_df['target']\r\n    \r\n    split_idx = int(len(X) * 0.8)\r\n    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\r\n    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\r\n    \r\n    print(f\"è®­ç»ƒé›†å¤§å°: {X_train.shape}\")\r\n    print(f\"æµ‹è¯•é›†å¤§å°: {X_test.shape}\")\r\n    \r\n    # 3. è®­ç»ƒXGBoostæ¨¡å‹\r\n    print(\"\\nè®­ç»ƒXGBoostæ¨¡å‹...\")\r\n    \r\n    xgb_model = xgb.XGBRegressor(\r\n        n_estimators=100,\r\n        max_depth=5,\r\n        learning_rate=0.05,\r\n        subsample=0.8,\r\n        colsample_bytree=0.8,\r\n        tree_method='hist',  # å†…å­˜å‹å¥½\r\n        n_jobs=2,           # 6GBå†…å­˜ä¸‹ä½¿ç”¨2ä¸ªçº¿ç¨‹\r\n        random_state=42,\r\n        verbosity=0\r\n    )\r\n    \r\n    xgb_model.fit(X_train, y_train)\r\n    \r\n    # 4. æ¨¡å‹è¯„ä¼°\r\n    y_pred = xgb_model.predict(X_test)\r\n    \r\n    mae = mean_absolute_error(y_test, y_pred)\r\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\r\n    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\r\n    \r\n    print(f\"\\n=== XGBoostæ¨¡å‹æ€§èƒ½ ===\")\r\n    print(f\"MAE (å¹³å‡ç»å¯¹è¯¯å·®): {mae:.2f}\")\r\n    print(f\"RMSE (å‡æ–¹æ ¹è¯¯å·®): {rmse:.2f}\")\r\n    print(f\"MAPE (å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®): {mape:.2f}%\")\r\n    \r\n    # 5. ç‰¹å¾é‡è¦æ€§\r\n    feature_importance = pd.DataFrame({\r\n        'ç‰¹å¾': X.columns,\r\n        'é‡è¦æ€§': xgb_model.feature_importances_\r\n    }).sort_values('é‡è¦æ€§', ascending=False)\r\n    \r\n    print(f\"\\n=== ç‰¹å¾é‡è¦æ€§ï¼ˆTop 10ï¼‰===\")\r\n    for _, row in feature_importance.head(10).iterrows():\r\n        print(f\"  {row['ç‰¹å¾']}: {row['é‡è¦æ€§']:.4f}\")\r\n    \r\n    # 6. å¯è§†åŒ–\r\n    plt.figure(figsize=(15, 10))\r\n    \r\n    # é¢„æµ‹ vs å®é™…\r\n    plt.subplot(2, 3, 1)\r\n    plt.scatter(y_test, y_pred, alpha=0.6)\r\n    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\r\n    plt.xlabel('å®é™…å€¼')\r\n    plt.ylabel('é¢„æµ‹å€¼')\r\n    plt.title(f'XGBoosté¢„æµ‹æ•ˆæœ (MAE = {mae:.2f})')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # ç‰¹å¾é‡è¦æ€§\r\n    plt.subplot(2, 3, 2)\r\n    top_features = feature_importance.head(10)\r\n    plt.barh(top_features['ç‰¹å¾'], top_features['é‡è¦æ€§'])\r\n    plt.xlabel('é‡è¦æ€§')\r\n    plt.title('Top 10 ç‰¹å¾é‡è¦æ€§')\r\n    plt.gca().invert_yaxis()\r\n    \r\n    # æ—¶é—´åºåˆ—é¢„æµ‹å¯¹æ¯”\r\n    plt.subplot(2, 3, 3)\r\n    plt.plot(y_test.index, y_test.values, label='å®é™…å€¼', alpha=0.7)\r\n    plt.plot(y_test.index, y_pred, label='é¢„æµ‹å€¼', alpha=0.7)\r\n    plt.xlabel('æ—¶é—´')\r\n    plt.ylabel('å€¼')\r\n    plt.title('æ—¶é—´åºåˆ—é¢„æµ‹å¯¹æ¯”')\r\n    plt.legend()\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # æ®‹å·®åˆ†æ\r\n    plt.subplot(2, 3, 4)\r\n    residuals = y_test - y_pred\r\n    plt.scatter(y_pred, residuals, alpha=0.6)\r\n    plt.axhline(y=0, color='r', linestyle='--')\r\n    plt.xlabel('é¢„æµ‹å€¼')\r\n    plt.ylabel('æ®‹å·®')\r\n    plt.title('æ®‹å·®åˆ†æ')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # è¯¯å·®åˆ†å¸ƒ\r\n    plt.subplot(2, 3, 5)\r\n    plt.hist(residuals, bins=30, alpha=0.7, edgecolor='black')\r\n    plt.xlabel('æ®‹å·®')\r\n    plt.ylabel('é¢‘æ•°')\r\n    plt.title('è¯¯å·®åˆ†å¸ƒ')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # æ»šåŠ¨é¢„æµ‹\r\n    plt.subplot(2, 3, 6)\r\n    # å–æœ€å100ä¸ªç‚¹å±•ç¤º\r\n    last_n = min(100, len(y_test))\r\n    plt.plot(y_test.index[-last_n:], y_test.values[-last_n:], label='å®é™…å€¼')\r\n    plt.plot(y_test.index[-last_n:], y_pred[-last_n:], label='é¢„æµ‹å€¼')\r\n    plt.xlabel('æ—¶é—´')\r\n    plt.ylabel('å€¼')\r\n    plt.title('æ»šåŠ¨é¢„æµ‹å¯¹æ¯”ï¼ˆæœ€å100ç‚¹ï¼‰')\r\n    plt.legend()\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    plt.tight_layout()\r\n    plt.show()\r\n    \r\n    # 7. æœªæ¥é¢„æµ‹ï¼ˆå¦‚æœéœ€è¦ï¼‰\r\n    if forecast_steps > 0:\r\n        print(f\"\\n=== æœªæ¥{forecast_steps}æœŸé¢„æµ‹ ===\")\r\n        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“ä¸šåŠ¡é€»è¾‘å®ç°æ»šåŠ¨é¢„æµ‹\r\n        # ç®€åŒ–ç‰ˆï¼šä½¿ç”¨æœ€ålag_featuresä¸ªç‚¹ä½œä¸ºåˆå§‹ç‰¹å¾\r\n        \r\n        last_features = X.iloc[-1:].copy()\r\n        future_predictions = []\r\n        \r\n        for i in range(forecast_steps):\r\n            # é¢„æµ‹ä¸‹ä¸€æ­¥\r\n            pred = xgb_model.predict(last_features)[0]\r\n            future_predictions.append(pred)\r\n            \r\n            # æ›´æ–°ç‰¹å¾ï¼ˆå¦‚æœæ˜¯æ—¶é—´åºåˆ—é¢„æµ‹ï¼Œéœ€è¦æ›´æ–°æ»åç‰¹å¾ï¼‰\r\n            # è¿™é‡Œç®€åŒ–ä¸ºåªä½¿ç”¨æœ€æ–°é¢„æµ‹å€¼\r\n            # å®é™…åº”ç”¨ä¸­éœ€è¦æ ¹æ®ç‰¹å¾å·¥ç¨‹é€»è¾‘æ›´æ–°\r\n            \r\n        print(f\"æœªæ¥é¢„æµ‹å€¼: {future_predictions}\")\r\n    \r\n    return {\r\n        'model': xgb_model,\r\n        'metrics': {'mae': mae, 'rmse': rmse, 'mape': mape},\r\n        'feature_importance': feature_importance,\r\n        'predictions': y_pred,\r\n        'future_predictions': future_predictions if forecast_steps > 0 else None\r\n    }\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\n# å‡è®¾dfæ˜¯ä¸€ä¸ªDataFrameï¼ŒåŒ…å«æ—¶é—´åºåˆ—å’Œå¤–éƒ¨ç‰¹å¾\r\n# results = time_series_xgboost_analysis(df, target_col='Sales', lag_features=14, forecast_steps=30)\r\n```\r\n\r\n### æ—¶é—´åºåˆ—æ¨¡å‹å¯¹æ¯”\r\n\r\n```python\r\ndef compare_time_series_models(df, target_col, seasonal_period=7, lag_features=14):\r\n    \"\"\"å¯¹æ¯”ä¸åŒæ—¶é—´åºåˆ—æ¨¡å‹æ€§èƒ½\"\"\"\r\n    \r\n    print(\"=== æ—¶é—´åºåˆ—æ¨¡å‹å¯¹æ¯”åˆ†æ ===\")\r\n    \r\n    # å‡†å¤‡æ•°æ®\r\n    series = df[target_col]\r\n    \r\n    # 1. ARIMAæ¨¡å‹\r\n    print(\"\\n1. è®­ç»ƒARIMAæ¨¡å‹...\")\r\n    arima_results = time_series_arima_analysis(series, seasonal_period, forecast_steps=0)\r\n    \r\n    # 2. XGBoostæ¨¡å‹\r\n    print(\"\\n2. è®­ç»ƒXGBoostæ¨¡å‹...\")\r\n    xgb_results = time_series_xgboost_analysis(df, target_col, lag_features, forecast_steps=0)\r\n    \r\n    # 3. LightGBMæ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰\r\n    try:\r\n        import lightgbm as lgb\r\n        print(\"\\n3. è®­ç»ƒLightGBMæ¨¡å‹...\")\r\n        \r\n        # å‡†å¤‡ç‰¹å¾ï¼ˆå¤ç”¨XGBoostçš„ç‰¹å¾ï¼‰\r\n        features_df = pd.DataFrame(index=df.index)\r\n        for lag in range(1, lag_features + 1):\r\n            features_df[f'lag_{lag}'] = df[target_col].shift(lag)\r\n        \r\n        for window in [3, 7, 14, 30]:\r\n            features_df[f'ma_{window}'] = df[target_col].rolling(window).mean().shift(1)\r\n        \r\n        if hasattr(df.index, 'month'):\r\n            features_df['month'] = df.index.month\r\n            features_df['dayofweek'] = df.index.dayofweek\r\n        \r\n        external_features = ['Temperature', 'Promotion', 'Competitor_Price', 'Holiday']\r\n        for feat in external_features:\r\n            if feat in df.columns:\r\n                features_df[feat] = df[feat]\r\n        \r\n        features_df['target'] = df[target_col]\r\n        features_df = features_df.dropna()\r\n        \r\n        X = features_df.drop('target', axis=1)\r\n        y = features_df['target']\r\n        \r\n        split_idx = int(len(X) * 0.8)\r\n        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\r\n        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\r\n        \r\n        # è®­ç»ƒLightGBM\r\n        lgb_model = lgb.LGBMRegressor(\r\n            num_leaves=31,\r\n            learning_rate=0.05,\r\n            n_estimators=100,\r\n            n_jobs=2,\r\n            random_state=42,\r\n            verbose=-1\r\n        )\r\n        \r\n        lgb_model.fit(X_train, y_train)\r\n        y_pred_lgb = lgb_model.predict(X_test)\r\n        \r\n        mae_lgb = mean_absolute_error(y_test, y_pred_lgb)\r\n        rmse_lgb = np.sqrt(mean_squared_error(y_test, y_pred_lgb))\r\n        \r\n        print(f\"LightGBMæ€§èƒ½: MAE={mae_lgb:.2f}, RMSE={rmse_lgb:.2f}\")\r\n        \r\n        lgb_results = {\r\n            'model': lgb_model,\r\n            'metrics': {'mae': mae_lgb, 'rmse': rmse_lgb}\r\n        }\r\n        \r\n    except ImportError:\r\n        print(\"LightGBMä¸å¯ç”¨ï¼Œè·³è¿‡\")\r\n        lgb_results = None\r\n    \r\n    # 4. æ¨¡å‹å¯¹æ¯”\r\n    print(\"\\n=== æ¨¡å‹æ€§èƒ½å¯¹æ¯” ===\")\r\n    \r\n    comparison_data = []\r\n    \r\n    if arima_results:\r\n        comparison_data.append({\r\n            'æ¨¡å‹': 'ARIMA',\r\n            'MAE': arima_results['metrics']['mae'],\r\n            'RMSE': arima_results['metrics']['rmse'],\r\n            'MAPE': arima_results['metrics']['mape']\r\n        })\r\n    \r\n    if xgb_results:\r\n        comparison_data.append({\r\n            'æ¨¡å‹': 'XGBoost',\r\n            'MAE': xgb_results['metrics']['mae'],\r\n            'RMSE': xgb_results['metrics']['rmse'],\r\n            'MAPE': xgb_results['metrics']['mape']\r\n        })\r\n    \r\n    if lgb_results:\r\n        comparison_data.append({\r\n            'æ¨¡å‹': 'LightGBM',\r\n            'MAE': lgb_results['metrics']['mae'],\r\n            'RMSE': lgb_results['metrics']['rmse'],\r\n            'MAPE': None\r\n        })\r\n    \r\n    comparison_df = pd.DataFrame(comparison_data)\r\n    print(comparison_df.to_string(index=False))\r\n    \r\n    # 5. å¯è§†åŒ–å¯¹æ¯”\r\n    if len(comparison_data) > 1:\r\n        plt.figure(figsize=(12, 5))\r\n        \r\n        # MAEå¯¹æ¯”\r\n        plt.subplot(1, 2, 1)\r\n        models = [d['æ¨¡å‹'] for d in comparison_data]\r\n        maes = [d['MAE'] for d in comparison_data]\r\n        \r\n        bars = plt.bar(models, maes, alpha=0.7)\r\n        plt.xlabel('æ¨¡å‹')\r\n        plt.ylabel('MAE')\r\n        plt.title('æ¨¡å‹MAEå¯¹æ¯”')\r\n        plt.grid(True, alpha=0.3)\r\n        \r\n        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼\r\n        for bar, mae in zip(bars, maes):\r\n            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \r\n                    f'{mae:.2f}', ha='center', va='bottom')\r\n        \r\n        # RMSEå¯¹æ¯”\r\n        plt.subplot(1, 2, 2)\r\n        rmses = [d['RMSE'] for d in comparison_data]\r\n        \r\n        bars = plt.bar(models, rmses, alpha=0.7, color='orange')\r\n        plt.xlabel('æ¨¡å‹')\r\n        plt.ylabel('RMSE')\r\n        plt.title('æ¨¡å‹RMSEå¯¹æ¯”')\r\n        plt.grid(True, alpha=0.3)\r\n        \r\n        for bar, rmse in zip(bars, rmses):\r\n            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \r\n                    f'{rmse:.2f}', ha='center', va='bottom')\r\n        \r\n        plt.tight_layout()\r\n        plt.show()\r\n        \r\n        # æ¨èæ¨¡å‹\r\n        best_model_idx = np.argmin(maes)\r\n        best_model = models[best_model_idx]\r\n        print(f\"\\n=== æ¨èæ¨¡å‹ ===\")\r\n        print(f\"æ ¹æ®MAEæŒ‡æ ‡ï¼Œæ¨èä½¿ç”¨: {best_model}æ¨¡å‹\")\r\n        print(f\"ç†ç”±: åœ¨æµ‹è¯•é›†ä¸Šè¡¨ç°æœ€ä½³ (MAE = {maes[best_model_idx]:.2f})\")\r\n    \r\n    return {\r\n        'arima': arima_results,\r\n        'xgboost': xgb_results,\r\n        'lightgbm': lgb_results,\r\n        'comparison': comparison_df\r\n    }\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\n# å‡è®¾æœ‰å®Œæ•´çš„æ—¶é—´åºåˆ—æ•°æ®é›†df\r\n# model_comparison = compare_time_series_models(df, target_col='Sales', seasonal_period=7, lag_features=14)\r\n```\r\n\r\n## ğŸ”§ æ¨¡å‹ä¼˜åŒ–ä¸è°ƒå‚\r\n\r\n```python\r\nfrom sklearn.model_selection import GridSearchCV\r\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\r\n\r\ndef model_optimization_pipeline(X, y, problem_type='regression'):\r\n    \"\"\"æ¨¡å‹è¶…å‚æ•°ä¼˜åŒ–æµç¨‹\"\"\"\r\n    \r\n    print(f\"=== å¼€å§‹ {problem_type} æ¨¡å‹ä¼˜åŒ– ===\")\r\n    \r\n    # æ•°æ®åˆ†å‰²\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\r\n    \r\n    # é€‰æ‹©æ¨¡å‹å’Œå‚æ•°ç½‘æ ¼\r\n    if problem_type == 'regression':\r\n        model = RandomForestRegressor(random_state=42)\r\n        param_grid = {\r\n            'n_estimators': [50, 100, 200],\r\n            'max_depth': [None, 10, 20],\r\n            'min_samples_split': [2, 5, 10],\r\n            'min_samples_leaf': [1, 2, 4]\r\n        }\r\n        scoring = 'r2'\r\n    else:\r\n        model = RandomForestClassifier(random_state=42)\r\n        param_grid = {\r\n            'n_estimators': [50, 100, 200],\r\n            'max_depth': [None, 10, 20],\r\n            'min_samples_split': [2, 5, 10],\r\n            'min_samples_leaf': [1, 2, 4]\r\n        }\r\n        scoring = 'accuracy'\r\n    \r\n    # ç½‘æ ¼æœç´¢\r\n    print(\"æ­£åœ¨è¿›è¡Œç½‘æ ¼æœç´¢...\")\r\n    grid_search = GridSearchCV(\r\n        model, param_grid, cv=5, scoring=scoring, \r\n        n_jobs=-1, verbose=1\r\n    )\r\n    grid_search.fit(X_train, y_train)\r\n    \r\n    # è¾“å‡ºæœ€ä¼˜å‚æ•°\r\n    print(f\"\\n=== æœ€ä¼˜å‚æ•° ===\")\r\n    for param, value in grid_search.best_params_.items():\r\n        print(f\"  {param}: {value}\")\r\n    \r\n    print(f\"æœ€ä¼˜æ¨¡å‹å¾—åˆ†: {grid_search.best_score_:.4f}\")\r\n    \r\n    # æµ‹è¯•é›†æ€§èƒ½\r\n    best_model = grid_search.best_estimator_\r\n    y_pred = best_model.predict(X_test)\r\n    \r\n    if problem_type == 'regression':\r\n        test_score = r2_score(y_test, y_pred)\r\n        print(f\"æµ‹è¯•é›† RÂ²: {test_score:.4f}\")\r\n    else:\r\n        test_score = accuracy_score(y_test, y_pred)\r\n        print(f\"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_score:.4f}\")\r\n    \r\n    return {\r\n        'best_model': best_model,\r\n        'best_params': grid_search.best_params_,\r\n        'best_score': grid_search.best_score_,\r\n        'test_score': test_score\r\n    }\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\n# X_reg, y_reg, X_clf, y_clf = prepare_ml_data()\r\n# optimized_regression = model_optimization_pipeline(X_reg, y_reg, 'regression')\r\n# optimized_classification = model_optimization_pipeline(X_clf, y_clf, 'classification')\r\n```\r\n\r\n## æœºå™¨å­¦ä¹ å¢å¼º(v2.5æ–°å¢)\r\n\r\n### LightGBM - é«˜æ•ˆæ¢¯åº¦æå‡\r\n\r\n**ç”¨é€”**: é«˜æ€§èƒ½æ¢¯åº¦æå‡æ ‘ç®—æ³•  \r\n**ä¼˜åŠ¿**: æ¯”XGBoostè®­ç»ƒæ›´å¿«ï¼Œå†…å­˜å ç”¨æ›´å°‘  \r\n\r\n```python\r\nimport lightgbm as lgb\r\nfrom sklearn.model_selection import train_test_split\r\nimport pandas as pd\r\n\r\n# å‡†å¤‡æ•°æ®\r\ndata = pd.read_csv('/data/train.csv')\r\nX = data.drop('target', axis=1)\r\ny = data['target']\r\n\r\n# åˆ’åˆ†æ•°æ®é›†\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\r\n\r\n# åˆ›å»ºæ•°æ®é›†\r\ntrain_data = lgb.Dataset(X_train, label=y_train)\r\ntest_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\r\n\r\n# å‚æ•°è®¾ç½®ï¼ˆä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼‰\r\nparams = {\r\n    'boosting_type': 'gbdt',\r\n    'objective': 'binary',\r\n    'metric': 'binary_logloss',\r\n    'num_leaves': 31,\r\n    'learning_rate': 0.05,\r\n    'feature_fraction': 0.9,\r\n    'bagging_fraction': 0.8,\r\n    'bagging_freq': 5,\r\n    'verbose': -1,\r\n    'num_threads': 2  # é™åˆ¶çº¿ç¨‹æ•°\r\n}\r\n\r\n# è®­ç»ƒæ¨¡å‹\r\ngbm = lgb.train(params, train_data, num_boost_round=100)\r\n```\r\n\r\n### Category Encoders - åˆ†ç±»ç‰¹å¾ç¼–ç \r\n\r\n**ç”¨é€”**: å„ç§åˆ†ç±»ç¼–ç æ–¹æ³•  \r\n**ä¼˜åŠ¿**: æå‡åˆ†ç±»æ¨¡å‹æ€§èƒ½ï¼Œæ”¯æŒå¤šç§ç¼–ç ç­–ç•¥  \r\n\r\n```python\r\nimport pandas as pd\r\nimport category_encoders as ce\r\n\r\n# åˆ›å»ºç¤ºä¾‹æ•°æ®\r\ndf = pd.DataFrame({\r\n    'category': ['A', 'B', 'A', 'C', 'B', 'A'],\r\n    'value': [1, 2, 3, 4, 5, 6]\r\n})\r\n\r\n# ä½¿ç”¨Target Encoding\r\nencoder = ce.TargetEncoder(cols=['category'])\r\ndf_encoded = encoder.fit_transform(df['category'], df['value'])\r\n\r\nprint(df_encoded)\r\n```\r\n\r\n### XGBoost - é«˜æ€§èƒ½æ¢¯åº¦æå‡æ ‘ (v2.3æ–°å¢)\r\n\r\n**ç”¨é€”**: é«˜çº§æ¢¯åº¦æå‡æ ‘ç®—æ³•ï¼Œæ”¯æŒå›å½’ã€åˆ†ç±»ã€æ’åºä»»åŠ¡  \r\n**ä¼˜åŠ¿**: ç²¾åº¦é«˜ï¼Œæ”¯æŒè‡ªå®šä¹‰ç›®æ ‡å‡½æ•°ï¼Œå¯è§£é‡Šæ€§å¥½  \r\n\r\n```python\r\nimport xgboost as xgb\r\nimport pandas as pd\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.metrics import accuracy_score, mean_squared_error\r\n\r\n# å‡†å¤‡æ•°æ®\r\ndata = pd.read_csv('/data/train.csv')\r\nX = data.drop('target', axis=1)\r\ny = data['target']\r\n\r\n# åˆ’åˆ†æ•°æ®é›†\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\r\n\r\n# åˆ›å»ºDMatrixï¼ˆXGBoosté«˜æ•ˆæ•°æ®ç»“æ„ï¼‰\r\ndtrain = xgb.DMatrix(X_train, label=y_train)\r\ndtest = xgb.DMatrix(X_test, label=y_test)\r\n\r\n# å‚æ•°è®¾ç½®ï¼ˆå›å½’é—®é¢˜ç¤ºä¾‹ï¼‰\r\nparams = {\r\n    'objective': 'reg:squarederror',  # å›å½’ä»»åŠ¡\r\n    'max_depth': 5,\r\n    'eta': 0.1,  # å­¦ä¹ ç‡\r\n    'subsample': 0.8,\r\n    'colsample_bytree': 0.8,\r\n    'tree_method': 'hist',  # å†…å­˜å‹å¥½çš„ç›´æ–¹å›¾ç®—æ³•\r\n    'n_jobs': 2,  # 6GBå†…å­˜ä¸‹ä½¿ç”¨2ä¸ªçº¿ç¨‹\r\n    'random_state': 42\r\n}\r\n\r\n# è®­ç»ƒæ¨¡å‹\r\nnum_rounds = 100\r\nmodel = xgb.train(params, dtrain, num_rounds)\r\n\r\n# é¢„æµ‹\r\ny_pred = model.predict(dtest)\r\n\r\n# è¯„ä¼°\r\nif data['target'].dtype == 'object':  # åˆ†ç±»ä»»åŠ¡\r\n    accuracy = accuracy_score(y_test, y_pred.round())\r\n    print(f\"å‡†ç¡®ç‡: {accuracy:.4f}\")\r\nelse:  # å›å½’ä»»åŠ¡\r\n    mse = mean_squared_error(y_test, y_pred)\r\n    print(f\"MSE: {mse:.4f}\")\r\n\r\n# ç‰¹å¾é‡è¦æ€§\r\nimportance = model.get_score(importance_type='weight')\r\nprint(\"ç‰¹å¾é‡è¦æ€§:\", importance)\r\n\r\n# ä¿å­˜æ¨¡å‹\r\nmodel.save_model('/data/xgboost_model.json')\r\n```\r\n\r\n### scikit-optimize - è´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ–\r\n\r\n**ç”¨é€”**: è‡ªåŠ¨åŒ–è¶…å‚æ•°ä¼˜åŒ–  \r\n**ä¼˜åŠ¿**: æ¯”ç½‘æ ¼æœç´¢æ›´é«˜æ•ˆï¼Œæ‰¾åˆ°æ›´å¥½å‚æ•°ç»„åˆ  \r\n\r\n```python\r\nfrom skopt import BayesSearchCV\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nimport pandas as pd\r\n\r\n# å‡†å¤‡æ•°æ®\r\ndata = pd.read_csv('/data/train.csv')\r\nX = data.drop('target', axis=1)\r\ny = data['target']\r\n\r\n# å®šä¹‰å‚æ•°æœç´¢ç©ºé—´\r\nparam_space = {\r\n    'n_estimators': (50, 200),\r\n    'max_depth': (3, 10),\r\n    'min_samples_split': (2, 10),\r\n    'min_samples_leaf': (1, 4)\r\n}\r\n\r\n# è´å¶æ–¯ä¼˜åŒ–æœç´¢\r\nopt = BayesSearchCV(\r\n    RandomForestClassifier(),\r\n    param_space,\r\n    n_iter=50,\r\n    cv=5,\r\n    n_jobs=2  # é™åˆ¶å¹¶è¡Œçº¿ç¨‹\r\n)\r\n\r\nopt.fit(X, y)\r\nprint(f\"æœ€ä½³å‚æ•°: {opt.best_params_}\")\r\nprint(f\"æœ€ä½³åˆ†æ•°: {opt.best_score_:.4f}\")\r\n```\r\n\r\n## âš ï¸ ä½¿ç”¨æ³¨æ„äº‹é¡¹\r\n\r\n### âœ… æ¨èåšæ³•ï¼š\r\n- ä½¿ç”¨æ ‡å‡†çš„ scikit-learn å’Œ statsmodels æ¥å£\r\n- ç›´æ¥ä½¿ç”¨ `print()` è¾“å‡ºç»“æœå’ŒæŒ‡æ ‡\r\n- ä½¿ç”¨ `plt.show()` æ˜¾ç¤ºå›¾è¡¨\r\n- å¯¹æ•°æ®è¿›è¡Œé€‚å½“çš„é¢„å¤„ç†å’Œæ ‡å‡†åŒ–\r\n- æ—¶é—´åºåˆ—åˆ†æä¼˜å…ˆä½¿ç”¨pmdarimaè‡ªåŠ¨é€‰æ‹©ARIMAå‚æ•°\r\n- éçº¿æ€§å»ºæ¨¡ä¼˜å…ˆä½¿ç”¨XGBoostæˆ–LightGBM\r\n\r\n### âŒ é¿å…çš„æ“ä½œï¼š\r\n- ä¸è¦æ‰‹åŠ¨æ„å»º JSON è¾“å‡º\r\n- ä¸è¦ä½¿ç”¨ `base64` ç¼–ç \r\n- ä¸è¦åˆ›å»ºå¤æ‚çš„è‡ªå®šä¹‰è¾“å‡ºæ ¼å¼\r\n- ä¸è¦å¯¹æ˜æ˜¾å­£èŠ‚æ€§æ•°æ®ä½¿ç”¨éå­£èŠ‚æ€§ARIMA\r\n\r\n### ğŸ”§ é”™è¯¯å¤„ç†ï¼š\r\n```python\r\ntry:\r\n    from sklearn.ensemble import RandomForestRegressor\r\n    # æ¨¡å‹è®­ç»ƒä»£ç \r\nexcept ImportError:\r\n    print(\"scikit-learn ä¸å¯ç”¨\")\r\n\r\ntry:\r\n    import statsmodels.api as sm\r\n    # ç»Ÿè®¡å»ºæ¨¡ä»£ç \r\nexcept ImportError:\r\n    print(\"statsmodels ä¸å¯ç”¨\")\r\n\r\ntry:\r\n    import xgboost as xgb\r\n    # XGBoostä»£ç \r\nexcept ImportError:\r\n    print(\"XGBoost ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®\")\r\n\r\ntry:\r\n    import pmdarima as pm\r\n    # ARIMAä»£ç \r\nexcept ImportError:\r\n    print(\"pmdarima ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®\")\r\n```\r\n\r\n### ğŸ’¡ å®ç”¨æŠ€å·§ï¼š\r\n```python\r\n# å¿«é€Ÿæ¨¡å‹è¯„ä¼°å‡½æ•°\r\ndef quick_model_evaluation(model, X_test, y_test, problem_type='regression'):\r\n    \"\"\"å¿«é€Ÿæ¨¡å‹è¯„ä¼°\"\"\"\r\n    y_pred = model.predict(X_test)\r\n    \r\n    if problem_type == 'regression':\r\n        r2 = r2_score(y_test, y_pred)\r\n        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\r\n        print(f\"RÂ²: {r2:.4f}, RMSE: {rmse:.4f}\")\r\n    else:\r\n        accuracy = accuracy_score(y_test, y_pred)\r\n        print(f\"å‡†ç¡®ç‡: {accuracy:.4f}\")\r\n    \r\n    return y_pred\r\n\r\n# æ—¶é—´åºåˆ—åˆ†æå¿«é€Ÿæ¨¡æ¿\r\ndef quick_time_series_analysis(series, model_type='auto_arima'):\r\n    \"\"\"å¿«é€Ÿæ—¶é—´åºåˆ—åˆ†ææ¨¡æ¿\"\"\"\r\n    if model_type == 'auto_arima':\r\n        from pmdarima import auto_arima\r\n        model = auto_arima(series, seasonal=True, m=7, suppress_warnings=True)\r\n        forecast = model.predict(n_periods=30)\r\n    elif model_type == 'xgboost':\r\n        # ä½¿ç”¨time_series_xgboost_analysiså‡½æ•°\r\n        pass\r\n    \r\n    return model, forecast\r\n```\r\n\r\n**è®°ä½**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰è¾“å‡ºæ ¼å¼ï¼Œæ‚¨åªéœ€è¦ä¸“æ³¨äºæœºå™¨å­¦ä¹ å»ºæ¨¡å’Œåˆ†æé€»è¾‘ï¼\r\n\r\n## ğŸ“ æ²™ç›’ç¯å¢ƒæ–‡ä»¶æ“ä½œæŒ‡å—\r\n\r\n### æ–‡ä»¶ä¸Šä¼ ï¼ˆå¿…é¡»æ­¥éª¤ï¼‰\r\nåœ¨æ²™ç›’ä¸­è¿è¡Œä»£ç å‰ï¼Œ**å¿…é¡»å…ˆä¸Šä¼ æ•°æ®æ–‡ä»¶**ï¼š\r\n\r\n```python\r\n# ç¤ºä¾‹ï¼šå¦‚ä½•å¼•ç”¨å·²ä¸Šä¼ çš„æ–‡ä»¶\r\n# å‡è®¾æ‚¨å·²ç»é€šè¿‡å‰ç«¯ç•Œé¢ä¸Šä¼ äº†ä»¥ä¸‹æ–‡ä»¶ï¼š\r\n# - /data/train.csv      ï¼ˆé€šè¿‡æ–‡ä»¶ä¸Šä¼ APIä¸Šä¼ ï¼‰\r\n# - /data/dataset.xlsx   ï¼ˆé€šè¿‡æ–‡ä»¶ä¸Šä¼ APIä¸Šä¼ ï¼‰\r\n# - /data/sales.parquet  ï¼ˆé€šè¿‡æ–‡ä»¶ä¸Šä¼ APIä¸Šä¼ ï¼‰\r\n\r\nimport pandas as pd\r\nimport os\r\n\r\ndef list_uploaded_files():\r\n    \"\"\"åˆ—å‡ºæ‰€æœ‰å·²ä¸Šä¼ çš„æ–‡ä»¶\"\"\"\r\n    data_dir = '/data'\r\n    if os.path.exists(data_dir):\r\n        files = os.listdir(data_dir)\r\n        print(f\"å·²ä¸Šä¼ çš„æ–‡ä»¶: {files}\")\r\n        return files\r\n    else:\r\n        print(\"æ²¡æœ‰æ‰¾åˆ°/dataç›®å½•\")\r\n        return []\r\n\r\n# åˆ—å‡ºæ–‡ä»¶\r\navailable_files = list_uploaded_files()\r\n\r\n# è¯»å–ç‰¹å®šæ–‡ä»¶\r\nif 'train.csv' in available_files:\r\n    df = pd.read_csv('/data/train.csv')\r\n    print(f\"æˆåŠŸè¯»å– train.csvï¼Œå½¢çŠ¶: {df.shape}\")\r\n    \r\nif 'dataset.xlsx' in available_files:\r\n    df = pd.read_excel('/data/dataset.xlsx')\r\n    print(f\"æˆåŠŸè¯»å– dataset.xlsxï¼Œå½¢çŠ¶: {df.shape}\")\r\n```\r\n\r\n### æ”¯æŒçš„æ–‡ä»¶æ ¼å¼\r\næ ¹æ®code_interpreter.pyï¼Œç³»ç»Ÿæ”¯æŒä»¥ä¸‹æ–‡ä»¶æ ¼å¼ï¼š\r\n- ğŸ“Š æ•°æ®æ–‡ä»¶ï¼š`.csv`, `.xlsx`, `.xls`, `.parquet`, `.json`\r\n\r\n### æ–‡ä»¶è¯»å–æœ€ä½³å®è·µ\r\n```python\r\ndef safe_read_data(filename):\r\n    \"\"\"å®‰å…¨è¯»å–æ•°æ®æ–‡ä»¶ï¼Œå¸¦é”™è¯¯å¤„ç†\"\"\"\r\n    try:\r\n        filepath = f'/data/{filename}'\r\n        \r\n        # æ ¹æ®æ‰©å±•åé€‰æ‹©è¯»å–æ–¹æ³•\r\n        if filename.endswith('.csv'):\r\n            df = pd.read_csv(filepath)\r\n        elif filename.endswith('.parquet'):\r\n            df = pd.read_parquet(filepath)\r\n        elif filename.endswith(('.xlsx', '.xls')):\r\n            df = pd.read_excel(filepath)\r\n        elif filename.endswith('.json'):\r\n            df = pd.read_json(filepath)\r\n        else:\r\n            raise ValueError(f\"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {filename}\")\r\n        \r\n        print(f\"âœ… æˆåŠŸè¯»å– {filename}\")\r\n        print(f\"   è¡Œæ•°: {len(df)}, åˆ—æ•°: {len(df.columns)}\")\r\n        print(f\"   åˆ—å: {list(df.columns)}\")\r\n        \r\n        return df\r\n        \r\n    except FileNotFoundError:\r\n        print(f\"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filename}\")\r\n        print(\"è¯·å…ˆé€šè¿‡æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½ä¸Šä¼ æ–‡ä»¶\")\r\n        return None\r\n    except Exception as e:\r\n        print(f\"âŒ è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}\")\r\n        return None\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\nif __name__ == \"__main__\":\r\n    # æ£€æŸ¥å¯ç”¨çš„æ–‡ä»¶\r\n    files = list_uploaded_files()\r\n    if files:\r\n        for file in files:\r\n            print(f\"å‘ç°æ–‡ä»¶: {file}\")\r\n        \r\n        # è¯»å–ç¬¬ä¸€ä¸ªCSVæ–‡ä»¶\r\n        csv_files = [f for f in files if f.endswith('.csv')]\r\n        if csv_files:\r\n            df = safe_read_data(csv_files[0])\r\n            if df is not None:\r\n                # è¿›è¡Œæœºå™¨å­¦ä¹ åˆ†æ\r\n                pass\r\n```\r\n\r\n### å·¥ä½œæµæ•´åˆç¤ºä¾‹\r\n```python\r\n# å®Œæ•´çš„MLå·¥ä½œæµï¼ŒåŒ…å«æ–‡ä»¶æ£€æŸ¥\r\ndef complete_ml_workflow_with_file_check():\r\n    \"\"\"åŒ…å«æ–‡ä»¶æ£€æŸ¥çš„å®Œæ•´MLå·¥ä½œæµ\"\"\"\r\n    \r\n    print(\"=== æœºå™¨å­¦ä¹ å·¥ä½œæµå¼€å§‹ ===\")\r\n    \r\n    # 1. æ£€æŸ¥æ•°æ®æ–‡ä»¶\r\n    files = list_uploaded_files()\r\n    if not files:\r\n        print(\"è­¦å‘Šï¼šæ²¡æœ‰æ‰¾åˆ°ä¸Šä¼ çš„æ–‡ä»¶ï¼Œå°†ä½¿ç”¨ç¤ºä¾‹æ•°æ®\")\r\n        # ä½¿ç”¨generate_sample_data()å‡½æ•°åˆ›å»ºç¤ºä¾‹æ•°æ®\r\n        from sklearn.datasets import make_regression\r\n        X, y = make_regression(n_samples=1000, n_features=10, random_state=42)\r\n    else:\r\n        print(f\"æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶: {files}\")\r\n        \r\n        # è¯»å–ç¬¬ä¸€ä¸ªæ•°æ®æ–‡ä»¶\r\n        data_file = files[0]\r\n        df = safe_read_data(data_file)\r\n        \r\n        if df is None:\r\n            print(\"æ— æ³•è¯»å–æ–‡ä»¶ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®\")\r\n            from sklearn.datasets import make_regression\r\n            X, y = make_regression(n_samples=1000, n_features=10, random_state=42)\r\n        else:\r\n            # å‡è®¾æœ€åä¸€åˆ—æ˜¯ç›®æ ‡å˜é‡\r\n            X = df.iloc[:, :-1].values\r\n            y = df.iloc[:, -1].values\r\n    \r\n    # 2. æ‰§è¡ŒMLåˆ†æï¼ˆä½¿ç”¨æ–‡æ¡£ä¸­çš„å‡½æ•°ï¼‰\r\n    results = standard_ml_pipeline(X, y, problem_type='regression')\r\n    \r\n    return results\r\n```\r\n\r\n### âš¡ å¿«é€Ÿä½¿ç”¨æ¨¡æ¿\r\n```python\r\n# åœ¨æ²™ç›’ä¸­è¿è¡Œæœºå™¨å­¦ä¹ åˆ†æçš„å®Œæ•´ç¤ºä¾‹\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.ensemble import RandomForestRegressor\r\nfrom sklearn.metrics import mean_squared_error, r2_score\r\nimport matplotlib.pyplot as plt\r\n\r\n# æ­¥éª¤1ï¼šè¯»å–æ•°æ®ï¼ˆæ›¿æ¢ä¸ºæ‚¨çš„æ–‡ä»¶åï¼‰\r\ntry:\r\n    # å¦‚æœæ‚¨ä¸Šä¼ äº†train.csv\r\n    df = pd.read_csv('/data/train.csv')\r\n    print(f\"æ•°æ®å½¢çŠ¶: {df.shape}\")\r\n    \r\n    # æ­¥éª¤2ï¼šå‡†å¤‡ç‰¹å¾å’Œç›®æ ‡\r\n    X = df.drop('target_column', axis=1)  # æ›¿æ¢ä¸ºæ‚¨çš„ç›®æ ‡åˆ—å\r\n    y = df['target_column']\r\n    \r\n    # æ­¥éª¤3ï¼šè®­ç»ƒæ¨¡å‹\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\r\n    \r\n    model = RandomForestRegressor(n_estimators=100, random_state=42)\r\n    model.fit(X_train, y_train)\r\n    \r\n    # æ­¥éª¤4ï¼šè¯„ä¼°\r\n    y_pred = model.predict(X_test)\r\n    r2 = r2_score(y_test, y_pred)\r\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\r\n    \r\n    print(f\"æ¨¡å‹æ€§èƒ½: RÂ²={r2:.4f}, RMSE={rmse:.4f}\")\r\n    \r\n    # æ­¥éª¤5ï¼šå¯è§†åŒ–\r\n    plt.figure(figsize=(10, 5))\r\n    plt.scatter(y_test, y_pred, alpha=0.6)\r\n    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\r\n    plt.xlabel('å®é™…å€¼')\r\n    plt.ylabel('é¢„æµ‹å€¼')\r\n    plt.title(f'é¢„æµ‹æ•ˆæœ (RÂ² = {r2:.3f})')\r\n    plt.grid(True, alpha=0.3)\r\n    plt.show()\r\n    \r\nexcept FileNotFoundError:\r\n    print(\"âŒ æœªæ‰¾åˆ°æ–‡ä»¶ã€‚è¯·ç¡®ä¿ï¼š\")\r\n    print(\"   1. å·²é€šè¿‡æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½ä¸Šä¼ train.csv\")\r\n    print(\"   2. æ–‡ä»¶ä½äº/dataç›®å½•ä¸‹\")\r\n    print(\"   3. æ–‡ä»¶åæ‹¼å†™æ­£ç¡®\")\r\n    \r\n    # æä¾›ç¤ºä¾‹æ•°æ®ä½œä¸ºå¤‡é€‰\r\n    print(\"\\nğŸ”§ æ­£åœ¨ç”Ÿæˆç¤ºä¾‹æ•°æ®è¿›è¡Œåˆ†æ...\")\r\n    from sklearn.datasets import make_regression\r\n    X, y = make_regression(n_samples=1000, n_features=5, random_state=42)\r\n    \r\n    # ç»§ç»­æ‰§è¡Œåˆ†æ...\r\n```\r\n\n\n### ğŸ“– pandas_cheatsheet\n\n# ä»£ç è§£é‡Šå™¨ä½¿ç”¨æŒ‡å— v2.5 (æœ€ç»ˆèåˆç‰ˆ)\n\n## ğŸ¯ æ ¸å¿ƒåŸåˆ™ï¼šåç«¯è‡ªåŠ¨åŒ–ï¼Œä»£ç è¦ç®€æ´\n\n### âœ… **åç«¯å·²è‡ªåŠ¨å¤„ç†çš„åŠŸèƒ½ï¼š**\n1. **å›¾è¡¨æ•è·**ï¼š`plt.show()` è‡ªåŠ¨ç”Ÿæˆå›¾ç‰‡ï¼Œæ— éœ€æ‰‹åŠ¨ç¼–ç \n2. **æ–‡ä»¶ç®¡ç†**ï¼š`/data` ç›®å½•å·²é…ç½®å¥½ï¼Œæ”¯æŒä¼šè¯æŒä¹…åŒ–\n3. **è¾“å‡ºå¤„ç†**ï¼šç³»ç»Ÿè‡ªåŠ¨å¤„ç†æ‰€æœ‰ `print()` è¾“å‡º\n4. **é”™è¯¯æ•è·**ï¼šåç«¯æœ‰å®Œæ•´çš„é”™è¯¯å¤„ç†ç³»ç»Ÿ\n\n### âš ï¸ **èµ„æºé™åˆ¶ï¼š**\n1. **å†…å­˜é™åˆ¶**ï¼šå¯ç”¨å†…å­˜ä¸Šé™ä¸º6GB\n2. **æ—¶é—´é™åˆ¶**ï¼šä»£ç æ‰§è¡Œæœ‰90ç§’è¶…æ—¶é™åˆ¶\n\n### âŒ **æ¨¡å‹ä¸éœ€è¦åšçš„ï¼š**\n1. ä¸è¦æ‰‹åŠ¨ç¼–ç å›¾è¡¨ä¸º base64\n2. ä¸è¦ç¼–å†™å¤æ‚çš„é”™è¯¯å¤„ç†åŒ…è£…å™¨\n3. ä¸è¦ç®¡ç†æ–‡ä»¶æ ¼å¼è½¬æ¢ï¼ˆåç«¯è‡ªåŠ¨å¤„ç†ï¼‰\n4. ä¸è¦å¤„ç†å›¾è¡¨æ ‡é¢˜å’Œæ ¼å¼ï¼ˆç³»ç»Ÿè‡ªåŠ¨ä¼˜åŒ–ï¼‰\n\n---\n\n## ğŸ“‚ æ–‡ä»¶æ“ä½œï¼ˆä¼šè¯å·¥ä½œåŒºï¼š`/data`ï¼‰\n\n### ä»å·¥ä½œåŒºè¯»å–æ–‡ä»¶\n```python\nimport pandas as pd\n\n# æœ€ç®€å•çš„æ–‡ä»¶è¯»å–ï¼ˆæ”¯æŒ CSVã€Excelã€Parquet ç­‰ï¼‰\ndf = pd.read_csv('/data/your_file.csv')\n\n# å¿«é€ŸæŸ¥çœ‹æ•°æ®\nprint(f\"æ•°æ®å½¢çŠ¶: {df.shape}\")\nprint(df.head())\n```\n\n### ä¿å­˜æ–‡ä»¶åˆ°å·¥ä½œåŒº\n```python\n# ä¿å­˜å¤„ç†ç»“æœ\ndf_processed.to_csv('/data/processed_data.csv', index=False)\n\n# ä¿å­˜ä¸ºé«˜æ•ˆæ ¼å¼ï¼ˆä¾›åç»­ä½¿ç”¨ï¼‰\nimport pyarrow.feather as feather\nfeather.write_feather(df_processed, '/data/processed_data.feather')\n```\n\n### ğŸ“ é‡è¦è¯´æ˜\n- **æ–‡ä»¶ä½ç½®**ï¼šæ‰€æœ‰æ–‡ä»¶éƒ½åœ¨ `/data` ç›®å½•ä¸‹\n- **ä¼šè¯æŒä¹…**ï¼šæ–‡ä»¶åœ¨åŒä¸€ä¼šè¯çš„å¤šæ¬¡æ‰§è¡Œä¸­ä¿æŒå¯ç”¨\n- **è‡ªåŠ¨æ¸…ç†**ï¼š24å°æ—¶åä¼šè¯æ–‡ä»¶è‡ªåŠ¨æ¸…ç†\n\n---\n\n## ğŸ“Š æ•°æ®å¯è§†åŒ–ï¼ˆè‡ªåŠ¨æ•è·ï¼‰\n\n### åŸºç¡€å›¾è¡¨\n```python\nimport matplotlib.pyplot as plt\n\n# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆåç«¯å·²é…ç½®ï¼Œè¿™é‡Œåªæ˜¯ç¡®ä¿ï¼‰\nplt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei']\n\n# 1. æŠ˜çº¿å›¾\nplt.figure(figsize=(10, 6))\nplt.plot(df['date'], df['value'], marker='o', linewidth=2)\nplt.title('é”€å”®è¶‹åŠ¿å›¾')  # æ ‡é¢˜ä¼šè¢«è‡ªåŠ¨æ•è·\nplt.xlabel('æ—¥æœŸ')\nplt.ylabel('é”€å”®é¢')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()  # ğŸ¯ å…³é”®ï¼šç›´æ¥ show()ï¼Œç³»ç»Ÿè‡ªåŠ¨æ•è·ï¼\n\n# 2. æŸ±çŠ¶å›¾\nplt.figure(figsize=(10, 6))\ndf.groupby('category')['sales'].sum().plot(kind='bar')\nplt.title('å„å“ç±»é”€å”®é¢')\nplt.tight_layout()\nplt.show()  # ğŸ¯ å…³é”®ï¼šç³»ç»Ÿè‡ªåŠ¨å¤„ç†ï¼\n\n# 3. æ•£ç‚¹å›¾\nplt.figure(figsize=(10, 6))\nplt.scatter(df['x'], df['y'], alpha=0.6, c=df['value'], cmap='viridis')\nplt.title('æ•£ç‚¹åˆ†å¸ƒå›¾')\nplt.colorbar(label='å€¼å¤§å°')\nplt.tight_layout()\nplt.show()  # ğŸ¯ å…³é”®ï¼šç³»ç»Ÿè‡ªåŠ¨æ•è·ï¼\n```\n\n### é«˜çº§å›¾è¡¨\n```python\n# 4. å­å›¾ï¼ˆå¤šå›¾è¡¨ï¼‰\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\naxes[0, 0].plot(df['date'], df['value1'])\naxes[0, 0].set_title('å›¾è¡¨1')\n\naxes[0, 1].hist(df['value2'], bins=30)\naxes[0, 1].set_title('å›¾è¡¨2')\n\naxes[1, 0].scatter(df['x'], df['y'])\naxes[1, 0].set_title('å›¾è¡¨3')\n\naxes[1, 1].boxplot([df['group1'], df['group2']])\naxes[1, 1].set_title('å›¾è¡¨4')\n\nplt.tight_layout()\nplt.show()  # ğŸ¯ ç³»ç»Ÿè‡ªåŠ¨æ•è·æ•´ä¸ªå›¾å½¢ï¼\n\n# 5. çƒ­åŠ›å›¾ï¼ˆç›¸å…³æ€§çŸ©é˜µï¼‰\nimport seaborn as sns\n\ncorr_matrix = df.corr()\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\nplt.title('ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾')\nplt.tight_layout()\nplt.show()  # ğŸ¯ ç³»ç»Ÿè‡ªåŠ¨æ•è·ï¼\n```\n\n### ğŸ“ å›¾è¡¨è¯´æ˜\n- **åç«¯è‡ªåŠ¨å¤„ç†**ï¼šæ‰€æœ‰å›¾è¡¨ç±»å‹ï¼ˆMatplotlibã€Seabornã€Graphvizã€NetworkXï¼‰\n- **æ ‡é¢˜æ•è·**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨æå–å›¾è¡¨æ ‡é¢˜æ˜¾ç¤ºç»™ç”¨æˆ·\n- **ä¸­æ–‡å­—ä½“**ï¼šåç«¯å·²é…ç½®ä¸­æ–‡æ”¯æŒï¼Œæ— éœ€æ‹…å¿ƒä¹±ç \n\n---\n\n## ğŸ§¹ æ•°æ®å¤„ç†ï¼ˆç®€æ´å®ç”¨ç‰ˆï¼‰\n\n### åŸºç¡€æ¸…æ´—\n```python\nimport pandas as pd\nimport numpy as np\n\n# è¯»å–æ•°æ®\ndf = pd.read_csv('/data/raw_data.csv')\n\n# æ‰“å°åŸºæœ¬ä¿¡æ¯\nprint(f\"åŸå§‹æ•°æ®: {df.shape[0]}è¡Œ Ã— {df.shape[1]}åˆ—\")\nprint(f\"ç¼ºå¤±å€¼æ€»æ•°: {df.isnull().sum().sum()}\")\n\n# å¤„ç†ç¼ºå¤±å€¼ï¼ˆæ•°å€¼åˆ—ç”¨ä¸­ä½æ•°ï¼‰\nnumeric_cols = df.select_dtypes(include=[np.number]).columns\nfor col in numeric_cols:\n    if df[col].isnull().any():\n        df[col].fillna(df[col].median(), inplace=True)\n\n# å¤„ç†ç¼ºå¤±å€¼ï¼ˆæ–‡æœ¬åˆ—ç”¨ä¼—æ•°ï¼‰\ntext_cols = df.select_dtypes(include=['object']).columns\nfor col in text_cols:\n    if df[col].isnull().any():\n        if not df[col].mode().empty:\n            df[col].fillna(df[col].mode()[0], inplace=True)\n\n# åˆ é™¤é‡å¤è¡Œ\ndf = df.drop_duplicates()\nprint(f\"æ¸…æ´—åæ•°æ®: {df.shape[0]}è¡Œ Ã— {df.shape[1]}åˆ—\")\n```\n\n### ç»Ÿè®¡åˆ†æ\n```python\n# åŸºç¡€ç»Ÿè®¡\nprint(\"æ•°å€¼åˆ—ç»Ÿè®¡:\")\nprint(df.describe())\n\n# åˆ†ç»„ç»Ÿè®¡\nprint(\"\\nåˆ†ç»„ç»Ÿè®¡:\")\ngroup_stats = df.groupby('category').agg({\n    'value': ['mean', 'sum', 'count', 'std']\n}).round(2)\nprint(group_stats)\n\n# é€è§†è¡¨\nprint(\"\\né€è§†è¡¨:\")\npivot = pd.pivot_table(df, \n                      values='sales', \n                      index='region', \n                      columns='month',\n                      aggfunc='sum')\nprint(pivot)\n```\n\n---\n\n## ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼ˆé’ˆå¯¹å¤§æ–‡ä»¶ï¼‰\n\n### æ–¹æ³•1ï¼šDuckDBï¼ˆSQLæŸ¥è¯¢ï¼Œæ¯”Pandaså¿«3-10å€ï¼‰\n```python\nimport duckdb\n\n# ç›´æ¥æŸ¥è¯¢CSV/Parquetæ–‡ä»¶ï¼ˆä¸åŠ è½½åˆ°å†…å­˜ï¼‰\nresult = duckdb.sql(\"\"\"\n    SELECT department, \n           AVG(salary) as avg_salary,\n           COUNT(*) as employee_count\n    FROM read_csv_auto('/data/employees.csv')\n    WHERE department IS NOT NULL\n    GROUP BY department\n    ORDER BY avg_salary DESC\n\"\"\").df()\n\nprint(\"éƒ¨é—¨è–ªèµ„ç»Ÿè®¡:\")\nprint(result)\n```\n\n### æ–¹æ³•2ï¼šåˆ†å—å¤„ç†ï¼ˆå¤§CSVæ–‡ä»¶ï¼‰\n```python\n# åˆ†å—è¯»å–å¤§æ–‡ä»¶\nchunks = []\nfor chunk in pd.read_csv('/data/large_file.csv', chunksize=50000):\n    # å¤„ç†æ¯ä¸ªæ•°æ®å—\n    processed = chunk[chunk['value'] > 0]  # ç¤ºä¾‹ç­›é€‰\n    chunks.append(processed)\n\n# åˆå¹¶ç»“æœ\nfinal_df = pd.concat(chunks, ignore_index=True)\nprint(f\"å¤„ç†å®Œæˆ: {len(final_df)}è¡Œ\")\n```\n\n### æ–¹æ³•3ï¼šé«˜æ•ˆæ ¼å¼è½¬æ¢\n```python\n# å°†CSVè½¬æ¢ä¸ºFeatheræ ¼å¼ï¼ˆæé€Ÿ10-100å€ï¼‰\nimport pyarrow.feather as feather\n\ndf = pd.read_csv('/data/large.csv')\nfeather.write_feather(df, '/data/large.feather')\n\n# ä¸‹æ¬¡è¯»å–æ—¶ï¼ˆæé€Ÿï¼‰\ndf_fast = feather.read_feather('/data/large.feather')\n```\n\n---\n\n## ğŸ’¡ å®ç”¨ä»£ç ç‰‡æ®µ\n\n### æ¨¡æ¿1ï¼šåŸºç¡€åˆ†æ\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# 1. è¯»å–æ•°æ®\ndf = pd.read_csv('/data/data.csv')\n\n# 2. å¿«é€Ÿåˆ†æ\nprint(f\"æ•°æ®å½¢çŠ¶: {df.shape}\")\nprint(df.describe())\n\n# 3. ç®€å•å¯è§†åŒ–\ndf.groupby('category')['value'].mean().plot(kind='bar')\nplt.title('å„åˆ†ç±»å¹³å‡å€¼')\nplt.tight_layout()\nplt.show()\n```\n\n### æ¨¡æ¿2ï¼šæ•°æ®æ¸…æ´—æµæ°´çº¿\n```python\n# 1. è¯»å–\ndf = pd.read_csv('/data/raw.csv')\n\n# 2. æ¸…æ´—\ndf = df.dropna().drop_duplicates()\n\n# 3. åˆ†æ\nprint(f\"æ¸…æ´—å: {df.shape}\")\nprint(df.groupby('group')['value'].mean())\n\n# 4. ä¿å­˜\ndf.to_csv('/data/cleaned.csv', index=False)\n```\n\n### æ¨¡æ¿3ï¼šå®Œæ•´æŠ¥å‘Šç”Ÿæˆ\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n# æ·»åŠ èµ„æºä½¿ç”¨æç¤º\nprint(f\"å¯ç”¨å†…å­˜é™åˆ¶: 6GB\")\nprint(f\"å»ºè®®å¤§æ–‡ä»¶å¤„ç†: ä½¿ç”¨åˆ†å—æˆ–DuckDB\")\nprint(\"æ³¨æ„ï¼šä»£ç æ‰§è¡Œæœ‰90ç§’è¶…æ—¶é™åˆ¶ï¼Œå¤æ‚è®¡ç®—è¯·ä¼˜åŒ–\")\n\nprint(\"=\" * 50)\nprint(f\"æ•°æ®åˆ†ææŠ¥å‘Š - {datetime.now().strftime('%Y-%m-%d')}\")\nprint(\"=\" * 50)\n\n# 1. æ•°æ®æ¦‚è§ˆ\ndf = pd.read_csv('/data/sales.csv')\nprint(f\"æ•°æ®é›†: {df.shape[0]}è¡Œ Ã— {df.shape[1]}åˆ—\")\nprint(f\"æ—¶é—´èŒƒå›´: {df['date'].min()} è‡³ {df['date'].max()}\")\n\n# 2. å…³é”®æŒ‡æ ‡\ntotal_sales = df['amount'].sum()\navg_sale = df['amount'].mean()\nprint(f\"\\nå…³é”®æŒ‡æ ‡:\")\nprint(f\"  æ€»é”€å”®é¢: Â¥{total_sales:,.2f}\")\nprint(f\"  å¹³å‡äº¤æ˜“é¢: Â¥{avg_sale:,.2f}\")\n\n# 3. å¯è§†åŒ–\nplt.figure(figsize=(12, 5))\n\n# é”€å”®é¢è¶‹åŠ¿\nplt.subplot(1, 2, 1)\ndf.groupby('date')['amount'].sum().plot()\nplt.title('æ¯æ—¥é”€å”®é¢')\nplt.grid(True, alpha=0.3)\n\n# å“ç±»åˆ†å¸ƒ\nplt.subplot(1, 2, 2)\ndf['category'].value_counts().head(10).plot(kind='bar')\nplt.title('Top 10 å“ç±»')\nplt.xticks(rotation=45)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nâœ… åˆ†æå®Œæˆï¼\")\n```\n\n---\n\n## âš ï¸ é‡è¦æé†’ï¼ˆåŸºäºåç«¯ç‰¹æ€§ï¼‰\n\n### åç«¯å·²é…ç½®ï¼Œæ— éœ€æ‹…å¿ƒï¼š\n1. **ä¸­æ–‡å­—ä½“**ï¼šå·²å®‰è£… WenQuanYi å­—ä½“ï¼Œå›¾è¡¨æ— ä¹±ç \n2. **å›¾è¡¨æ•è·**ï¼šæ‰€æœ‰ `plt.show()` è‡ªåŠ¨è½¬æ¢ä¸ºå›¾ç‰‡\n3. **å†…å­˜ç®¡ç†**ï¼šå®¹å™¨é™åˆ¶ 6GBï¼Œè‡ªåŠ¨å¤„ç†å†…å­˜æº¢å‡º\n4. **æ–‡ä»¶æƒé™**ï¼š`/data` ç›®å½•æœ‰è¯»å†™æƒé™\n\n### ä»£ç æ‰§è¡Œé™åˆ¶ï¼š\n1. **å†…å­˜é™åˆ¶**ï¼šå¯ç”¨å†…å­˜ä¸Šé™ä¸º6GBï¼Œå¤„ç†å¤§æ–‡ä»¶æ—¶å»ºè®®ä½¿ç”¨åˆ†å—å¤„ç†æˆ–DuckDB\n2. **è¶…æ—¶é™åˆ¶**ï¼šä»£ç æ‰§è¡Œæœ‰90ç§’è¶…æ—¶é™åˆ¶ï¼Œå¤æ‚è®¡ç®—è¯·ä¼˜åŒ–ç®—æ³•æˆ–åˆ†æ­¥æ‰§è¡Œ\n\n### ä»£ç ç¼–å†™åŸåˆ™ï¼š\n1. **ä¿æŒç®€æ´**ï¼šå†™ç›´ç™½çš„ Python ä»£ç ï¼Œæ— éœ€å¤æ‚åŒ…è£…\n2. **ç›¸ä¿¡åç«¯**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†å›¾è¡¨ã€é”™è¯¯ã€è¾“å‡ºæ ¼å¼\n3. **ä½¿ç”¨æ ‡å‡†åº“**ï¼šPandasã€Matplotlibã€NumPy ç­‰å·²é¢„è£…\n4. **å…³æ³¨ä¸šåŠ¡é€»è¾‘**ï¼šè®©åç«¯å¤„ç†æŠ€æœ¯ç»†èŠ‚\n\n---\n\n## ğŸ”§ æ•…éšœæ’é™¤\n\n### å¸¸è§é—®é¢˜ï¼š\n1. **æ–‡ä»¶ä¸å­˜åœ¨**ï¼šæ£€æŸ¥æ–‡ä»¶åæ˜¯å¦æ­£ç¡®ï¼Œæ³¨æ„å¤§å°å†™\n2. **å†…å­˜ä¸è¶³**ï¼šä½¿ç”¨åˆ†å—å¤„ç†æˆ– DuckDB æŸ¥è¯¢\n3. **å›¾è¡¨ä¸æ˜¾ç¤º**ï¼šç¡®ä¿è°ƒç”¨äº† `plt.show()`\n4. **ä¸­æ–‡ä¹±ç **ï¼šåç«¯å·²é…ç½®å­—ä½“ï¼Œæ— éœ€é¢å¤–å¤„ç†\n5. **æ‰§è¡Œè¶…æ—¶**ï¼šä»£ç æ‰§è¡Œè¶…è¿‡90ç§’é™åˆ¶ï¼Œè¯·ä¼˜åŒ–ç®—æ³•æˆ–åˆ†æ­¥æ‰§è¡Œ\n\n### æ€§èƒ½å»ºè®®ï¼š\n- **å°æ–‡ä»¶**ï¼šç›´æ¥ä½¿ç”¨ Pandas\n- **å¤§æ–‡ä»¶**ï¼šä½¿ç”¨ DuckDB æˆ–åˆ†å—å¤„ç†\n- **é‡å¤è®¡ç®—**ï¼šä¿å­˜ä¸­é—´ç»“æœåˆ° `/data` ç›®å½•\n- **å¤æ‚å›¾è¡¨**ï¼šåç«¯ä¼šè‡ªåŠ¨ä¼˜åŒ–æ¸²æŸ“\n- **å†…å­˜ä½¿ç”¨**ï¼šå¯ç”¨å†…å­˜é™åˆ¶ä¸º6GBï¼Œå¤„ç†å¤§å‹æ•°æ®é›†æ—¶è¯·æ³¨æ„ä½¿ç”¨åˆ†å—æˆ–DuckDBä»¥é™ä½å†…å­˜å ç”¨\n- **æ‰§è¡Œæ—¶é—´**ï¼šä»£ç æ‰§è¡Œæœ‰90ç§’è¶…æ—¶é™åˆ¶ï¼Œå¯¹äºå¤æ‚è®¡ç®—å»ºè®®ä¼˜åŒ–ç®—æ³•æˆ–åˆ†è§£ä¸ºå¤šä¸ªæ­¥éª¤æ‰§è¡Œ\n\n---\n\n## ğŸ“‹ å¿«é€Ÿå‚è€ƒå¡\n\n```python\n# è¯»å–æ–‡ä»¶\ndf = pd.read_csv('/data/file.csv')\n\n# ä¿å­˜æ–‡ä»¶\ndf.to_csv('/data/output.csv', index=False)\n\n# æ˜¾ç¤ºå›¾è¡¨\nplt.plot(x, y)\nplt.show()  # ğŸ¯ å…³é”®ï¼\n\n# æ‰“å°ç»“æœ\nprint(df.describe())\n\n# é«˜æ•ˆæŸ¥è¯¢ï¼ˆå¤§æ–‡ä»¶ï¼‰\nimport duckdb\nresult = duckdb.sql(\"SELECT * FROM read_csv_auto('/data/big.csv')\").df()\n\n# æ·»åŠ èµ„æºä½¿ç”¨æç¤º\nprint(f\"å¯ç”¨å†…å­˜é™åˆ¶: 6GB\")\nprint(f\"å»ºè®®å¤§æ–‡ä»¶å¤„ç†: ä½¿ç”¨åˆ†å—æˆ–DuckDB\")\nprint(\"æ³¨æ„ï¼šä»£ç æ‰§è¡Œæœ‰90ç§’è¶…æ—¶é™åˆ¶ï¼Œå¤æ‚è®¡ç®—è¯·ä¼˜åŒ–\")\n```\n\n---\n\n**æœ€ç»ˆåŸåˆ™**ï¼šå†™ä½ **æƒ³å†™**çš„ä»£ç ï¼Œåç«¯ä¼šå¤„ç†**è¯¥å¤„ç†**çš„ç»†èŠ‚ï¼å›¾è¡¨ã€æ–‡ä»¶ã€è¾“å‡ºéƒ½äº¤ç»™ç³»ç»Ÿï¼Œä½ åªéœ€è¦å…³æ³¨æ•°æ®åˆ†æå’Œä¸šåŠ¡é€»è¾‘ã€‚\n\n### ğŸ“– report_generator_workflow\n\n# è‡ªåŠ¨åŒ–æŠ¥å‘Šç”ŸæˆæŒ‡å— (v3.0 - å®Œæ•´ç‰ˆ)\r\n\r\n## ğŸš€ æ ¸å¿ƒè¾“å‡ºåè®® (å¼ºåˆ¶éµå¾ª)\r\n\r\n**é‡è¦æç¤º**: è¦ç”Ÿæˆä¸€ä¸ªå¯ä¾›ç”¨æˆ·ä¸‹è½½çš„æ–‡ä»¶ï¼ˆWord, Excel, PDF, PPTç­‰ï¼‰ï¼Œä½ çš„Pythonä»£ç **å¿…é¡»**å°†æ–‡ä»¶å†…å®¹è¿›è¡ŒBase64ç¼–ç ï¼Œå¹¶å°†å…¶åŒ…è£¹åœ¨ä¸€ä¸ªç‰¹å®šæ ¼å¼çš„JSONå¯¹è±¡ä¸­ï¼Œç„¶å `print` è¿™ä¸ªJSONå¯¹è±¡ã€‚\r\n\r\n**å·¥ä½œæµ**:\r\n1. **å¯¼å…¥å¿…è¦åº“**: `io`, `base64`, `json`ã€‚\r\n2. **åœ¨å†…å­˜ä¸­åˆ›å»ºæ–‡ä»¶**: ä½¿ç”¨ `io.BytesIO()` åˆ›å»ºä¸€ä¸ªå†…å­˜ç¼“å†²åŒºã€‚\r\n3. **ä¿å­˜åˆ°å†…å­˜**: è°ƒç”¨ç›¸åº”åº“çš„ `.save(buffer)` æ–¹æ³•å°†æ–‡ä»¶å†…å®¹å†™å…¥å†…å­˜ç¼“å†²åŒºã€‚\r\n4. **ç¼–ç **: å°†ç¼“å†²åŒºä¸­çš„äºŒè¿›åˆ¶æ•°æ®ç¼–ç ä¸ºBase64å­—ç¬¦ä¸²ã€‚\r\n5. **æ‰“åŒ…å¹¶æ‰“å°**: æ„å»ºä¸€ä¸ªåŒ…å« `type` å’Œ `data_base64` å­—æ®µçš„å­—å…¸ï¼Œå¹¶ä½¿ç”¨ `json.dumps()` æ‰“å°å‡ºæ¥ã€‚\r\n\r\n**JSONæ ¼å¼è§„èŒƒ**:\r\n```json\r\n{\r\n    \"type\": \"æ–‡ä»¶ç±»å‹\",  // å¿…é¡»æ˜¯ï¼šword, excel, pdf, ppt ä¹‹ä¸€\r\n    \"title\": \"æ–‡ä»¶å.åç¼€\",\r\n    \"data_base64\": \"Base64ç¼–ç çš„äºŒè¿›åˆ¶æ•°æ®\"\r\n}\r\n```\r\n\r\n---\r\n\r\n## ğŸ“Š Word æŠ¥å‘Šç”Ÿæˆ (.docx)\r\n\r\n### âœ… å¯ç›´æ¥ä½¿ç”¨çš„ä»£ç æ¨¡æ¿\r\n```python\r\nimport io\r\nimport base64\r\nimport json\r\nfrom docx import Document\r\nfrom docx.shared import Inches, Pt\r\nfrom docx.enum.text import WD_ALIGN_PARAGRAPH\r\nfrom docx.enum.table import WD_TABLE_ALIGNMENT\r\nfrom datetime import datetime\r\n\r\n# --- 1. åœ¨å†…å­˜ä¸­æ„å»º Word æ–‡æ¡£ ---\r\ndoc = Document()\r\ndoc.add_heading('ä¸šåŠ¡åˆ†ææŠ¥å‘Š', 0)\r\n\r\n# æ·»åŠ æŠ¥å‘Šä¿¡æ¯\r\np = doc.add_paragraph()\r\np.add_run(f'ç”Ÿæˆæ—¶é—´: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}').bold = True\r\ndoc.add_paragraph('è¿™æ˜¯ä¸€ä¸ªç”±ä»£ç è§£é‡Šå™¨ç”Ÿæˆçš„Wordæ–‡æ¡£ç¤ºä¾‹ã€‚')\r\n\r\n# æ·»åŠ è¡¨æ ¼\r\ntable = doc.add_table(rows=3, cols=3)\r\ntable.style = 'Light Grid Accent 1'\r\ntable.alignment = WD_TABLE_ALIGNMENT.CENTER\r\n\r\n# è®¾ç½®è¡¨å¤´\r\nheader_cells = table.rows[0].cells\r\nheader_cells[0].text = 'é¡¹ç›®'\r\nheader_cells[1].text = 'é¢„ç®—(å…ƒ)'\r\nheader_cells[2].text = 'å®é™…æ”¯å‡º(å…ƒ)'\r\n\r\n# æ·»åŠ æ•°æ®\r\ndata_rows = [\r\n    ('è¥é”€æ´»åŠ¨', '50,000', '48,200'),\r\n    ('ç ”å‘æŠ•å…¥', '200,000', '198,500'),\r\n    ('è¡Œæ”¿è´¹ç”¨', '30,000', '31,200')\r\n]\r\n\r\nfor i, (item, budget, actual) in enumerate(data_rows, 1):\r\n    row_cells = table.rows[i].cells\r\n    row_cells[0].text = item\r\n    row_cells[1].text = budget\r\n    row_cells[2].text = actual\r\n\r\n# æ·»åŠ æ€»ç»“æ®µè½\r\ndoc.add_heading('æ€»ç»“', level=2)\r\ndoc.add_paragraph('æ€»ä½“æ¥çœ‹ï¼Œå„éƒ¨é—¨é¢„ç®—æ‰§è¡Œæƒ…å†µè‰¯å¥½ï¼Œå®é™…æ”¯å‡ºåŸºæœ¬æ§åˆ¶åœ¨é¢„ç®—èŒƒå›´å†…ã€‚')\r\n\r\n# --- 2. ä¿å­˜åˆ°å†…å­˜ç¼“å†²åŒº ---\r\nbuffer = io.BytesIO()\r\ndoc.save(buffer)\r\nbuffer.seek(0)  # é‡ç½®æŒ‡é’ˆåˆ°å¼€å¤´\r\n\r\n# --- 3. Base64 ç¼–ç å¹¶æ‰“åŒ…ä¸º JSON ---\r\ndata_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\r\nresult = {\r\n    \"type\": \"word\",\r\n    \"title\": f\"ä¸šåŠ¡åˆ†ææŠ¥å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx\",\r\n    \"data_base64\": data_base64\r\n}\r\n\r\n# --- 4. æ‰“å°æœ€ç»ˆçš„ JSON å¯¹è±¡ ---\r\nprint(json.dumps(result))\r\n```\r\n\r\n---\r\n\r\n## ğŸ“ˆ Excel æŠ¥å‘Šç”Ÿæˆ (.xlsx)\r\n\r\n### âœ… å¯ç›´æ¥ä½¿ç”¨çš„ä»£ç æ¨¡æ¿\r\n```python\r\nimport io\r\nimport base64\r\nimport json\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom datetime import datetime\r\nfrom openpyxl.styles import Font, Alignment, PatternFill\r\n\r\n# --- 1. åˆ›å»º DataFrame å¹¶å‡†å¤‡ Excel å†…å®¹ ---\r\ndata = {\r\n    'éƒ¨é—¨': ['é”€å”®éƒ¨', 'ç ”å‘éƒ¨', 'å¸‚åœºéƒ¨', 'äººåŠ›èµ„æºéƒ¨', 'è´¢åŠ¡éƒ¨'],\r\n    'é¢„ç®—(å…ƒ)': [500000, 800000, 300000, 200000, 150000],\r\n    'å®é™…æ”¯å‡º(å…ƒ)': [485000, 795000, 310000, 195000, 148000],\r\n    'å·®å¼‚ç‡(%)': [-3.0, -0.6, 3.3, -2.5, -1.3]\r\n}\r\ndf = pd.DataFrame(data)\r\n\r\n# è®¡ç®—æ€»è®¡\r\nsummary_data = {\r\n    'éƒ¨é—¨': ['æ€»è®¡'],\r\n    'é¢„ç®—(å…ƒ)': [df['é¢„ç®—(å…ƒ)'].sum()],\r\n    'å®é™…æ”¯å‡º(å…ƒ)': [df['å®é™…æ”¯å‡º(å…ƒ)'].sum()],\r\n    'å·®å¼‚ç‡(%)': [round((df['å®é™…æ”¯å‡º(å…ƒ)'].sum() - df['é¢„ç®—(å…ƒ)'].sum()) / df['é¢„ç®—(å…ƒ)'].sum() * 100, 2)]\r\n}\r\nsummary_df = pd.DataFrame(summary_data)\r\n\r\n# --- 2. ä½¿ç”¨ ExcelWriter å°† DataFrame å†™å…¥å†…å­˜ç¼“å†²åŒº ---\r\noutput_buffer = io.BytesIO()\r\nwith pd.ExcelWriter(output_buffer, engine='openpyxl') as writer:\r\n    # å†™å…¥è¯¦ç»†æ•°æ®è¡¨\r\n    df.to_excel(writer, sheet_name='éƒ¨é—¨é¢„ç®—è¯¦æƒ…', index=False)\r\n    \r\n    # å†™å…¥æ±‡æ€»è¡¨\r\n    summary_df.to_excel(writer, sheet_name='é¢„ç®—æ±‡æ€»', index=False)\r\n    \r\n    # è·å–å·¥ä½œç°¿å’Œå·¥ä½œè¡¨ä»¥è¿›è¡Œæ ¼å¼è®¾ç½®\r\n    workbook = writer.book\r\n    detail_sheet = writer.sheets['éƒ¨é—¨é¢„ç®—è¯¦æƒ…']\r\n    summary_sheet = writer.sheets['é¢„ç®—æ±‡æ€»']\r\n    \r\n    # è®¾ç½®åˆ—å®½\r\n    for column in detail_sheet.columns:\r\n        max_length = 0\r\n        column_letter = column[0].column_letter\r\n        for cell in column:\r\n            try:\r\n                if len(str(cell.value)) > max_length:\r\n                    max_length = len(str(cell.value))\r\n            except:\r\n                pass\r\n        adjusted_width = min(max_length + 2, 30)\r\n        detail_sheet.column_dimensions[column_letter].width = adjusted_width\r\n        \r\n    # è®¾ç½®æ ‡é¢˜æ ·å¼\r\n    for cell in detail_sheet[1]:\r\n        cell.font = Font(bold=True, size=12)\r\n        cell.fill = PatternFill(start_color=\"C6EFCE\", end_color=\"C6EFCE\", fill_type=\"solid\")\r\n        cell.alignment = Alignment(horizontal='center')\r\n    \r\n    for cell in summary_sheet[1]:\r\n        cell.font = Font(bold=True, size=14, color=\"FFFFFF\")\r\n        cell.fill = PatternFill(start_color=\"366092\", end_color=\"366092\", fill_type=\"solid\")\r\n        cell.alignment = Alignment(horizontal='center')\r\n\r\noutput_buffer.seek(0)\r\n\r\n# --- 3. Base64 ç¼–ç å¹¶æ‰“åŒ…ä¸º JSON ---\r\ndata_base64 = base64.b64encode(output_buffer.getvalue()).decode('utf-8')\r\nresult = {\r\n    \"type\": \"excel\",\r\n    \"title\": f\"éƒ¨é—¨é¢„ç®—æŠ¥å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\",\r\n    \"data_base64\": data_base64\r\n}\r\n\r\n# --- 4. æ‰“å°æœ€ç»ˆçš„ JSON å¯¹è±¡ ---\r\nprint(json.dumps(result))\r\n```\r\n\r\n---\r\n\r\n## ğŸ“Š é«˜çº§Excelæ“ä½œï¼ˆv2.5æ–°å¢ï¼‰\r\n\r\n### ä½¿ç”¨å¤šä¸ªå·¥ä½œè¡¨å’Œæ•°æ®é€è§†è¡¨\r\n```python\r\nimport pandas as pd\r\nimport io\r\nimport base64\r\nimport json\r\nfrom datetime import datetime\r\n\r\ndef create_advanced_excel_report():\r\n    \"\"\"åˆ›å»ºåŒ…å«å¤šä¸ªå·¥ä½œè¡¨å’Œå¤æ‚åˆ†æçš„ExcelæŠ¥å‘Š\"\"\"\r\n    \r\n    # åˆ›å»ºç¤ºä¾‹æ•°æ®\r\n    sales_data = {\r\n        'æ—¥æœŸ': pd.date_range('2024-01-01', periods=30, freq='D'),\r\n        'äº§å“': np.random.choice(['A', 'B', 'C', 'D'], 30),\r\n        'é”€å”®é¢': np.random.randint(1000, 10000, 30),\r\n        'æ•°é‡': np.random.randint(10, 100, 30)\r\n    }\r\n    \r\n    customer_data = {\r\n        'å®¢æˆ·ID': [f'C{1000+i}' for i in range(10)],\r\n        'å®¢æˆ·åç§°': [f'å®¢æˆ·_{i}' for i in range(10)],\r\n        'åœ°åŒº': np.random.choice(['åä¸œ', 'åå—', 'ååŒ—', 'è¥¿å—'], 10),\r\n        'ä¿¡ç”¨è¯„çº§': np.random.choice(['A', 'B', 'C'], 10)\r\n    }\r\n    \r\n    df_sales = pd.DataFrame(sales_data)\r\n    df_customers = pd.DataFrame(customer_data)\r\n    \r\n    # åˆ›å»ºæ•°æ®é€è§†è¡¨\r\n    pivot_table = pd.pivot_table(df_sales, \r\n                                 values='é”€å”®é¢', \r\n                                 index='äº§å“', \r\n                                 columns=df_sales['æ—¥æœŸ'].dt.strftime('%Y-%m-%d'), \r\n                                 aggfunc='sum',\r\n                                 fill_value=0)\r\n    \r\n    # åˆ›å»ºç¼“å†²åŒº\r\n    buffer = io.BytesIO()\r\n    \r\n    with pd.ExcelWriter(buffer, engine='openpyxl') as writer:\r\n        # å†™å…¥åŸå§‹æ•°æ®\r\n        df_sales.to_excel(writer, sheet_name='é”€å”®æ•°æ®', index=False)\r\n        df_customers.to_excel(writer, sheet_name='å®¢æˆ·æ•°æ®', index=False)\r\n        \r\n        # å†™å…¥æ•°æ®é€è§†è¡¨\r\n        pivot_table.to_excel(writer, sheet_name='é”€å”®æ±‡æ€»')\r\n        \r\n        # å†™å…¥åˆ†æç»“æœ\r\n        analysis_data = {\r\n            'æŒ‡æ ‡': ['æ€»é”€å”®é¢', 'å¹³å‡é”€å”®é¢', 'æœ€é«˜é”€å”®é¢', 'æœ€ä½é”€å”®é¢'],\r\n            'æ•°å€¼': [\r\n                df_sales['é”€å”®é¢'].sum(),\r\n                df_sales['é”€å”®é¢'].mean(),\r\n                df_sales['é”€å”®é¢'].max(),\r\n                df_sales['é”€å”®é¢'].min()\r\n            ]\r\n        }\r\n        df_analysis = pd.DataFrame(analysis_data)\r\n        df_analysis.to_excel(writer, sheet_name='åˆ†æç»“æœ', index=False)\r\n    \r\n    buffer.seek(0)\r\n    \r\n    # ç¼–ç å¹¶è¾“å‡º\r\n    data_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\r\n    result = {\r\n        \"type\": \"excel\",\r\n        \"title\": f\"é«˜çº§é”€å”®åˆ†ææŠ¥å‘Š_{datetime.now().strftime('%Y%m%d')}.xlsx\",\r\n        \"data_base64\": data_base64\r\n    }\r\n    \r\n    print(json.dumps(result))\r\n```\r\n\r\n---\r\n\r\n## ğŸ“„ PDF æŠ¥å‘Šç”Ÿæˆ (.pdf)\r\n\r\n### âœ… å¯ç›´æ¥ä½¿ç”¨çš„ä»£ç æ¨¡æ¿\r\n```python\r\nimport io\r\nimport base64\r\nimport json\r\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, Image\r\nfrom reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\r\nfrom reportlab.lib.units import inch\r\nfrom reportlab.lib import colors\r\nfrom reportlab.lib.pagesizes import letter, A4\r\nfrom datetime import datetime\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\n# --- åˆ›å»ºå›¾è¡¨å¹¶ä¿å­˜åˆ°å†…å­˜ ---\r\ndef create_chart_image():\r\n    \"\"\"åˆ›å»ºä¸€ä¸ªç¤ºä¾‹å›¾è¡¨å¹¶è¿”å›Base64ç¼–ç \"\"\"\r\n    plt.figure(figsize=(8, 4))\r\n    categories = ['Q1', 'Q2', 'Q3', 'Q4']\r\n    values = [120, 145, 180, 160]\r\n    plt.bar(categories, values, color=['#2E86AB', '#A23B72', '#F18F01', '#C73E1D'])\r\n    plt.title('å­£åº¦é”€å”®é¢è¶‹åŠ¿')\r\n    plt.xlabel('å­£åº¦')\r\n    plt.ylabel('é”€å”®é¢(ä¸‡å…ƒ)')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # å°†å›¾è¡¨ä¿å­˜åˆ°å†…å­˜\r\n    chart_buffer = io.BytesIO()\r\n    plt.savefig(chart_buffer, format='png', dpi=100, bbox_inches='tight')\r\n    plt.close()\r\n    chart_buffer.seek(0)\r\n    return chart_buffer\r\n\r\n# --- 1. åœ¨å†…å­˜ä¸­æ„å»º PDF æ–‡æ¡£ ---\r\nbuffer = io.BytesIO()\r\ndoc = SimpleDocTemplate(buffer, pagesize=A4, rightMargin=72, leftMargin=72, topMargin=72, bottomMargin=18)\r\nstyles = getSampleStyleSheet()\r\n\r\n# è‡ªå®šä¹‰æ ·å¼\r\ntitle_style = ParagraphStyle(\r\n    'CustomTitle',\r\n    parent=styles['Title'],\r\n    fontSize=24,\r\n    spaceAfter=30,\r\n    alignment=1  # å±…ä¸­\r\n)\r\n\r\nheading_style = ParagraphStyle(\r\n    'CustomHeading',\r\n    parent=styles['Heading2'],\r\n    fontSize=16,\r\n    spaceBefore=20,\r\n    spaceAfter=10\r\n)\r\n\r\n# æ„å»ºå†…å®¹\r\nstory = []\r\nstory.append(Paragraph(\"å…¬å¸å¹´åº¦è´¢åŠ¡æŠ¥å‘Š\", title_style))\r\nstory.append(Paragraph(f\"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}\", styles['Normal']))\r\nstory.append(Spacer(1, 20))\r\n\r\n# æ·»åŠ æ‘˜è¦\r\nstory.append(Paragraph(\"æ‰§è¡Œæ‘˜è¦\", heading_style))\r\nstory.append(Paragraph(\"æœ¬æŠ¥å‘Šè¯¦ç»†åˆ†æäº†å…¬å¸2024å¹´åº¦çš„è´¢åŠ¡çŠ¶å†µå’Œä¸šåŠ¡è¡¨ç°ï¼ŒåŒ…æ‹¬æ”¶å…¥ã€æ”¯å‡ºã€åˆ©æ¶¦ç­‰å…³é”®æŒ‡æ ‡ã€‚\", styles['BodyText']))\r\nstory.append(Spacer(1, 15))\r\n\r\n# æ·»åŠ å›¾è¡¨\r\nchart_buffer = create_chart_image()\r\nstory.append(Paragraph(\"å­£åº¦é”€å”®è¶‹åŠ¿\", heading_style))\r\nstory.append(Image(chart_buffer, width=6*inch, height=3*inch))\r\nstory.append(Spacer(1, 15))\r\n\r\n# æ·»åŠ è¡¨æ ¼\r\nstory.append(Paragraph(\"è´¢åŠ¡æ•°æ®æ±‡æ€»\", heading_style))\r\ndata = [\r\n    ['é¡¹ç›®', 'Q1', 'Q2', 'Q3', 'Q4', 'å¹´åº¦æ€»è®¡'],\r\n    ['æ”¶å…¥(ä¸‡å…ƒ)', '450', '520', '610', '580', '2160'],\r\n    ['æˆæœ¬(ä¸‡å…ƒ)', '280', '310', '350', '320', '1260'],\r\n    ['åˆ©æ¶¦(ä¸‡å…ƒ)', '170', '210', '260', '260', '900'],\r\n    ['åˆ©æ¶¦ç‡(%)', '37.8', '40.4', '42.6', '44.8', '41.7']\r\n]\r\n\r\ntable = Table(data, colWidths=[1.5*inch, 1*inch, 1*inch, 1*inch, 1*inch, 1.2*inch])\r\ntable.setStyle(TableStyle([\r\n    ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\r\n    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\r\n    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\r\n    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\r\n    ('FONTSIZE', (0, 0), (-1, 0), 12),\r\n    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\r\n    ('BACKGROUND', (0, 1), (-1, -1), colors.beige),\r\n    ('GRID', (0, 0), (-1, -1), 1, colors.black),\r\n    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\r\n    ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),\r\n]))\r\n\r\nstory.append(table)\r\nstory.append(Spacer(1, 20))\r\n\r\n# æ·»åŠ ç»“è®º\r\nstory.append(Paragraph(\"ç»“è®ºä¸å»ºè®®\", heading_style))\r\nstory.append(Paragraph(\"1. å…¬å¸å…¨å¹´æ”¶å…¥ç¨³æ­¥å¢é•¿ï¼Œç¬¬å››å­£åº¦ç•¥æœ‰å›è½ä½†æ•´ä½“è¡¨ç°è‰¯å¥½ã€‚\", styles['BodyText']))\r\nstory.append(Paragraph(\"2. åˆ©æ¶¦ç‡é€å­£åº¦æå‡ï¼Œæ˜¾ç¤ºæˆæœ¬æ§åˆ¶æªæ–½æ•ˆæœæ˜¾è‘—ã€‚\", styles['BodyText']))\r\nstory.append(Paragraph(\"3. å»ºè®®æ˜å¹´åŠ å¤§ç ”å‘æŠ•å…¥ï¼Œä¼˜åŒ–äº§å“ç»“æ„ï¼Œè¿›ä¸€æ­¥æå‡ç›ˆåˆ©èƒ½åŠ›ã€‚\", styles['BodyText']))\r\n\r\n# æ„å»ºæ–‡æ¡£\r\ndoc.build(story)\r\nbuffer.seek(0)\r\n\r\n# --- 2. Base64 ç¼–ç å¹¶æ‰“åŒ…ä¸º JSON ---\r\ndata_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\r\nresult = {\r\n    \"type\": \"pdf\",\r\n    \"title\": f\"å…¬å¸å¹´åº¦è´¢åŠ¡æŠ¥å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf\",\r\n    \"data_base64\": data_base64\r\n}\r\n\r\n# --- 3. æ‰“å°æœ€ç»ˆçš„ JSON å¯¹è±¡ ---\r\nprint(json.dumps(result))\r\n```\r\n\r\n---\r\n\r\n## ğŸ¤ PowerPoint æŠ¥å‘Šç”Ÿæˆ (.pptx) - v2.5æ–°å¢\r\n\r\n### âœ… å¯ç›´æ¥ä½¿ç”¨çš„ä»£ç æ¨¡æ¿\r\n```python\r\nimport io\r\nimport base64\r\nimport json\r\nfrom pptx import Presentation\r\nfrom pptx.util import Inches, Pt\r\nfrom pptx.enum.text import PP_ALIGN\r\nfrom pptx.dml.color import RGBColor\r\nfrom datetime import datetime\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\n# --- 1. åœ¨å†…å­˜ä¸­æ„å»º PowerPoint æ–‡æ¡£ ---\r\nprs = Presentation()\r\n\r\n# åˆ›å»ºæ ‡é¢˜é¡µ\r\ntitle_slide_layout = prs.slide_layouts[0]\r\nslide = prs.slides.add_slide(title_slide_layout)\r\ntitle = slide.shapes.title\r\nsubtitle = slide.placeholders[1]\r\n\r\ntitle.text = \"å­£åº¦ä¸šåŠ¡æ±‡æŠ¥\"\r\nsubtitle.text = f\"{datetime.now().strftime('%Yå¹´%mæœˆ')}\\næ•°æ®åˆ†æå›¢é˜Ÿ\"\r\n\r\n# åˆ›å»ºç›®å½•é¡µ\r\nbullet_slide_layout = prs.slide_layouts[1]\r\nslide = prs.slides.add_slide(bullet_slide_layout)\r\nshapes = slide.shapes\r\n\r\ntitle_shape = shapes.title\r\ntitle_shape.text = 'æ±‡æŠ¥ç›®å½•'\r\n\r\nbody_shape = shapes.placeholders[1]\r\ntf = body_shape.text_frame\r\ntf.text = '1. ä¸šç»©æ¦‚è§ˆ'\r\np = tf.add_paragraph()\r\np.text = '2. å¸‚åœºåˆ†æ'\r\np = tf.add_paragraph()\r\np.text = '3. è´¢åŠ¡æ•°æ®'\r\np = tf.add_paragraph()\r\np.text = '4. æœªæ¥å±•æœ›'\r\n\r\n# åˆ›å»ºå›¾è¡¨é¡µ - ä¸šç»©æ¦‚è§ˆ\r\nslide = prs.slides.add_slide(prs.slide_layouts[5])\r\ntitle = slide.shapes.title\r\ntitle.text = \"ä¸šç»©æ¦‚è§ˆ\"\r\n\r\n# åœ¨å†…å­˜ä¸­åˆ›å»ºå›¾è¡¨\r\nplt.figure(figsize=(6, 4))\r\nmonths = ['1æœˆ', '2æœˆ', '3æœˆ', '4æœˆ', '5æœˆ', '6æœˆ']\r\nsales = [120, 135, 150, 145, 160, 180]\r\ntargets = [110, 130, 140, 150, 155, 170]\r\n\r\nx = np.arange(len(months))\r\nwidth = 0.35\r\n\r\nfig, ax = plt.subplots()\r\nrects1 = ax.bar(x - width/2, sales, width, label='å®é™…é”€å”®é¢', color='#2E86AB')\r\nrects2 = ax.bar(x + width/2, targets, width, label='ç›®æ ‡é”€å”®é¢', color='#A23B72')\r\n\r\nax.set_xlabel('æœˆä»½')\r\nax.set_ylabel('é”€å”®é¢(ä¸‡å…ƒ)')\r\nax.set_title('ä¸ŠåŠå¹´é”€å”®é¢å¯¹æ¯”')\r\nax.set_xticks(x)\r\nax.set_xticklabels(months)\r\nax.legend()\r\nax.grid(True, alpha=0.3)\r\n\r\n# ä¿å­˜å›¾è¡¨åˆ°å†…å­˜\r\nchart_buffer = io.BytesIO()\r\nplt.savefig(chart_buffer, format='png', dpi=150, bbox_inches='tight')\r\nplt.close()\r\nchart_buffer.seek(0)\r\n\r\n# æ·»åŠ å›¾è¡¨åˆ°å¹»ç¯ç‰‡\r\nleft = Inches(1)\r\ntop = Inches(1.5)\r\npic = slide.shapes.add_picture(chart_buffer, left, top, width=Inches(8), height=Inches(4.5))\r\n\r\n# åˆ›å»ºæ•°æ®é¡µ - è´¢åŠ¡æ•°æ®\r\nslide = prs.slides.add_slide(prs.slide_layouts[1])\r\ntitle = slide.shapes.title\r\ntitle.text = \"è´¢åŠ¡æ•°æ®\"\r\n\r\nbody_shape = slide.shapes.placeholders[1]\r\ntf = body_shape.text_frame\r\ntf.text = 'æ”¶å…¥æƒ…å†µ:'\r\np = tf.add_paragraph()\r\np.text = 'â€¢ æ€»æ”¶å…¥: 850ä¸‡å…ƒ'\r\np = tf.add_paragraph()\r\np.text = 'â€¢ åŒæ¯”å¢é•¿: 15.2%'\r\np = tf.add_paragraph()\r\np.text = 'â€¢ æ¯›åˆ©ç‡: 42.3%'\r\n\r\np = tf.add_paragraph()\r\np.text = 'æˆæœ¬åˆ†æ:'\r\np = tf.add_paragraph()\r\np.text = 'â€¢ æ€»æˆæœ¬: 490ä¸‡å…ƒ'\r\np = tf.add_paragraph()\r\np.text = 'â€¢ äººåŠ›æˆæœ¬: 45%'\r\np = tf.add_paragraph()\r\np.text = 'â€¢ è¥é”€è´¹ç”¨: 28%'\r\n\r\n# åˆ›å»ºæ€»ç»“é¡µ\r\nslide = prs.slides.add_slide(prs.slide_layouts[1])\r\ntitle = slide.shapes.title\r\ntitle.text = \"æ€»ç»“ä¸å±•æœ›\"\r\n\r\nbody_shape = slide.shapes.placeholders[1]\r\ntf = body_shape.text_frame\r\ntf.text = 'æ ¸å¿ƒæˆæœ:'\r\np = tf.add_paragraph()\r\np.text = 'âœ“ è¶…é¢å®Œæˆä¸ŠåŠå¹´é”€å”®ç›®æ ‡'\r\np = tf.add_paragraph()\r\np.text = 'âœ“ å¸‚åœºå æœ‰ç‡æå‡è‡³18.5%'\r\np = tf.add_paragraph()\r\np.text = 'âœ“ å®¢æˆ·æ»¡æ„åº¦è¾¾åˆ°92%'\r\n\r\np = tf.add_paragraph()\r\np.text = 'ä¸‹ä¸€æ­¥è®¡åˆ’:'\r\np = tf.add_paragraph()\r\np.text = 'â€¢ æ‹“å±•æ–°å¸‚åœºï¼Œç›®æ ‡å¢é•¿20%'\r\np = tf.add_paragraph()\r\np.text = 'â€¢ ä¼˜åŒ–ä¾›åº”é“¾ï¼Œé™ä½è¿è¥æˆæœ¬'\r\np = tf.add_paragraph()\r\np.text = 'â€¢ åŠ å¼ºäººæ‰åŸ¹å…»ï¼Œæå‡å›¢é˜Ÿèƒ½åŠ›'\r\n\r\n# --- 2. ä¿å­˜åˆ°å†…å­˜ç¼“å†²åŒº ---\r\nbuffer = io.BytesIO()\r\nprs.save(buffer)\r\nbuffer.seek(0)\r\n\r\n# --- 3. Base64 ç¼–ç å¹¶æ‰“åŒ…ä¸º JSON ---\r\ndata_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\r\nresult = {\r\n    \"type\": \"ppt\",\r\n    \"title\": f\"å­£åº¦ä¸šåŠ¡æ±‡æŠ¥_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pptx\",\r\n    \"data_base64\": data_base64\r\n}\r\n\r\n# --- 4. æ‰“å°æœ€ç»ˆçš„ JSON å¯¹è±¡ ---\r\nprint(json.dumps(result))\r\n```\r\n\r\n---\r\n\r\n## ğŸ“ å¤åˆæŠ¥å‘Šç”Ÿæˆï¼ˆWord + Excel + PDFï¼‰\r\n\r\n### âœ… å®Œæ•´å·¥ä½œæµç¤ºä¾‹\r\n```python\r\nimport io\r\nimport base64\r\nimport json\r\nimport pandas as pd\r\nfrom docx import Document\r\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\r\nfrom reportlab.lib.styles import getSampleStyleSheet\r\nfrom datetime import datetime\r\n\r\ndef generate_comprehensive_report():\r\n    \"\"\"ç”ŸæˆåŒ…å«Wordæ‘˜è¦ã€Excelè¯¦ç»†æ•°æ®å’ŒPDFæŠ¥å‘Šçš„å®Œæ•´åˆ†æåŒ…\"\"\"\r\n    \r\n    # åˆ›å»ºç¤ºä¾‹æ•°æ®\r\n    data = {\r\n        'æŒ‡æ ‡': ['æ”¶å…¥', 'æˆæœ¬', 'åˆ©æ¶¦', 'åˆ©æ¶¦ç‡', 'å¢é•¿ç‡'],\r\n        'Q1': [450, 280, 170, '37.8%', '12.5%'],\r\n        'Q2': [520, 310, 210, '40.4%', '15.6%'],\r\n        'Q3': [610, 350, 260, '42.6%', '23.8%'],\r\n        'Q4': [580, 320, 260, '44.8%', '0%']\r\n    }\r\n    df = pd.DataFrame(data)\r\n    \r\n    # 1. ç”ŸæˆWordæ‘˜è¦æŠ¥å‘Š\r\n    doc = Document()\r\n    doc.add_heading('å­£åº¦åˆ†ææ‘˜è¦', 0)\r\n    doc.add_paragraph(f'ç”Ÿæˆæ—¶é—´: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\r\n    doc.add_paragraph('æœ¬æŠ¥å‘ŠåŸºäº2024å¹´å››ä¸ªå­£åº¦çš„è´¢åŠ¡æ•°æ®ï¼Œåˆ†æäº†å…¬å¸çš„æ•´ä½“ç»è¥çŠ¶å†µã€‚')\r\n    \r\n    word_buffer = io.BytesIO()\r\n    doc.save(word_buffer)\r\n    word_buffer.seek(0)\r\n    \r\n    # 2. ç”ŸæˆExcelè¯¦ç»†æ•°æ®\r\n    excel_buffer = io.BytesIO()\r\n    with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:\r\n        df.to_excel(writer, sheet_name='å­£åº¦è´¢åŠ¡æ•°æ®', index=False)\r\n        \r\n        # æ·»åŠ è®¡ç®—è¡¨\r\n        summary_df = pd.DataFrame({\r\n            'å¹´åº¦æŒ‡æ ‡': ['æ€»æ”¶å…¥', 'æ€»æˆæœ¬', 'æ€»åˆ©æ¶¦', 'å¹³å‡åˆ©æ¶¦ç‡'],\r\n            'æ•°å€¼': [df[['Q1','Q2','Q3','Q4']].sum().sum(), \r\n                    df[['Q1','Q2','Q3','Q4']].iloc[1].sum(),\r\n                    df[['Q1','Q2','Q3','Q4']].iloc[2].sum(),\r\n                    '41.4%']\r\n        })\r\n        summary_df.to_excel(writer, sheet_name='å¹´åº¦æ±‡æ€»', index=False)\r\n    \r\n    excel_buffer.seek(0)\r\n    \r\n    # 3. ç”ŸæˆPDFæŠ¥å‘Š\r\n    pdf_buffer = io.BytesIO()\r\n    doc_pdf = SimpleDocTemplate(pdf_buffer)\r\n    styles = getSampleStyleSheet()\r\n    story = [\r\n        Paragraph('2024å¹´åº¦è´¢åŠ¡åˆ†ææŠ¥å‘Š', styles['Title']),\r\n        Spacer(1, 20),\r\n        Paragraph('åŸºäºå­£åº¦æ•°æ®çš„æ·±åº¦åˆ†æ', styles['Heading2']),\r\n        Spacer(1, 15),\r\n        Paragraph('æŠ¥å‘Šæ€»ç»“äº†å…¬å¸2024å¹´åº¦çš„ç»è¥è¡¨ç°ï¼Œå¹¶å¯¹æœªæ¥å‘å±•è¶‹åŠ¿è¿›è¡Œäº†å±•æœ›ã€‚', styles['Normal'])\r\n    ]\r\n    doc_pdf.build(story)\r\n    pdf_buffer.seek(0)\r\n    \r\n    # è¿”å›æ‰€æœ‰æ–‡ä»¶ï¼ˆå®é™…ä½¿ç”¨æ—¶ï¼Œä¸€æ¬¡åªèƒ½è¿”å›ä¸€ä¸ªæ–‡ä»¶ï¼‰\r\n    # è¿™é‡Œæ¼”ç¤ºå¦‚ä½•æ„å»ºå¤šä¸ªæ–‡ä»¶ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦åˆ†åˆ«æ‰§è¡Œ\r\n    files_info = [\r\n        {\r\n            \"type\": \"word\",\r\n            \"title\": \"åˆ†ææ‘˜è¦.docx\",\r\n            \"data_base64\": base64.b64encode(word_buffer.getvalue()).decode('utf-8')\r\n        },\r\n        {\r\n            \"type\": \"excel\", \r\n            \"title\": \"è¯¦ç»†æ•°æ®.xlsx\",\r\n            \"data_base64\": base64.b64encode(excel_buffer.getvalue()).decode('utf-8')\r\n        },\r\n        {\r\n            \"type\": \"pdf\",\r\n            \"title\": \"å®Œæ•´æŠ¥å‘Š.pdf\",\r\n            \"data_base64\": base64.b64encode(pdf_buffer.getvalue()).decode('utf-8')\r\n        }\r\n    ]\r\n    \r\n    print(\"æ³¨æ„ï¼šä¸€æ¬¡åªèƒ½è¿”å›ä¸€ä¸ªæ–‡ä»¶ï¼Œä»¥ä¸‹æ˜¯ä¸‰ä¸ªæ–‡ä»¶çš„JSONç¤ºä¾‹ï¼š\")\r\n    for i, file_info in enumerate(files_info):\r\n        print(f\"\\næ–‡ä»¶{i+1} JSON:\")\r\n        print(json.dumps(file_info, indent=2))\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\nif __name__ == \"__main__\":\r\n    generate_comprehensive_report()\r\n```\r\n\r\n---\r\n\r\n## âš ï¸ é‡è¦æ³¨æ„äº‹é¡¹\r\n\r\n### âœ… å¿…é¡»åšçš„:\r\n1. **å•ä¸€è¾“å‡º**: æ¯ä¸ªä»£ç æ‰§è¡Œåªèƒ½è¾“å‡ºä¸€ä¸ªJSONå¯¹è±¡ï¼ˆä¸€ä¸ªæ–‡ä»¶ï¼‰\r\n2. **Base64ç¼–ç **: å¿…é¡»ä½¿ç”¨`base64.b64encode().decode('utf-8')`è¿›è¡Œç¼–ç \r\n3. **æ–‡ä»¶åè§„èŒƒ**: æ–‡ä»¶ååº”åŒ…å«æ—¶é—´æˆ³ï¼Œé¿å…é‡å¤ï¼š`f\"æŠ¥å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx\"`\r\n4. **ç¼–ç ä¸€è‡´æ€§**: ä¸­æ–‡å­—ç¬¦ä½¿ç”¨`ensure_ascii=False`å‚æ•°ï¼ˆä½†åœ¨æ²™ç›’ä¸­ä¼šè‡ªåŠ¨å¤„ç†ï¼‰\r\n\r\n### âŒ ç»å¯¹ç¦æ­¢:\r\n1. **ç¦æ­¢ä¿å­˜åˆ°ç£ç›˜**: ä¸è¦ä½¿ç”¨`doc.save('filename.docx')`æˆ–`wb.save('filename.xlsx')`\r\n2. **ç¦æ­¢å¤šæ¬¡è¾“å‡º**: ä¸è¦åœ¨ä¸€æ¬¡æ‰§è¡Œä¸­ç”Ÿæˆå¤šä¸ªæ–‡ä»¶\r\n3. **ç¦æ­¢æ··åˆè¾“å‡º**: ä¸è¦åœ¨æ‰“å°JSONåæ‰“å°å…¶ä»–å†…å®¹\r\n4. **ç¦æ­¢è·¯å¾„è®¿é—®**: ä¸è¦å°è¯•è®¿é—®é™¤`/data`ç›®å½•å¤–çš„æ–‡ä»¶ç³»ç»Ÿ\r\n\r\n### ğŸ”§ æœ€ä½³å®è·µ:\r\n1. **ä½¿ç”¨å†…å­˜ç¼“å†²åŒº**: å§‹ç»ˆä½¿ç”¨`io.BytesIO()`åœ¨å†…å­˜ä¸­æ“ä½œæ–‡ä»¶\r\n2. **åŠæ—¶é‡Šæ”¾èµ„æº**: ä½¿ç”¨`buffer.seek(0)`é‡ç½®æŒ‡é’ˆ\r\n3. **åŒ…å«æ—¶é—´æˆ³**: åœ¨æ–‡ä»¶åä¸­æ·»åŠ æ—¶é—´æˆ³ï¼Œé¿å…å†²çª\r\n4. **æµ‹è¯•ä»£ç **: åœ¨ç”Ÿæˆå¤æ‚æŠ¥å‘Šå‰ï¼Œå…ˆæµ‹è¯•å›¾è¡¨ç”Ÿæˆå’Œæ•°æ®å¤„ç†éƒ¨åˆ†\r\n5. **åˆ†æ­¥éªŒè¯**: å¯¹äºå¤æ‚æŠ¥å‘Šï¼Œå¯ä»¥å…ˆéªŒè¯å„éƒ¨åˆ†åŠŸèƒ½å†æ•´åˆ\r\n\r\n### ğŸ“Š å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ:\r\n\r\n| é”™è¯¯ç±»å‹ | åŸå›  | è§£å†³æ–¹æ¡ˆ |\r\n|---------|------|----------|\r\n| JSONè§£æå¤±è´¥ | æ‰“å°äº†é¢å¤–å†…å®¹ | ç¡®ä¿åªæ‰“å°ä¸€ä¸ªJSONå­—ç¬¦ä¸² |\r\n| æ–‡ä»¶æŸå | Base64ç¼–ç é”™è¯¯ | ä½¿ç”¨æ­£ç¡®çš„`.decode('utf-8')` |\r\n| å†…å­˜ä¸è¶³ | æ–‡ä»¶å¤ªå¤§ | å‹ç¼©å›¾ç‰‡ï¼Œå‡å°‘æ•°æ®é‡ |\r\n| ä¸­æ–‡ä¹±ç  | ç¼–ç é—®é¢˜ | ç¡®ä¿ä½¿ç”¨UTF-8ç¼–ç  |\r\n\r\n---\r\n\r\n## ğŸ¯ å¿«é€Ÿå‚è€ƒè¡¨\r\n\r\n| æ–‡ä»¶ç±»å‹ | ä¸»è¦åº“ | è¾“å‡ºç±»å‹ | å¤‡æ³¨ |\r\n|---------|--------|----------|------|\r\n| Word (.docx) | `python-docx` | `\"type\": \"word\"` | æ”¯æŒè¡¨æ ¼ã€å›¾ç‰‡ã€æ ·å¼ |\r\n| Excel (.xlsx) | `openpyxl` + `pandas` | `\"type\": \"excel\"` | æ”¯æŒå¤šä¸ªsheetã€æ ¼å¼ |\r\n| PDF (.pdf) | `reportlab` | `\"type\": \"pdf\"` | æ”¯æŒå›¾è¡¨ã€è¡¨æ ¼ã€æ ·å¼ |\r\n| PowerPoint (.pptx) | `python-pptx` | `\"type\": \"ppt\"` | æ”¯æŒå¹»ç¯ç‰‡ã€å›¾è¡¨ |\r\n\r\n---\r\n\r\n## ğŸ”„ å·¥ä½œæµæ€»ç»“\r\n\r\n1. **å‡†å¤‡æ•°æ®**: ä»`/data`ç›®å½•è¯»å–æˆ–ç”Ÿæˆåˆ†ææ•°æ®\r\n2. **åˆ›å»ºæ–‡æ¡£**: ä½¿ç”¨ç›¸åº”åº“åœ¨å†…å­˜ä¸­æ„å»ºæ–‡æ¡£\r\n3. **æ·»åŠ å†…å®¹**: æ’å…¥æ–‡æœ¬ã€è¡¨æ ¼ã€å›¾è¡¨ã€æ ¼å¼ç­‰\r\n4. **ä¿å­˜åˆ°å†…å­˜**: ä½¿ç”¨`io.BytesIO()`ä¿å­˜æ–‡æ¡£\r\n5. **Base64ç¼–ç **: å°†äºŒè¿›åˆ¶æ•°æ®ç¼–ç ä¸ºå­—ç¬¦ä¸²\r\n6. **æ„å»ºJSON**: åˆ›å»ºåŒ…å«ç±»å‹ã€æ–‡ä»¶åå’Œæ•°æ®çš„å­—å…¸\r\n7. **è¾“å‡ºç»“æœ**: ä½¿ç”¨`print(json.dumps())`è¾“å‡ºå•ä¸ªJSONå¯¹è±¡\r\n\r\n**è®°ä½**: ç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†JSONè¾“å‡ºå¹¶æç¤ºç”¨æˆ·ä¸‹è½½æ–‡ä»¶ï¼\r\n\n\n### ğŸ“– scipy_cookbook\n\n# SciPy ç§‘å­¦è®¡ç®—æŒ‡å— (v2.5)\n\n## ğŸ¯ å·¥å…·æ¦‚è¿°\n**ç¯å¢ƒç‰¹æ€§**ï¼šåŸºäº SciPy çš„ç§‘å­¦è®¡ç®—ç¯å¢ƒï¼Œæ”¯æŒä¼˜åŒ–ã€ç§¯åˆ†ã€ä¿¡å·å¤„ç†ç­‰\n**è¾“å‡ºåŸåˆ™**ï¼šç³»ç»Ÿè‡ªåŠ¨å¤„ç†ç»“æœè¾“å‡ºï¼Œç›´æ¥æ‰“å°ç»“æœï¼Œå›¾è¡¨ä½¿ç”¨ `plt.show()`\n\n## ğŸ”§ æ ¸å¿ƒæ¨¡å—æ¦‚è§ˆ\n\n### ä¸»è¦åŠŸèƒ½æ¨¡å—ï¼š\n- **ä¼˜åŒ–ç®—æ³•** (`scipy.optimize`) - å‡½æ•°æœ€å°åŒ–ã€æ–¹ç¨‹æ±‚è§£\n- **ç§¯åˆ†è®¡ç®—** (`scipy.integrate`) - æ•°å€¼ç§¯åˆ†ã€å¾®åˆ†æ–¹ç¨‹\n- **ä¿¡å·å¤„ç†** (`scipy.signal`) - æ»¤æ³¢å™¨ã€é¢‘è°±åˆ†æ\n- **çº¿æ€§ä»£æ•°** (`scipy.linalg`) - çŸ©é˜µè¿ç®—ã€çº¿æ€§ç³»ç»Ÿ\n- **ç»Ÿè®¡å‡½æ•°** (`scipy.stats`) - æ¦‚ç‡åˆ†å¸ƒã€ç»Ÿè®¡æ£€éªŒ\n- **ç©ºé—´ç®—æ³•** (`scipy.spatial`) - ç©ºé—´æ•°æ®ã€è·ç¦»è®¡ç®—\n\n## âœ… ä»£ç è§£é‡Šå™¨é€‚é…è¯´æ˜\n- **ç›´æ¥æ‰“å°**ï¼šæ‰€æœ‰è®¡ç®—ç»“æœç›´æ¥ä½¿ç”¨ `print()` è¾“å‡º\n- **è‡ªåŠ¨å›¾è¡¨**ï¼šä½¿ç”¨ `plt.show()` å³å¯è‡ªåŠ¨æ•è·å›¾è¡¨\n- **å®Œæ•´é›†æˆ**ï¼šSciPy å·²é¢„è£…ï¼Œæ— éœ€é¢å¤–å®‰è£…\n- **å†…å­˜ä¼˜åŒ–**ï¼šå¤§è®¡ç®—æ—¶æ³¨æ„å†…å­˜ä½¿ç”¨ï¼Œå¯åˆ†æ‰¹å¤„ç†\n\n## ğŸ¯ ä¼˜åŒ–ä¸æ–¹ç¨‹æ±‚è§£\n\n### å‡½æ•°æœ€å°åŒ–\n```python\nimport numpy as np\nfrom scipy import optimize\nimport matplotlib.pyplot as plt\n\nprint(\"=== å•å˜é‡å‡½æ•°ä¼˜åŒ– ===\")\n\n# 1. å•å˜é‡å‡½æ•°ä¼˜åŒ–\ndef single_variable_func(x):\n    return (x - 3)**2 * np.sin(x) + x**2\n\nresult = optimize.minimize_scalar(single_variable_func, bounds=(0, 10), method='bounded')\nprint(f\"æœ€ä¼˜è§£: x = {result.x:.4f}, å‡½æ•°å€¼: {result.fun:.4f}\")\n\n# å¯è§†åŒ–\nx_plot = np.linspace(0, 10, 100)\ny_plot = single_variable_func(x_plot)\nplt.figure(figsize=(10, 6))\nplt.plot(x_plot, y_plot, label='f(x)')\nplt.axvline(result.x, color='red', linestyle='--', label=f'æœ€ä¼˜è§£ x={result.x:.3f}')\nplt.title('å•å˜é‡å‡½æ•°ä¼˜åŒ–')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n\n### å¤šå˜é‡ä¼˜åŒ–\n```python\nimport numpy as np\nfrom scipy import optimize\nimport matplotlib.pyplot as plt\n\nprint(\"=== å¤šå˜é‡å‡½æ•°ä¼˜åŒ– ===\")\n\n# Rosenbrock å‡½æ•°ä¼˜åŒ–\ndef rosenbrock(x):\n    return sum(100.0 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\n\nx0 = np.array([-1.2, 1.0])\nresult = optimize.minimize(rosenbrock, x0, method='BFGS')\n\nprint(f\"åˆå§‹ç‚¹: {x0}\")\nprint(f\"æœ€ä¼˜ç‚¹: {result.x}\")\nprint(f\"æœ€ä¼˜å€¼: {result.fun:.6f}\")\nprint(f\"è¿­ä»£æ¬¡æ•°: {result.nit}\")\nprint(f\"æ±‚è§£æˆåŠŸ: {result.success}\")\n\n# å¯è§†åŒ–\nx = np.linspace(-2, 2, 100)\ny = np.linspace(-1, 3, 100)\nX, Y = np.meshgrid(x, y)\nZ = np.zeros_like(X)\n\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        Z[i,j] = rosenbrock([X[i,j], Y[i,j]])\n\nplt.figure(figsize=(10, 8))\ncontour = plt.contour(X, Y, Z, levels=50)\nplt.clabel(contour, inline=True, fontsize=8)\nplt.plot(result.x[0], result.x[1], 'ro', markersize=10, label='æœ€ä¼˜è§£')\nplt.title('Rosenbrock å‡½æ•°ä¼˜åŒ–')\nplt.legend()\nplt.tight_layout()\nplt.show()\n```\n\n### çº¦æŸä¼˜åŒ–\n```python\nimport numpy as np\nfrom scipy import optimize\nimport matplotlib.pyplot as plt\n\nprint(\"=== çº¦æŸä¼˜åŒ– ===\")\n\n# å¸¦çº¦æŸçš„ä¼˜åŒ–é—®é¢˜\ndef objective(x):\n    return x[0]**2 + x[1]**2\n\ndef constraint1(x):\n    return x[0] + x[1] - 1  # x + y >= 1\n\nconstraints = [{'type': 'ineq', 'fun': constraint1}]\nbounds = [(0, None), (0, None)]\n\nresult = optimize.minimize(objective, [0.5, 0.5], \n                         method='SLSQP', bounds=bounds, \n                         constraints=constraints)\n\nprint(f\"çº¦æŸä¼˜åŒ–ç»“æœ:\")\nprint(f\"æœ€ä¼˜ç‚¹: {result.x}\")\nprint(f\"æœ€ä¼˜å€¼: {result.fun:.4f}\")\nprint(f\"çº¦æŸæ»¡è¶³: {result.success}\")\nprint(f\"è¿­ä»£æ¬¡æ•°: {result.nit}\")\n\n# å¯è§†åŒ–çº¦æŸåŒºåŸŸ\nx_const = np.linspace(0, 2, 100)\ny_const = np.linspace(0, 2, 100)\nX, Y = np.meshgrid(x_const, y_const)\nZ = objective([X, Y])\n\nplt.figure(figsize=(10, 8))\nplt.contourf(X, Y, Z, levels=20, alpha=0.6)\nplt.contour(X, Y, Z, levels=10, colors='black', alpha=0.4)\n\n# ç»˜åˆ¶çº¦æŸæ¡ä»¶\ny_constraint = 1 - x_const\nplt.plot(x_const, y_constraint, 'r-', linewidth=2, label='x + y = 1')\nplt.fill_between(x_const, y_constraint, 2, alpha=0.3, color='gray', label='å¯è¡ŒåŸŸ')\n\nplt.plot(result.x[0], result.x[1], 'go', markersize=10, label='æœ€ä¼˜è§£')\nplt.xlim(0, 2)\nplt.ylim(0, 2)\nplt.title('çº¦æŸä¼˜åŒ–é—®é¢˜')\nplt.legend()\nplt.tight_layout()\nplt.show()\n```\n\n### æ–¹ç¨‹æ±‚è§£\n```python\nimport numpy as np\nfrom scipy import optimize\nimport matplotlib.pyplot as plt\n\nprint(\"=== éçº¿æ€§æ–¹ç¨‹æ±‚è§£ ===\")\n\n# å®šä¹‰éçº¿æ€§æ–¹ç¨‹ç»„\ndef equations(vars):\n    x, y = vars\n    eq1 = x**2 + y**2 - 25\n    eq2 = x**2 - y - 5\n    return [eq1, eq2]\n\n# åˆå§‹çŒœæµ‹\ninitial_guess = [4, 2]\n\n# æ±‚è§£æ–¹ç¨‹ç»„\nresult = optimize.root(equations, initial_guess, method='hybr')\n\nprint(f\"æ±‚è§£ç»“æœ:\")\nprint(f\"è§£: x = {result.x[0]:.4f}, y = {result.x[1]:.4f}\")\nprint(f\"å‡½æ•°å€¼: {result.fun}\")\nprint(f\"æ±‚è§£æˆåŠŸ: {result.success}\")\n\n# å¯è§†åŒ–\nfig, ax = plt.subplots(figsize=(8, 8))\ncircle = plt.Circle((0, 0), 5, color='blue', fill=False, linewidth=2, label='xÂ² + yÂ² = 25')\nax.add_patch(circle)\n\nx_parabola = np.linspace(-5, 5, 100)\ny_parabola = x_parabola**2 - 5\nax.plot(x_parabola, y_parabola, 'r-', linewidth=2, label='xÂ² - y = 5')\n\n# ç»˜åˆ¶äº¤ç‚¹\nax.plot(result.x[0], result.x[1], 'go', markersize=10, label='è§£')\nax.text(result.x[0]+0.2, result.x[1]+0.2, f'({result.x[0]:.2f}, {result.x[1]:.2f})')\n\nax.set_xlim(-6, 6)\nax.set_ylim(-6, 6)\nax.set_aspect('equal')\nax.grid(True, alpha=0.3)\nax.legend()\nplt.title('éçº¿æ€§æ–¹ç¨‹ç»„æ±‚è§£')\nplt.tight_layout()\nplt.show()\n```\n\n## ğŸ“ æ•°å€¼ç§¯åˆ†\n\n### å®šç§¯åˆ†è®¡ç®—\n```python\nfrom scipy import integrate\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(\"=== æ•°å€¼ç§¯åˆ†è®¡ç®— ===\")\n\n# 1. å•å˜é‡ç§¯åˆ†\ndef func1(x):\n    return np.exp(-x**2) * np.sin(x)\n\nintegral1, error1 = integrate.quad(func1, 0, np.inf)\n\nprint(f\"ç§¯åˆ†ç»“æœ: {integral1:.6f}\")\nprint(f\"ä¼°è®¡è¯¯å·®: {error1:.2e}\")\nprint(f\"æœ‰æ•ˆä½æ•°: {-np.log10(error1/abs(integral1)):.1f}\")\n\n# å¯è§†åŒ–è¢«ç§¯å‡½æ•°\nx_plot = np.linspace(0, 3, 100)\ny_plot = func1(x_plot)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x_plot, y_plot, 'b-', linewidth=2, label='è¢«ç§¯å‡½æ•°')\nplt.fill_between(x_plot, y_plot, alpha=0.3, label=f'ç§¯åˆ†é¢ç§¯ â‰ˆ {integral1:.4f}')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.title(f'å®šç§¯åˆ†: âˆ«â‚€^âˆ e^(-xÂ²)sin(x)dx = {integral1:.6f}')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n\n### å¤šé‡ç§¯åˆ†\n```python\nfrom scipy import integrate\nimport numpy as np\n\nprint(\"=== å¤šé‡ç§¯åˆ†è®¡ç®— ===\")\n\n# äºŒé‡ç§¯åˆ†\ndef integrand(y, x):\n    return np.sin(x) * np.cos(y)\n\n# ç§¯åˆ†åŒºåŸŸ: xä»0åˆ°Ï€, yä»0åˆ°Ï€/2\nresult, error = integrate.dblquad(integrand, 0, np.pi, \n                                 lambda x: 0, \n                                 lambda x: np.pi/2)\n\nprint(f\"äºŒé‡ç§¯åˆ†ç»“æœ: {result:.6f}\")\nprint(f\"ä¼°è®¡è¯¯å·®: {error:.2e}\")\n\n# ä¸‰é‡ç§¯åˆ†\ndef integrand3(z, y, x):\n    return x * y * z\n\nresult3, error3 = integrate.tplquad(integrand3, \n                                   0, 1,                    # x bounds\n                                   lambda x: 0, \n                                   lambda x: 1 - x,        # y bounds\n                                   lambda x, y: 0, \n                                   lambda x, y: 1 - x - y) # z bounds\n\nprint(f\"\\nä¸‰é‡ç§¯åˆ†ç»“æœ: {result3:.6f}\")\nprint(f\"ä¼°è®¡è¯¯å·®: {error3:.2e}\")\nprint(f\"ç†è®ºå€¼: 1/120 = {1/120:.6f}\")\n```\n\n### å¾®åˆ†æ–¹ç¨‹æ±‚è§£\n```python\nfrom scipy import integrate\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(\"=== å¾®åˆ†æ–¹ç¨‹æ•°å€¼æ±‚è§£ ===\")\n\n# Lotka-Volterra æ•é£Ÿè€…-è¢«æ•é£Ÿè€…æ¨¡å‹\ndef ode_system(t, y):\n    alpha, beta, delta, gamma = 1.0, 0.1, 0.075, 1.5\n    prey, predator = y\n    dprey_dt = alpha * prey - beta * prey * predator\n    dpredator_dt = delta * prey * predator - gamma * predator\n    return [dprey_dt, dpredator_dt]\n\n# æ±‚è§£å¾®åˆ†æ–¹ç¨‹\nt_span = (0, 50)\ny0 = [10, 5]  # åˆå§‹ç§ç¾¤\nt_eval = np.linspace(0, 50, 1000)\nsolution = integrate.solve_ivp(ode_system, t_span, y0, t_eval=t_eval, method='RK45')\n\nprint(f\"æ±‚è§£æˆåŠŸ: {solution.success}\")\nprint(f\"è®¡ç®—æ­¥æ•°: {len(solution.t)}\")\nprint(f\"æœ€ç»ˆè¢«æ•é£Ÿè€…æ•°é‡: {solution.y[0, -1]:.2f}\")\nprint(f\"æœ€ç»ˆæ•é£Ÿè€…æ•°é‡: {solution.y[1, -1]:.2f}\")\n\n# å¯è§†åŒ–ç§ç¾¤åŠ¨æ€\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# æ—¶åŸŸå›¾\naxes[0].plot(solution.t, solution.y[0], 'g-', label='è¢«æ•é£Ÿè€…', linewidth=2)\naxes[0].plot(solution.t, solution.y[1], 'r-', label='æ•é£Ÿè€…', linewidth=2)\naxes[0].set_xlabel('æ—¶é—´')\naxes[0].set_ylabel('ç§ç¾¤æ•°é‡')\naxes[0].set_title('Lotka-Volterra æ¨¡å‹ç§ç¾¤åŠ¨æ€')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# ç›¸å›¾\naxes[1].plot(solution.y[0], solution.y[1], 'b-', linewidth=1)\naxes[1].scatter(solution.y[0, 0], solution.y[1, 0], color='green', s=100, label='èµ·ç‚¹', zorder=5)\naxes[1].scatter(solution.y[0, -1], solution.y[1, -1], color='red', s=100, label='ç»ˆç‚¹', zorder=5)\naxes[1].set_xlabel('è¢«æ•é£Ÿè€…æ•°é‡')\naxes[1].set_ylabel('æ•é£Ÿè€…æ•°é‡')\naxes[1].set_title('ç›¸å›¾')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n\n## ğŸ“¡ ä¿¡å·å¤„ç†\n\n### ä¿¡å·æ»¤æ³¢ä¸é¢‘è°±åˆ†æ\n```python\nfrom scipy import signal\nfrom scipy.fft import fft, fftfreq\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(\"=== ä¿¡å·å¤„ç†ä¸é¢‘è°±åˆ†æ ===\")\n\n# ç”Ÿæˆæµ‹è¯•ä¿¡å·\nt = np.linspace(0, 1, 1000, endpoint=False)\noriginal_signal = (np.sin(2 * np.pi * 5 * t) + \n                  0.5 * np.sin(2 * np.pi * 20 * t) + \n                  0.2 * np.sin(2 * np.pi * 50 * t))\n\n# æ·»åŠ å™ªå£°\nnp.random.seed(42)\nnoisy_signal = original_signal + 0.3 * np.random.normal(size=len(t))\n\nprint(f\"ä¿¡å·é•¿åº¦: {len(t)}\")\nprint(f\"é‡‡æ ·é¢‘ç‡: {1/(t[1]-t[0]):.0f} Hz\")\nprint(f\"å¥ˆå¥æ–¯ç‰¹é¢‘ç‡: {0.5/(t[1]-t[0]):.0f} Hz\")\n\n# è®¾è®¡ä½é€šæ»¤æ³¢å™¨\nnyquist = 0.5/(t[1]-t[0])  # å¥ˆå¥æ–¯ç‰¹é¢‘ç‡\ncutoff = 15 / nyquist\nb, a = signal.butter(4, cutoff, btype='low')\nfiltered_signal = signal.filtfilt(b, a, noisy_signal)\n\nprint(f\"æ»¤æ³¢å™¨é˜¶æ•°: 4\")\nprint(f\"æˆªæ­¢é¢‘ç‡: 15 Hz\")\n\n# å¯è§†åŒ–ä¿¡å·\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# æ—¶åŸŸä¿¡å·\naxes[0, 0].plot(t[:100], original_signal[:100], 'b-', alpha=0.7, label='åŸå§‹ä¿¡å·')\naxes[0, 0].plot(t[:100], noisy_signal[:100], 'r-', alpha=0.5, label='å¸¦å™ªå£°ä¿¡å·')\naxes[0, 0].plot(t[:100], filtered_signal[:100], 'g-', linewidth=2, label='æ»¤æ³¢åä¿¡å·')\naxes[0, 0].set_xlabel('æ—¶é—´ (s)')\naxes[0, 0].set_ylabel('å¹…åº¦')\naxes[0, 0].set_title('æ—¶åŸŸä¿¡å·ï¼ˆå‰100ç‚¹ï¼‰')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# é¢‘åŸŸåˆ†æ\nfft_original = fft(original_signal)\nfft_noisy = fft(noisy_signal)\nfft_filtered = fft(filtered_signal)\nfreqs = fftfreq(len(t), t[1] - t[0])\npositive_freq_idx = np.where((freqs > 0) & (freqs < 100))\n\naxes[0, 1].plot(freqs[positive_freq_idx], np.abs(fft_original[positive_freq_idx]), 'b-', label='åŸå§‹é¢‘è°±')\naxes[0, 1].plot(freqs[positive_freq_idx], np.abs(fft_noisy[positive_freq_idx]), 'r-', alpha=0.5, label='å™ªå£°é¢‘è°±')\naxes[0, 1].plot(freqs[positive_freq_idx], np.abs(fft_filtered[positive_freq_idx]), 'g-', label='æ»¤æ³¢é¢‘è°±')\naxes[0, 1].set_xlabel('é¢‘ç‡ (Hz)')\naxes[0, 1].set_ylabel('å¹…åº¦')\naxes[0, 1].set_title('é¢‘åŸŸåˆ†æ')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\naxes[0, 1].axvline(15, color='gray', linestyle='--', label='æˆªæ­¢é¢‘ç‡')\n\n# æ»¤æ³¢å™¨é¢‘ç‡å“åº”\nw, h = signal.freqz(b, a, worN=2000)\naxes[1, 0].plot(0.5 * w / np.pi * 500, 20 * np.log10(np.abs(h)), 'b-')\naxes[1, 0].set_xlabel('é¢‘ç‡ (Hz)')\naxes[1, 0].set_ylabel('å¢ç›Š (dB)')\naxes[1, 0].set_title('æ»¤æ³¢å™¨é¢‘ç‡å“åº”')\naxes[1, 0].grid(True, alpha=0.3)\naxes[1, 0].axvline(15, color='gray', linestyle='--', label='æˆªæ­¢é¢‘ç‡')\n\n# è¯¯å·®åˆ†æ\naxes[1, 1].plot(t[:100], filtered_signal[:100] - original_signal[:100], 'purple')\naxes[1, 1].set_xlabel('æ—¶é—´ (s)')\naxes[1, 1].set_ylabel('è¯¯å·®')\naxes[1, 1].set_title('æ»¤æ³¢è¯¯å·®ï¼ˆå‰100ç‚¹ï¼‰')\naxes[1, 1].grid(True, alpha=0.3)\naxes[1, 1].axhline(0, color='black', linestyle='-', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n\n## ğŸ§® çº¿æ€§ä»£æ•°\n\n### çŸ©é˜µè¿ç®—ä¸åˆ†è§£\n```python\nfrom scipy import linalg\nimport numpy as np\n\nprint(\"=== çº¿æ€§ä»£æ•°è¿ç®— ===\")\n\n# çŸ©é˜µè¿ç®—ç¤ºä¾‹\nA = np.array([[4, 2, 1], \n              [2, 5, 3], \n              [1, 3, 6]], dtype=float)\nb = np.array([1, 2, 3], dtype=float)\n\nprint(\"çŸ©é˜µ A:\")\nprint(A)\nprint(f\"\\nå‘é‡ b: {b}\")\n\n# çŸ©é˜µæ€§è´¨\ndet_A = linalg.det(A)\ncond_A = linalg.cond(A)\nprint(f\"\\nè¡Œåˆ—å¼: {det_A:.4f}\")\nprint(f\"æ¡ä»¶æ•°: {cond_A:.4f}\")\nprint(f\"çŸ©é˜µæ˜¯å¦å¯¹ç§°: {np.allclose(A, A.T)}\")\nprint(f\"çŸ©é˜µæ˜¯å¦æ­£å®š: {np.all(linalg.eigvals(A) > 0)}\")\n\n# çº¿æ€§æ–¹ç¨‹ç»„æ±‚è§£\nx = linalg.solve(A, b)\nprint(f\"\\næ–¹ç¨‹è§£: {x}\")\n\n# éªŒè¯è§£\nprint(f\"éªŒè¯ A*x: {A.dot(x)}\")\nprint(f\"ç›®æ ‡ b: {b}\")\nprint(f\"æ®‹å·®èŒƒæ•°: {np.linalg.norm(A.dot(x) - b):.2e}\")\n\n# ç‰¹å¾å€¼åˆ†è§£\neigenvalues, eigenvectors = linalg.eig(A)\nprint(f\"\\nç‰¹å¾å€¼: {eigenvalues}\")\nprint(\"ç‰¹å¾å‘é‡çŸ©é˜µ:\")\nprint(eigenvectors)\n\n# å¥‡å¼‚å€¼åˆ†è§£\nU, s, Vh = linalg.svd(A)\nprint(f\"\\nå¥‡å¼‚å€¼: {s}\")\nprint(f\"å¥‡å¼‚å€¼æ¡ä»¶æ•°: {s.max()/s.min():.4f}\")\n```\n\n### ç¨€ç–çŸ©é˜µå¤„ç†\n```python\nfrom scipy import sparse\nfrom scipy.sparse import linalg as splinalg\nimport numpy as np\n\nprint(\"=== ç¨€ç–çŸ©é˜µå¤„ç† ===\")\n\n# åˆ›å»ºç¨€ç–çŸ©é˜µ\nn = 100\ndiag = np.ones(n)\noffsets = [0, 1, -1]\ndata = [2*diag, -1*diag, -1*diag]\nA_sparse = sparse.diags(data, offsets, format='csr')\n\nprint(f\"ç¨€ç–çŸ©é˜µå½¢çŠ¶: {A_sparse.shape}\")\nprint(f\"éé›¶å…ƒç´ æ•°é‡: {A_sparse.nnz}\")\nprint(f\"ç¨€ç–åº¦: {100 * A_sparse.nnz / (n*n):.2f}%\")\n\n# åˆ›å»ºç¨ å¯†å‘é‡è¿›è¡Œæ¯”è¾ƒ\nb_dense = np.random.randn(n)\n\n# ç¨€ç–æ±‚è§£\nprint(\"\\nä½¿ç”¨ç¨€ç–æ±‚è§£å™¨:\")\nx_sparse = splinalg.spsolve(A_sparse, b_dense)\nprint(f\"æ±‚è§£å®Œæˆï¼Œè§£çš„å½¢çŠ¶: {x_sparse.shape}\")\n\n# ä¸ç¨ å¯†æ±‚è§£æ¯”è¾ƒ\nA_dense = A_sparse.toarray()\nprint(\"\\nä¸ç¨ å¯†æ±‚è§£å™¨æ¯”è¾ƒ:\")\nx_dense = linalg.solve(A_dense, b_dense)\nresidual = np.linalg.norm(A_dense @ x_sparse - b_dense)\nprint(f\"æ®‹å·®èŒƒæ•°: {residual:.2e}\")\nprint(f\"ä¸ç¨ å¯†è§£çš„æœ€å¤§å·®å¼‚: {np.max(np.abs(x_sparse - x_dense)):.2e}\")\n```\n\n## ğŸ“Š ç»Ÿè®¡è®¡ç®—\n\n### æ¦‚ç‡åˆ†å¸ƒä¸ç»Ÿè®¡æ£€éªŒ\n```python\nfrom scipy import stats\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(\"=== ç»Ÿè®¡è®¡ç®—ä¸æ¦‚ç‡åˆ†å¸ƒ ===\")\n\n# ç”Ÿæˆæ­£æ€åˆ†å¸ƒæ ·æœ¬\nnp.random.seed(42)\nnormal_samples = np.random.normal(loc=0, scale=1, size=1000)\n\nprint(f\"æ ·æœ¬æ•°é‡: {len(normal_samples)}\")\nprint(f\"æ ·æœ¬å‡å€¼: {np.mean(normal_samples):.4f}\")\nprint(f\"æ ·æœ¬æ ‡å‡†å·®: {np.std(normal_samples):.4f}\")\n\n# æ­£æ€æ€§æ£€éªŒ\nk2_statistic, p_value = stats.normaltest(normal_samples)\nprint(f\"\\næ­£æ€æ€§æ£€éªŒ:\")\nprint(f\"ç»Ÿè®¡é‡: {k2_statistic:.4f}\")\nprint(f\"på€¼: {p_value:.4f}\")\nprint(f\"æ˜¯å¦æ­£æ€åˆ†å¸ƒ (Î±=0.05): {p_value > 0.05}\")\n\n# æ‹Ÿåˆåˆ†å¸ƒå‚æ•°\nparams = stats.norm.fit(normal_samples)\nprint(f\"\\næ‹Ÿåˆæ­£æ€åˆ†å¸ƒå‚æ•°:\")\nprint(f\"å‡å€¼: {params[0]:.4f}\")\nprint(f\"æ ‡å‡†å·®: {params[1]:.4f}\")\n\n# å¯è§†åŒ–\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# ç›´æ–¹å›¾ä¸ç†è®ºPDF\naxes[0].hist(normal_samples, bins=30, density=True, alpha=0.7, label='æ ·æœ¬ç›´æ–¹å›¾')\nx = np.linspace(-4, 4, 100)\naxes[0].plot(x, stats.norm.pdf(x), 'r-', linewidth=2, label='ç†è®ºPDF')\naxes[0].set_xlabel('å€¼')\naxes[0].set_ylabel('æ¦‚ç‡å¯†åº¦')\naxes[0].set_title('æ­£æ€åˆ†å¸ƒæ ·æœ¬ä¸ç†è®ºPDF')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# QQå›¾\nstats.probplot(normal_samples, dist=\"norm\", plot=axes[1])\naxes[1].set_title('æ­£æ€QQå›¾')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n\n### å‡è®¾æ£€éªŒ\n```python\nfrom scipy import stats\nimport numpy as np\n\nprint(\"=== å‡è®¾æ£€éªŒ ===\")\n\n# ç”Ÿæˆä¸¤ç»„æ ·æœ¬\nnp.random.seed(42)\ngroup1 = np.random.normal(loc=10, scale=2, size=50)\ngroup2 = np.random.normal(loc=12, scale=2, size=50)\n\nprint(f\"ç¬¬ä¸€ç»„: å‡å€¼={np.mean(group1):.2f}, æ ‡å‡†å·®={np.std(group1):.2f}, n={len(group1)}\")\nprint(f\"ç¬¬äºŒç»„: å‡å€¼={np.mean(group2):.2f}, æ ‡å‡†å·®={np.std(group2):.2f}, n={len(group2)}\")\n\n# tæ£€éªŒï¼ˆç‹¬ç«‹æ ·æœ¬ï¼‰\nt_statistic, p_value = stats.ttest_ind(group1, group2)\nprint(f\"\\nç‹¬ç«‹æ ·æœ¬tæ£€éªŒ:\")\nprint(f\"tç»Ÿè®¡é‡: {t_statistic:.4f}\")\nprint(f\"på€¼: {p_value:.4f}\")\nprint(f\"æ˜¯å¦æ˜¾è‘—ä¸åŒ (Î±=0.05): {p_value < 0.05}\")\n\n# æ–¹å·®é½æ€§æ£€éªŒ\nf_statistic, p_value_var = stats.levene(group1, group2)\nprint(f\"\\næ–¹å·®é½æ€§æ£€éªŒ:\")\nprint(f\"Fç»Ÿè®¡é‡: {f_statistic:.4f}\")\nprint(f\"på€¼: {p_value_var:.4f}\")\nprint(f\"æ–¹å·®æ˜¯å¦é½ (Î±=0.05): {p_value_var > 0.05}\")\n\n# ç›¸å…³æ€§æ£€éªŒ\ncorrelation, p_value_corr = stats.pearsonr(group1, np.random.permutation(group2))\nprint(f\"\\nç›¸å…³æ€§æ£€éªŒ:\")\nprint(f\"ç›¸å…³ç³»æ•°: {correlation:.4f}\")\nprint(f\"på€¼: {p_value_corr:.4f}\")\nprint(f\"æ˜¯å¦æ˜¾è‘—ç›¸å…³ (Î±=0.05): {p_value_corr < 0.05}\")\n```\n\n## ğŸ§­ ç©ºé—´ç®—æ³•\n\n### ç©ºé—´æ•°æ®ç»“æ„\n```python\nfrom scipy import spatial\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(\"=== ç©ºé—´ç®—æ³•ä¸æ•°æ®ç»“æ„ ===\")\n\n# åˆ›å»ºéšæœºç‚¹é›†\nnp.random.seed(42)\npoints = np.random.rand(30, 2) * 10\n\nprint(f\"ç‚¹é›†å¤§å°: {points.shape}\")\nprint(f\"åæ ‡èŒƒå›´: X[{points[:,0].min():.2f}, {points[:,0].max():.2f}], \"\n      f\"Y[{points[:,1].min():.2f}, {points[:,1].max():.2f}]\")\n\n# è®¡ç®—å‡¸åŒ…\nhull = spatial.ConvexHull(points)\nprint(f\"\\nå‡¸åŒ…è®¡ç®—:\")\nprint(f\"å‡¸åŒ…é¡¶ç‚¹æ•°é‡: {len(hull.vertices)}\")\nprint(f\"å‡¸åŒ…é¢ç§¯: {hull.area:.2f}\")\nprint(f\"å‡¸åŒ…ä½“ç§¯: {hull.volume:.2f}\")\n\n# æœ€è¿‘é‚»æœç´¢\ntree = spatial.KDTree(points)\ndistances, indices = tree.query(points, k=3)  # æ¯ä¸ªç‚¹æ‰¾3ä¸ªæœ€è¿‘é‚»\nprint(f\"\\næœ€è¿‘é‚»æœç´¢:\")\nprint(f\"å¹³å‡æœ€è¿‘è·ç¦»: {distances[:,1].mean():.2f}\")\nprint(f\"æœ€è¿œæœ€è¿‘è·ç¦»: {distances[:,1].max():.2f}\")\n\n# å¯è§†åŒ–\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# ç‚¹é›†ä¸å‡¸åŒ…\naxes[0].scatter(points[:,0], points[:,1], c='blue', s=50, label='æ•°æ®ç‚¹')\nfor simplex in hull.simplices:\n    axes[0].plot(points[simplex, 0], points[simplex, 1], 'r-', linewidth=2)\naxes[0].set_title('ç©ºé—´ç‚¹é›†ä¸å‡¸åŒ…')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\naxes[0].axis('equal')\n\n# æœ€è¿‘é‚»è¿æ¥\naxes[1].scatter(points[:,0], points[:,1], c='blue', s=50, label='æ•°æ®ç‚¹')\nfor i in range(len(points)):\n    for j in range(1, 3):  # è¿æ¥ç¬¬1å’Œç¬¬2è¿‘é‚»\n        neighbor_idx = indices[i, j]\n        axes[1].plot([points[i,0], points[neighbor_idx,0]], \n                    [points[i,1], points[neighbor_idx,1]], \n                    'gray', alpha=0.3, linewidth=0.5)\naxes[1].set_title('æœ€è¿‘é‚»è¿æ¥å›¾')\naxes[1].grid(True, alpha=0.3)\naxes[1].axis('equal')\n\nplt.tight_layout()\nplt.show()\n```\n\n## âš ï¸ ä½¿ç”¨æ³¨æ„äº‹é¡¹\n\n### âœ… æ¨èåšæ³•ï¼š\n1. **æ¨¡å—å¯¼å…¥**ï¼šæŒ‰éœ€å¯¼å…¥å­æ¨¡å— `from scipy import optimize, integrate, stats`\n2. **æ•°å€¼ç¨³å®šæ€§**ï¼šæ³¨æ„çŸ©é˜µæ¡ä»¶æ•°ï¼Œä½¿ç”¨æ¡ä»¶è‰¯å¥½çš„é—®é¢˜\n3. **å†…å­˜ç®¡ç†**ï¼šå¤§æ•°æ®ä½¿ç”¨ç¨€ç–çŸ©é˜µæˆ–åˆ†å—å¤„ç†\n4. **ç»“æœéªŒè¯**ï¼šæ£€æŸ¥æ±‚è§£å™¨çš„ `success` æ ‡å¿—å’Œæ®‹å·®\n\n### âŒ é¿å…çš„æ“ä½œï¼š\n1. ä¸è¦é‡å¤è®¡ç®—å¯ç¼“å­˜çš„ç»“æœ\n2. ä¸è¦ä½¿ç”¨é»˜è®¤å‚æ•°å¤„ç†ç—…æ€é—®é¢˜\n3. ä¸è¦å¿½ç•¥æ±‚è§£å™¨çš„æ”¶æ•›çŠ¶æ€\n4. ä¸è¦åœ¨å¾ªç¯ä¸­é‡å¤åˆ›å»ºå¤§å‹æ•°ç»„\n\n### âš ï¸ å†…å­˜é™åˆ¶æé†’ï¼š\nåœ¨æ‰§è¡Œå¤§å‹è®¡ç®—å‰ï¼Œè¯·æ·»åŠ å†…å­˜ä½¿ç”¨æé†’ï¼š\n```python\n# åœ¨å¤§å‹è®¡ç®—å‰æ·»åŠ æé†’\nprint(\"æ³¨æ„ï¼šä»¥ä¸‹è®¡ç®—å¯èƒ½éœ€è¦è¾ƒå¤§å†…å­˜ï¼Œå¦‚æœ‰é—®é¢˜è¯·åˆ†å—å¤„ç†\")\n```\n\n### ğŸ“Š æ€§èƒ½ç›‘æ§ï¼š\næ·»åŠ æ€§èƒ½ç›‘æ§ä»£ç å¯ä»¥å¸®åŠ©äº†è§£è®¡ç®—èµ„æºæ¶ˆè€—ï¼š\n```python\nimport time\nimport psutil\n\nstart_time = time.time()\nprocess = psutil.Process()\ninitial_memory = process.memory_info().rss / 1024**2\n\n# ... æ‰§è¡Œè®¡ç®— ...\n\nend_time = time.time()\nfinal_memory = process.memory_info().rss / 1024**2\n\nprint(f\"è®¡ç®—æ—¶é—´: {end_time - start_time:.2f}ç§’\")\nprint(f\"å†…å­˜ä½¿ç”¨: {final_memory - initial_memory:.2f} MB\")\n```\n\n### ğŸ”§ é”™è¯¯å¤„ç†ï¼š\n```python\n# åœ¨å…³é”®è®¡ç®—å‘¨å›´æ·»åŠ try-except\ntry:\n    result = optimize.minimize_scalar(func, bounds=(0, 10))\n    if result.success:\n        print(f\"ä¼˜åŒ–æˆåŠŸ: x={result.x:.4f}\")\n    else:\n        print(f\"ä¼˜åŒ–å¤±è´¥: {result.message}\")\nexcept Exception as e:\n    print(f\"è®¡ç®—é”™è¯¯: {e}\")\n    # æä¾›æ›¿ä»£æ–¹æ¡ˆ\n    print(\"å°è¯•ä½¿ç”¨ä¸åŒçš„åˆå§‹å€¼æˆ–æ–¹æ³•...\")\n```\n\n### ğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®ï¼š\n```python\n# 1. ä½¿ç”¨å‘é‡åŒ–æ“ä½œæ›¿ä»£å¾ªç¯\n# 2. å¯¹äºå¤§å‹çº¿æ€§ç³»ç»Ÿï¼Œä½¿ç”¨ç¨€ç–çŸ©é˜µ\n# 3. é‡å¤è®¡ç®—æ—¶ç¼“å­˜ä¸­é—´ç»“æœ\n# 4. ä½¿ç”¨é€‚å½“ç²¾åº¦ï¼Œé¿å…ä¸å¿…è¦çš„é«˜ç²¾åº¦è®¡ç®—\n# 5. å¤§å‹è®¡ç®—å‰æ·»åŠ å†…å­˜ä½¿ç”¨æé†’\n# 6. ç›‘æ§è®¡ç®—æ—¶é—´å’Œå†…å­˜æ¶ˆè€—\n# 7. ä¸ºå…³é”®è®¡ç®—æ·»åŠ é”™è¯¯å¤„ç†æœºåˆ¶\n```\n\n## ğŸ“‹ å¿«é€Ÿå‚è€ƒå¡\n\n```python\n# ä¼˜åŒ–\nfrom scipy import optimize\nresult = optimize.minimize(f, x0, method='BFGS')\n\n# ç§¯åˆ†\nfrom scipy import integrate\nresult, error = integrate.quad(f, a, b)\n\n# ä¿¡å·å¤„ç†\nfrom scipy import signal\nfiltered = signal.filtfilt(b, a, signal)\n\n# çº¿æ€§ä»£æ•°\nfrom scipy import linalg\nx = linalg.solve(A, b)\n\n# ç»Ÿè®¡\nfrom scipy import stats\nt, p = stats.ttest_ind(group1, group2)\n\n# ç©ºé—´ç®—æ³•\nfrom scipy import spatial\nhull = spatial.ConvexHull(points)\n```\n\n## ğŸš€ é«˜çº§åº”ç”¨ç¤ºä¾‹\n\n### å…¨å±€ä¼˜åŒ–\n```python\nimport numpy as np\nfrom scipy import optimize\nimport matplotlib.pyplot as plt\n\nprint(\"=== å…¨å±€ä¼˜åŒ–é—®é¢˜ ===\")\n\n# å¤šå³°å‡½æ•°\ndef multimodal_func(x):\n    return np.sin(5*x) + 0.5*x**2 + 0.1*np.random.randn() if len(x.shape)==0 else 0.1*np.random.randn(x.shape[0])\n\n# ä½¿ç”¨ basinhopping è¿›è¡Œå…¨å±€ä¼˜åŒ–\nresult = optimize.basinhopping(multimodal_func, x0=0, niter=100, \n                              stepsize=1.0, minimizer_kwargs={\"method\": \"BFGS\"})\n\nprint(f\"å…¨å±€ä¼˜åŒ–ç»“æœ:\")\nprint(f\"æœ€ä¼˜è§£: x = {result.x[0]:.4f}\")\nprint(f\"æœ€ä¼˜å€¼: {result.fun:.4f}\")\nprint(f\"å‘ç°å±€éƒ¨æå€¼æ•°é‡: {result.nit}\")\n\n# å¯è§†åŒ–\nx_plot = np.linspace(-5, 5, 1000)\ny_plot = np.sin(5*x_plot) + 0.5*x_plot**2\n\nplt.figure(figsize=(12, 6))\nplt.plot(x_plot, y_plot, 'b-', linewidth=2, label='ç›®æ ‡å‡½æ•°')\nplt.axvline(result.x, color='red', linestyle='--', linewidth=2, label='å…¨å±€æœ€ä¼˜è§£')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.title('å¤šå³°å‡½æ•°å…¨å±€ä¼˜åŒ–')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n\n### åå¾®åˆ†æ–¹ç¨‹æ±‚è§£\n```python\nimport numpy as np\nfrom scipy import integrate\nimport matplotlib.pyplot as plt\n\nprint(\"=== åå¾®åˆ†æ–¹ç¨‹æ•°å€¼æ±‚è§£ ===\")\n\n# çƒ­ä¼ å¯¼æ–¹ç¨‹ï¼ˆç®€åŒ–ç¤ºä¾‹ï¼‰\n# ä½¿ç”¨æœ‰é™å·®åˆ†æ³•\nL = 1.0  # æ†é•¿\nN = 50   # ç©ºé—´ç½‘æ ¼æ•°\nT = 0.5  # æ€»æ—¶é—´\ndt = 0.001  # æ—¶é—´æ­¥é•¿\n\n# ç©ºé—´ç½‘æ ¼\nx = np.linspace(0, L, N+1)\ndx = x[1] - x[0]\n\n# åˆå§‹æ¡ä»¶ï¼ˆä¸­å¿ƒåŠ çƒ­ï¼‰\nu = np.exp(-100*(x - L/2)**2)\n\n# æ—¶é—´æ­¥è¿›\nfor n in range(int(T/dt)):\n    # ä½¿ç”¨æ˜¾å¼æ¬§æ‹‰æ³•\n    u[1:-1] = u[1:-1] + dt/dx**2 * (u[:-2] - 2*u[1:-1] + u[2:])\n\nprint(f\"çƒ­ä¼ å¯¼æ–¹ç¨‹æ•°å€¼æ±‚è§£å®Œæˆ\")\nprint(f\"ç©ºé—´ç½‘æ ¼: {N+1} ç‚¹\")\nprint(f\"æ—¶é—´æ­¥æ•°: {int(T/dt)}\")\nprint(f\"æœ€ç»ˆæ¸©åº¦åˆ†å¸ƒèŒƒå›´: [{u.min():.4f}, {u.max():.4f}]\")\n\n# å¯è§†åŒ–\nplt.figure(figsize=(10, 6))\nplt.plot(x, np.exp(-100*(x - L/2)**2), 'b--', linewidth=2, label='åˆå§‹æ¸©åº¦åˆ†å¸ƒ')\nplt.plot(x, u, 'r-', linewidth=2, label=f'æœ€ç»ˆæ¸©åº¦åˆ†å¸ƒ (t={T})')\nplt.xlabel('ä½ç½® x')\nplt.ylabel('æ¸©åº¦ u(x,t)')\nplt.title('çƒ­ä¼ å¯¼æ–¹ç¨‹æ•°å€¼è§£')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n\n---\n\n**è®°ä½**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰è¾“å‡ºæ ¼å¼ï¼Œæ‚¨åªéœ€è¦ä¸“æ³¨äºç§‘å­¦è®¡ç®—é€»è¾‘ï¼SciPy å‡½æ•°ä¼šä»¥é€‚å½“æ ¼å¼æ˜¾ç¤ºç»“æœï¼Œå¤æ‚è®¡ç®—ä¹Ÿä¼šè¢«æ­£ç¡®å¤„ç†ã€‚\n\n### ğŸ“– sympy_cookbook\n\n# SymPy ç¬¦å·æ•°å­¦æŒ‡å— (v2.5)\n\n## ğŸ¯ å·¥å…·æ¦‚è¿°\n**åŠŸèƒ½**ï¼šç¬¦å·æ•°å­¦è®¡ç®—ï¼ŒåŒ…æ‹¬æ–¹ç¨‹æ±‚è§£ã€å¾®ç§¯åˆ†ã€ä»£æ•°è¿ç®—ç­‰\n**è¾“å‡ºåŸåˆ™**ï¼šç›´æ¥æ‰“å°ç»“æœï¼Œç³»ç»Ÿè‡ªåŠ¨å¤„ç†è¾“å‡ºæ ¼å¼\n\n## âœ… ä»£ç è§£é‡Šå™¨é€‚é…è¯´æ˜\n- **ç›´æ¥æ‰“å°**ï¼šæ‰€æœ‰è®¡ç®—ç»“æœç›´æ¥ä½¿ç”¨ `print()` è¾“å‡º\n- **ç¬¦å·è¡¨è¾¾å¼**ï¼šSymPy è¡¨è¾¾å¼ä¼šä»¥ç¾è§‚çš„æ•°å­¦æ ¼å¼æ˜¾ç¤º\n- **è‡ªåŠ¨æ¸²æŸ“**ï¼šå¤æ‚æ•°å­¦å…¬å¼ä¼šè‡ªåŠ¨è½¬æ¢ä¸ºæ˜“è¯»æ ¼å¼\n- **æ•°å€¼è®¡ç®—**ï¼šéœ€è¦æ•°å€¼ç»“æœæ—¶ä½¿ç”¨ `.evalf()` æˆ– `sp.N()`\n\n## ğŸ§® åŸºç¡€ç¬¦å·è¿ç®—\n\n### ç¬¦å·å®šä¹‰ä¸åŸºæœ¬æ“ä½œ\n```python\nimport sympy as sp\n\n# å®šä¹‰ç¬¦å·å˜é‡\nx, y, z = sp.symbols('x y z')\na, b, c = sp.symbols('a b c')\n\n# åŸºæœ¬è¡¨è¾¾å¼æ“ä½œ\nexpr1 = x**2 + 2*x + 1\nexpr2 = (x + 1)**2\n\nprint(\"=== åŸºç¡€ç¬¦å·è¿ç®— ===\")\nprint(f\"è¡¨è¾¾å¼1: {expr1}\")\nprint(f\"è¡¨è¾¾å¼2: {expr2}\")\nprint(f\"è¡¨è¾¾å¼1å±•å¼€: {sp.expand(expr1)}\")\nprint(f\"è¡¨è¾¾å¼2å› å¼åˆ†è§£: {sp.factor(expr2)}\")\nprint(f\"ä¸¤ä¸ªè¡¨è¾¾å¼æ˜¯å¦ç›¸ç­‰: {expr1.equals(expr2)}\")\n\n# è¡¨è¾¾å¼ç®€åŒ–\ncomplex_expr = (x**2 - 1)/(x - 1)\nsimplified = sp.simplify(complex_expr)\nprint(f\"å¤æ‚è¡¨è¾¾å¼: {complex_expr}\")\nprint(f\"ç®€åŒ–å: {simplified}\")\n```\n\n## ğŸ¯ æ–¹ç¨‹æ±‚è§£\n\n### ä»£æ•°æ–¹ç¨‹æ±‚è§£\n```python\nimport sympy as sp\n\nx, y, z = sp.symbols('x y z')\n\nprint(\"=== ä»£æ•°æ–¹ç¨‹æ±‚è§£ ===\")\n\n# ä¸€å…ƒäºŒæ¬¡æ–¹ç¨‹\neq1 = sp.Eq(x**2 - 5*x + 6, 0)\nsolutions1 = sp.solve(eq1, x)\nprint(f\"æ–¹ç¨‹: {eq1}\")\nprint(f\"è§£: {solutions1}\")\n\n# çº¿æ€§æ–¹ç¨‹ç»„\neq2 = sp.Eq(2*x + 3*y, 7)\neq3 = sp.Eq(4*x - y, 1)\nsolutions2 = sp.solve([eq2, eq3], (x, y))\nprint(f\"\\næ–¹ç¨‹ç»„:\")\nprint(f\"  {eq2}\")\nprint(f\"  {eq3}\")\nprint(f\"è§£: {solutions2}\")\n\n# éçº¿æ€§æ–¹ç¨‹æ•°å€¼è§£\neq4 = sp.Eq(sp.sin(x) - x/2, 0)\nsolution4 = sp.nsolve(eq4, x, 1)  # ä»x=1å¼€å§‹æ•°å€¼æ±‚è§£\nprint(f\"\\néçº¿æ€§æ–¹ç¨‹: {eq4}\")\nprint(f\"æ•°å€¼è§£: {solution4}\")\n```\n\n## ğŸ“ å¾®ç§¯åˆ†è¿ç®—\n\n### å¾®åˆ†è®¡ç®—\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\n\nprint(\"=== å¾®åˆ†è®¡ç®— ===\")\n\n# å®šä¹‰å‡½æ•°\nf = x**3 + 2*x**2 + sp.sin(x)\nprint(f\"å‡½æ•°: f(x) = {f}\")\n\n# ä¸€é˜¶å¯¼æ•°\nf_prime = sp.diff(f, x)\nprint(f\"ä¸€é˜¶å¯¼æ•°: f'(x) = {f_prime}\")\n\n# äºŒé˜¶å¯¼æ•°\nf_double_prime = sp.diff(f, x, 2)\nprint(f\"äºŒé˜¶å¯¼æ•°: f''(x) = {f_double_prime}\")\n\n# åå¯¼æ•°ï¼ˆå¤šå˜é‡ï¼‰\ny = sp.symbols('y')\ng = x**2 * y + sp.sin(x*y)\ng_x = sp.diff(g, x)\ng_y = sp.diff(g, y)\nprint(f\"\\nå¤šå˜é‡å‡½æ•°: g(x,y) = {g}\")\nprint(f\"å¯¹xåå¯¼: âˆ‚g/âˆ‚x = {g_x}\")\nprint(f\"å¯¹yåå¯¼: âˆ‚g/âˆ‚y = {g_y}\")\n```\n\n### ç§¯åˆ†è®¡ç®—\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\n\nprint(\"=== ç§¯åˆ†è®¡ç®— ===\")\n\n# ä¸å®šç§¯åˆ†\nf = x**2 + sp.sin(x)\nindefinite = sp.integrate(f, x)\nprint(f\"å‡½æ•°: f(x) = {f}\")\nprint(f\"ä¸å®šç§¯åˆ†: âˆ«f(x)dx = {indefinite} + C\")\n\n# å®šç§¯åˆ†\ndefinite = sp.integrate(f, (x, 0, sp.pi))\nprint(f\"å®šç§¯åˆ† [0,Ï€]: âˆ«â‚€^Ï€ f(x)dx = {definite}\")\nprint(f\"æ•°å€¼ç»“æœ: {definite.evalf()}\")\n\n# å¤šé‡ç§¯åˆ†\ny = sp.symbols('y')\ndouble_int = sp.integrate(x*y, (x, 0, 1), (y, 0, 2))\nprint(f\"\\näºŒé‡ç§¯åˆ†: âˆ«â‚€Â¹âˆ«â‚€Â² xy dy dx = {double_int}\")\n```\n\n### æé™è®¡ç®—\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\n\nprint(\"=== æé™è®¡ç®— ===\")\n\n# åŸºæœ¬æé™\nlimit1 = sp.limit(sp.sin(x)/x, x, 0)\nprint(f\"lim(xâ†’0) sin(x)/x = {limit1}\")\n\n# æ— ç©·æé™\nlimit2 = sp.limit(1/x, x, 0, '+')  # ä»æ­£æ–¹å‘é€¼è¿‘\nlimit3 = sp.limit(1/x, x, 0, '-')  # ä»è´Ÿæ–¹å‘é€¼è¿‘\nprint(f\"lim(xâ†’0âº) 1/x = {limit2}\")\nprint(f\"lim(xâ†’0â») 1/x = {limit3}\")\n\n# å¤æ‚æé™\nlimit4 = sp.limit((1 + 1/x)**x, x, sp.oo)\nprint(f\"lim(xâ†’âˆ) (1 + 1/x)Ë£ = {limit4}\")\n```\n\n## ğŸ” æ•°å­¦è¯æ˜ä¸æ’ç­‰å¼\n\n### ä»£æ•°æ’ç­‰å¼éªŒè¯\n```python\nimport sympy as sp\n\na, b, x = sp.symbols('a b x')\n\nprint(\"=== æ•°å­¦æ’ç­‰å¼éªŒè¯ ===\")\n\n# éªŒè¯ (a+b)Â² = aÂ² + 2ab + bÂ²\nlhs1 = (a + b)**2\nrhs1 = a**2 + 2*a*b + b**2\nidentity1 = sp.simplify(lhs1 - rhs1) == 0\nprint(f\"(a+b)Â² = aÂ² + 2ab + bÂ²: {identity1}\")\n\n# éªŒè¯ä¸‰è§’æ’ç­‰å¼ sinÂ²x + cosÂ²x = 1\nlhs2 = sp.sin(x)**2 + sp.cos(x)**2\nrhs2 = 1\nidentity2 = sp.simplify(lhs2 - rhs2) == 0\nprint(f\"sinÂ²x + cosÂ²x = 1: {identity2}\")\n\n# éªŒè¯æ¬§æ‹‰å…¬å¼\ntheta = sp.symbols('theta')\neuler_lhs = sp.exp(sp.I * theta)\neuler_rhs = sp.cos(theta) + sp.I * sp.sin(theta)\neuler_identity = sp.simplify(euler_lhs - euler_rhs) == 0\nprint(f\"e^(iÎ¸) = cosÎ¸ + i sinÎ¸: {euler_identity}\")\n```\n\n## ğŸ§© çº¿æ€§ä»£æ•°\n\n### çŸ©é˜µè¿ç®—\n```python\nimport sympy as sp\n\nprint(\"=== çŸ©é˜µè¿ç®— ===\")\n\n# å®šä¹‰ç¬¦å·çŸ©é˜µ\nA = sp.Matrix([[1, 2], [3, 4]])\nB = sp.Matrix([[2, 0], [1, 2]])\n\nprint(f\"çŸ©é˜µ A:\\n{A}\")\nprint(f\"çŸ©é˜µ B:\\n{B}\")\n\n# åŸºæœ¬è¿ç®—\nprint(f\"\\nçŸ©é˜µåŠ æ³• A+B:\\n{A + B}\")\nprint(f\"çŸ©é˜µä¹˜æ³• AÃ—B:\\n{A * B}\")\nprint(f\"Açš„è¡Œåˆ—å¼: {A.det()}\")\nprint(f\"Açš„é€†çŸ©é˜µ:\\n{A.inv()}\")\n\n# ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡\neigenvals = A.eigenvals()\neigenvects = A.eigenvects()\nprint(f\"\\nAçš„ç‰¹å¾å€¼: {eigenvals}\")\nprint(f\"Açš„ç‰¹å¾å‘é‡: {eigenvects}\")\n\n# è§£çº¿æ€§æ–¹ç¨‹ç»„\nx1, x2 = sp.symbols('x1 x2')\neq1 = sp.Eq(2*x1 + 3*x2, 7)\neq2 = sp.Eq(4*x1 + 5*x2, 13)\nsolution = sp.solve([eq1, eq2], (x1, x2))\nprint(f\"\\næ–¹ç¨‹ç»„:\")\nprint(f\"  {eq1}\")\nprint(f\"  {eq2}\")\nprint(f\"è§£: {solution}\")\n```\n\n## ğŸ“ˆ çº§æ•°å±•å¼€ä¸æ•°å€¼è®¡ç®—\n\n### æ³°å‹’çº§æ•°å±•å¼€\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\n\nprint(\"=== çº§æ•°å±•å¼€ ===\")\n\n# å¸¸ç”¨å‡½æ•°çš„æ³°å‹’å±•å¼€\nsin_series = sp.sin(x).series(x, 0, 6)  # åœ¨0å¤„å±•å¼€åˆ°6é˜¶\ncos_series = sp.cos(x).series(x, 0, 6)\nexp_series = sp.exp(x).series(x, 0, 5)\n\nprint(f\"sin(x)çš„æ³°å‹’å±•å¼€: {sin_series}\")\nprint(f\"cos(x)çš„æ³°å‹’å±•å¼€: {cos_series}\")\nprint(f\"e^xçš„æ³°å‹’å±•å¼€: {exp_series}\")\n\n# æ•°å€¼è¿‘ä¼¼\nprint(f\"\\næ•°å€¼è¿‘ä¼¼:\")\nprint(f\"Ï€ â‰ˆ {sp.N(sp.pi, 10)}\")  # 10ä½ç²¾åº¦\nprint(f\"e â‰ˆ {sp.N(sp.E, 8)}\")    # 8ä½ç²¾åº¦\nprint(f\"âˆš2 â‰ˆ {sp.N(sp.sqrt(2), 6)}\")\n\n# ç¬¦å·è¡¨è¾¾å¼çš„æ•°å€¼è®¡ç®—\nexpr = sp.integrate(sp.sin(x), (x, 0, sp.pi/2))\nnumerical_result = sp.N(expr)\nprint(f\"\\nç¬¦å·ç§¯åˆ†: âˆ«â‚€^(Ï€/2) sin(x) dx = {expr}\")\nprint(f\"æ•°å€¼ç»“æœ: {numerical_result}\")\n```\n\n## ğŸ“ å¤æ‚æ•°å­¦é—®é¢˜\n\n### å‡½æ•°åˆ†æä¸æå€¼\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\n\nprint(\"=== å‡½æ•°åˆ†æä¸æå€¼ ===\")\n\n# å®šä¹‰å‡½æ•°\nf = x**3 - 6*x**2 + 9*x + 1\nprint(f\"å‡½æ•°: f(x) = {f}\")\n\n# æ±‚å¯¼æ‰¾ä¸´ç•Œç‚¹\nf_prime = sp.diff(f, x)\ncritical_points = sp.solve(f_prime, x)\nprint(f\"ä¸€é˜¶å¯¼æ•°: f'(x) = {f_prime}\")\nprint(f\"ä¸´ç•Œç‚¹: {critical_points}\")\n\n# äºŒé˜¶å¯¼æ•°æµ‹è¯•\nf_double_prime = sp.diff(f, x, 2)\nfor point in critical_points:\n    second_deriv_val = f_double_prime.subs(x, point)\n    if second_deriv_val > 0:\n        extremum_type = \"å±€éƒ¨æå°å€¼\"\n    elif second_deriv_val < 0:\n        extremum_type = \"å±€éƒ¨æå¤§å€¼\"\n    else:\n        extremum_type = \"éœ€è¦è¿›ä¸€æ­¥åˆ†æ\"\n    print(f\"ç‚¹ x = {point}: {extremum_type}\")\n\n# å‡½æ•°å€¼\nfor point in critical_points:\n    func_val = f.subs(x, point)\n    print(f\"f({point}) = {func_val}\")\n```\n\n### æ›²çº¿æ€§è´¨åˆ†æ\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\n\nprint(\"=== æ›²çº¿æ€§è´¨åˆ†æ ===\")\n\nf = x**2 * sp.sin(x)\n\n# æ›²çº¿é•¿åº¦ï¼ˆå¼§é•¿ï¼‰\ncurve_length = sp.integrate(sp.sqrt(1 + sp.diff(f, x)**2), (x, 0, sp.pi))\nprint(f\"å‡½æ•°: f(x) = {f}\")\nprint(f\"æ›²çº¿åœ¨ [0,Ï€] ä¸Šçš„é•¿åº¦: {sp.N(curve_length)}\")\n\n# æ—‹è½¬ä½“ä½“ç§¯\nvolume = sp.pi * sp.integrate(f**2, (x, 0, sp.pi))\nprint(f\"æ›²çº¿ç»•xè½´æ—‹è½¬çš„ä½“ç§¯: {sp.N(volume)}\")\n\n# æ›²ç‡\nf_prime = sp.diff(f, x)\nf_double_prime = sp.diff(f, x, 2)\ncurvature = f_double_prime / (1 + f_prime**2)**(3/2)\nprint(f\"æ›²ç‡å…¬å¼: Îº(x) = {curvature}\")\n```\n\n## ğŸ’¡ å®ç”¨å·¥å…·å‡½æ•°\n\n### è‡ªåŠ¨éªŒè¯ç­‰å¼\n```python\nimport sympy as sp\n\ndef verify_identity(expr1, expr2, method=\"simplify\"):\n    \"\"\"\n    éªŒè¯ä¸¤ä¸ªè¡¨è¾¾å¼æ˜¯å¦æ’ç­‰\n    method: \"simplify\", \"expand\", \"factor\", \"trigsimp\"\n    \"\"\"\n    if method == \"simplify\":\n        difference = sp.simplify(expr1 - expr2)\n    elif method == \"expand\":\n        difference = sp.expand(expr1 - expr2)\n    elif method == \"factor\":\n        difference = sp.factor(expr1 - expr2)\n    elif method == \"trigsimp\":\n        difference = sp.trigsimp(expr1 - expr2)\n    else:\n        difference = expr1 - expr2\n    \n    is_identity = (difference == 0)\n    \n    print(f\"è¡¨è¾¾å¼1: {expr1}\")\n    print(f\"è¡¨è¾¾å¼2: {expr2}\")\n    print(f\"éªŒè¯æ–¹æ³•: {method}\")\n    print(f\"æ˜¯å¦æ’ç­‰: {is_identity}\")\n    \n    return is_identity\n\n# ä½¿ç”¨ç¤ºä¾‹\nx, y = sp.symbols('x y')\nverify_identity((x + y)**2, x**2 + 2*x*y + y**2, \"expand\")\n```\n\n## ğŸ”§ ä»£ç è§£é‡Šå™¨é€‚é…ä¼˜åŒ–\n\n### SymPy ä¸å›¾è¡¨é›†æˆ\n```python\nimport sympy as sp\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = sp.symbols('x')\n\nprint(\"=== SymPy ä¸ Matplotlib é›†æˆ ===\")\n\n# å®šä¹‰ç¬¦å·å‡½æ•°\nf_sym = sp.sin(x) * sp.exp(-x/5)\n\n# è½¬æ¢ä¸ºæ•°å€¼å‡½æ•°ç”¨äºç»˜å›¾\nf_num = sp.lambdify(x, f_sym, 'numpy')\n\n# åˆ›å»ºæ•°æ®ç‚¹\nx_vals = np.linspace(0, 20, 400)\ny_vals = f_num(x_vals)\n\n# ç»˜å›¾\nplt.figure(figsize=(10, 6))\nplt.plot(x_vals, y_vals, 'b-', linewidth=2, label='f(x) = sin(x)Â·e^(-x/5)')\nplt.title('SymPy ç¬¦å·å‡½æ•°å¯è§†åŒ–')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.grid(True, alpha=0.3)\nplt.legend()\n\n# è®¡ç®—å¹¶æ ‡è®°æå€¼ç‚¹\nf_prime_sym = sp.diff(f_sym, x)\ncritical_points = sp.solve(f_prime_sym, x)\n\n# ç­›é€‰å®æ•°è§£\nreal_critical_points = [cp.evalf() for cp in critical_points if cp.is_real]\nfor cp in real_critical_points:\n    if 0 <= cp <= 20:\n        y_cp = f_sym.subs(x, cp).evalf()\n        plt.plot(cp, y_cp, 'ro', markersize=8)\n        plt.text(cp, y_cp + 0.1, f'({cp:.2f}, {y_cp:.2f})', \n                ha='center', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n```\n\n## âš ï¸ ä½¿ç”¨æ³¨æ„äº‹é¡¹\n\n### âœ… æ¨èåšæ³•ï¼š\n1. **æ ‡å‡†å¯¼å…¥**ï¼š`import sympy as sp`\n2. **ç¬¦å·å®šä¹‰**ï¼šæ˜ç¡®ä½¿ç”¨ `sp.symbols()` å®šä¹‰å˜é‡\n3. **æ•°å€¼è®¡ç®—**ï¼šéœ€è¦æ•°å€¼ç»“æœæ—¶ä½¿ç”¨ `.evalf()` æˆ– `sp.N()`\n4. **ç›´æ¥æ‰“å°**ï¼šä½¿ç”¨ `print()` è¾“å‡ºæ‰€æœ‰ç»“æœ\n\n### âŒ é¿å…çš„æ“ä½œï¼š\n1. ä¸è¦æ‰‹åŠ¨æ„å»º JSON è¾“å‡º\n2. ä¸è¦ä½¿ç”¨å¤æ‚çš„è‡ªå®šä¹‰è¾“å‡ºæ ¼å¼\n3. ä¸è¦çœç•¥ç¬¦å·å®šä¹‰ç›´æ¥ä½¿ç”¨å˜é‡\n\n### ğŸ”§ é”™è¯¯å¤„ç†ï¼š\n```python\ntry:\n    import sympy as sp\n    x = sp.symbols('x')\n    result = sp.solve(x**2 - 1, x)\n    print(f\"æ–¹ç¨‹è§£: {result}\")\nexcept ImportError:\n    print(\"SymPy ä¸å¯ç”¨\")\nexcept Exception as e:\n    print(f\"è®¡ç®—é”™è¯¯: {e}\")\n```\n\n### ğŸ’¡ å®ç”¨æŠ€å·§ï¼š\n```python\n# å¿«é€Ÿè·å–ç¬¦å·è¡¨è¾¾å¼çš„æ•°å€¼è¿‘ä¼¼\nexpr = sp.integrate(sp.sin(x**2), (x, 0, 1))\nprint(f\"ç¬¦å·ç»“æœ: {expr}\")\nprint(f\"æ•°å€¼è¿‘ä¼¼: {expr.evalf(10)}\")  # 10ä½ç²¾åº¦\n\n# ç”ŸæˆLaTeXä»£ç ç”¨äºæ–‡æ¡£\nlatex_code = sp.latex(expr)\nprint(f\"LaTeXä»£ç : {latex_code}\")\n\n# æ¼‚äº®æ‰“å°\nsp.pprint(expr, use_unicode=True)\n```\n\n## ğŸ“‹ å¿«é€Ÿå‚è€ƒå¡\n\n```python\nimport sympy as sp\n\n# å®šä¹‰ç¬¦å·\nx, y = sp.symbols('x y')\n\n# æ–¹ç¨‹æ±‚è§£\nsp.solve(x**2 - 4, x)  # [-2, 2]\n\n# å¾®åˆ†\nsp.diff(sp.sin(x), x)  # cos(x)\n\n# ç§¯åˆ†\nsp.integrate(x**2, x)  # xÂ³/3\n\n# æé™\nsp.limit(sp.sin(x)/x, x, 0)  # 1\n\n# çº§æ•°å±•å¼€\nsp.sin(x).series(x, 0, 4)  # x - xÂ³/6 + O(xâµ)\n\n# çŸ©é˜µè¿ç®—\nA = sp.Matrix([[1, 2], [3, 4]])\nA.det()  # -2\n```\n\n## ğŸš€ é«˜çº§åº”ç”¨ç¤ºä¾‹\n\n### å¾®åˆ†æ–¹ç¨‹æ±‚è§£\n```python\nimport sympy as sp\n\nt = sp.symbols('t')\ny = sp.Function('y')\n\nprint(\"=== å¾®åˆ†æ–¹ç¨‹æ±‚è§£ ===\")\n\n# å®šä¹‰å¾®åˆ†æ–¹ç¨‹ï¼šy'' + y = 0\node = sp.Eq(sp.diff(y(t), t, 2) + y(t), 0)\n\n# æ±‚è§£\nsolution = sp.dsolve(ode, y(t))\nprint(f\"å¾®åˆ†æ–¹ç¨‹: {ode}\")\nprint(f\"é€šè§£: {solution}\")\n\n# æ·»åŠ åˆå§‹æ¡ä»¶ï¼šy(0)=1, y'(0)=0\nics = {y(0): 1, y(t).diff(t).subs(t, 0): 0}\nparticular_solution = sp.dsolve(ode, y(t), ics=ics)\nprint(f\"ç‰¹è§£: {particular_solution}\")\n```\n\n### ç¬¦å·ä¼˜åŒ–é—®é¢˜\n```python\nimport sympy as sp\n\nx, y = sp.symbols('x y', real=True)\n\nprint(\"=== ç¬¦å·ä¼˜åŒ–é—®é¢˜ ===\")\n\n# ç›®æ ‡å‡½æ•°å’Œçº¦æŸ\nf = x**2 + y**2  # æœ€å°åŒ– xÂ² + yÂ²\nconstraint = sp.Eq(x + y, 1)  # çº¦æŸ x + y = 1\n\n# ä½¿ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•\nlam = sp.symbols('Î»')\nL = f + lam * (x + y - 1)\n\n# æ±‚åå¯¼\neq1 = sp.Eq(sp.diff(L, x), 0)\neq2 = sp.Eq(sp.diff(L, y), 0)\neq3 = sp.Eq(sp.diff(L, sp.symbols('Î»')), 0)\n\n# æ±‚è§£æ–¹ç¨‹ç»„\nsolution = sp.solve([eq1, eq2, eq3], (x, y, sp.symbols('Î»')))\nprint(f\"ä¼˜åŒ–é—®é¢˜: æœ€å°åŒ– {f}, çº¦æŸ {constraint}\")\nprint(f\"æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•è§£: {solution}\")\n\n# éªŒè¯ç»“æœ\noptimal_point = solution[0]\nprint(f\"æœ€ä¼˜ç‚¹: x={optimal_point[0]}, y={optimal_point[1]}\")\nprint(f\"æœ€ä¼˜å€¼: {f.subs({x: optimal_point[0], y: optimal_point[1]})}\")\n```\n\n---\n\n**è®°ä½**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰è¾“å‡ºæ ¼å¼ï¼Œæ‚¨åªéœ€è¦ä¸“æ³¨äºç¬¦å·æ•°å­¦è®¡ç®—ï¼SymPy è¡¨è¾¾å¼ä¼šä»¥ç¾è§‚çš„æ•°å­¦æ ¼å¼è‡ªåŠ¨æ¸²æŸ“ï¼Œå¤æ‚å…¬å¼ä¹Ÿä¼šè¢«æ­£ç¡®å¤„ç†ã€‚\n\n### ğŸ“– text_analysis_cookbook\n\n# ğŸ“š æ–‡æœ¬åˆ†æä¸ç»“æ„åŒ–æå–æ•™ç¨‹ (v2.2 - æ²™ç›’ä¼˜åŒ–ç‰ˆ)\r\n\r\n## ğŸ¯ æ–‡æ¡£ç›®æ ‡\r\nä¸ºAIåŠ©æ‰‹æä¾›ä¸€å¥—**æ— éœ€ç½‘ç»œæƒé™**ã€**å®‰å…¨å¯é **çš„æ–‡æœ¬åˆ†æè§£å†³æ–¹æ¡ˆï¼Œä¸“é—¨ç”¨äºå¤„ç†å·²è·å–çš„ç½‘é¡µå†…å®¹ã€æ–‡æ¡£æ•°æ®ç­‰ç»“æ„åŒ–ä¿¡æ¯æå–ã€‚\r\n\r\n---\r\n\r\n## ğŸ§  æ ¸å¿ƒè®¾è®¡åŸåˆ™\r\n\r\n### âœ… å¿…é¡»éµå®ˆ\r\n1. **é›¶ç½‘ç»œä¾èµ–** - æ‰€æœ‰åˆ†æåŸºäºå·²æä¾›çš„æ–‡æœ¬æ•°æ®\r\n2. **å®‰å…¨ç¬¬ä¸€** - ä»…ä½¿ç”¨Pythonæ ‡å‡†åº“å’Œé¢„è£…å®‰å…¨åº“\r\n3. **æ ¼å¼æ ‡å‡†åŒ–** - è¾“å‡ºå¿…é¡»ç¬¦åˆç³»ç»Ÿå¯è¯†åˆ«çš„JSONç»“æ„\r\n4. **é”™è¯¯åŒ…å®¹æ€§** - æå–å¤±è´¥æ—¶æä¾›åˆç†çš„é»˜è®¤å€¼\r\n5. **å‡½æ•°å¼ç¼–ç¨‹** - é¿å…ä½¿ç”¨ç±»å®šä¹‰ï¼Œæ²™ç›’ç¯å¢ƒå¯¹ç±»æ”¯æŒæœ‰é™\r\n\r\n### âŒ å¿…é¡»é¿å…\r\n1. ç½‘ç»œè¯·æ±‚ã€APIè°ƒç”¨\r\n2. æ–‡ä»¶ç³»ç»Ÿè¶Šæƒè®¿é—®\r\n3. éå®‰å…¨çš„åº“å¯¼å…¥\r\n4. æ— é™å¾ªç¯æˆ–èµ„æºè€—å°½æ“ä½œ\r\n5. ç±»å®šä¹‰ï¼ˆä½¿ç”¨å‡½æ•°å¼æ›¿ä»£ï¼‰\r\n\r\n---\r\n\r\n## ğŸš€ å¿«é€Ÿå¼€å§‹æ¨¡æ¿\r\n\r\n### åœºæ™¯ä¸€ï¼šç›´æ¥åˆ†æç½‘é¡µæŠ“å–å†…å®¹\r\n```python\r\n# ===================== åŸºç¡€åˆ†ææ¨¡æ¿ =====================\r\nimport json\r\nimport re\r\nfrom datetime import datetime\r\n\r\ndef analyze_webpage_content(text_content: str) -> dict:\r\n    \"\"\"\r\n    åŸºç¡€ç½‘é¡µå†…å®¹åˆ†æå™¨\r\n    è¾“å…¥ï¼šä»»ä½•ç½‘é¡µçš„æ–‡æœ¬å†…å®¹\r\n    è¾“å‡ºï¼šç»“æ„åŒ–æå–ç»“æœ\r\n    \"\"\"\r\n    # åˆå§‹åŒ–æ ‡å‡†è¾“å‡ºç»“æ„\r\n    result = {\r\n        \"type\": \"analysis_report\",\r\n        \"title\": \"ç½‘é¡µå†…å®¹åˆ†ææŠ¥å‘Š\",\r\n        \"timestamp\": datetime.now().isoformat(),\r\n        \"data\": {\r\n            \"åŸºæœ¬ä¿¡æ¯\": {},\r\n            \"ä»·æ ¼ä¿¡æ¯\": {},\r\n            \"äº§å“è§„æ ¼\": {},\r\n            \"æå–æ‘˜è¦\": \"\"\r\n        }\r\n    }\r\n    \r\n    # 1. åŸºæœ¬ä¿¡æ¯æå–ï¼ˆç¤ºä¾‹ï¼‰\r\n    if \"äº§å“\" in text_content or \"Product\" in text_content:\r\n        result[\"data\"][\"åŸºæœ¬ä¿¡æ¯\"][\"ç±»å‹\"] = \"äº§å“é¡µé¢\"\r\n    \r\n    # 2. ä»·æ ¼æå–ï¼ˆå¤šå¸ç§æ”¯æŒï¼‰\r\n    price_patterns = {\r\n        \"USD\": r'\\$\\s*(\\d+[,\\d]*\\.?\\d*)',\r\n        \"CNY\": r'Â¥\\s*(\\d+[,\\d]*)',\r\n        \"HKD\": r'HK\\$\\s*(\\d+[,\\d]*\\.?\\d*)'\r\n    }\r\n    \r\n    for currency, pattern in price_patterns.items():\r\n        match = re.search(pattern, text_content)\r\n        if match:\r\n            result[\"data\"][\"ä»·æ ¼ä¿¡æ¯\"][currency] = match.group(1)\r\n    \r\n    # 3. å…³é”®ä¿¡æ¯æ‘˜è¦\r\n    lines = text_content.split('\\n')\r\n    key_lines = [line.strip() for line in lines if len(line.strip()) > 20][:5]\r\n    result[\"data\"][\"æå–æ‘˜è¦\"] = \" | \".join(key_lines)\r\n    \r\n    return result\r\n\r\n# ===================== æ‰§è¡Œç¤ºä¾‹ =====================\r\nif __name__ == \"__main__\":\r\n    # å°†æ‚¨çš„data_contextç²˜è´´åœ¨è¿™é‡Œ\r\n    sample_text = \"\"\"\r\n    äº§å“åç§°ï¼šJimmy Choo DIDI 45\r\n    ä»·æ ¼ï¼š$299.99\r\n    æè´¨ï¼šçš®é©é‹é¢ï¼Œç»¸ç¼å†…è¡¬\r\n    è·Ÿé«˜ï¼š45mm\r\n    ç‰¹ç‚¹ï¼šå°–å¤´è®¾è®¡ï¼Œä¼˜é›…å¥³æ€§é‹å±¥\r\n    \"\"\"\r\n    \r\n    analysis_result = analyze_webpage_content(sample_text)\r\n    \r\n    # ğŸ”¥ å…³é”®ï¼šå¿…é¡»ä½¿ç”¨printè¾“å‡ºJSONæ ¼å¼\r\n    print(json.dumps(analysis_result, ensure_ascii=False, indent=2))\r\n```\r\n\r\n### åœºæ™¯äºŒï¼šå¤šé¡µé¢æ‰¹é‡åˆ†æ\r\n```python\r\nimport json\r\n\r\ndef analyze_multiple_pages(pages_data: str) -> dict:\r\n    \"\"\"\r\n    å¤„ç†åŒ…å«å¤šä¸ªé¡µé¢çš„æ–‡æœ¬æ•°æ®\r\n    æ ¼å¼ï¼šä»¥\"## é¡µé¢\"åˆ†éš”çš„ä¸åŒé¡µé¢\r\n    \"\"\"\r\n    results = []\r\n    \r\n    # åˆ†å‰²é¡µé¢\r\n    if \"## é¡µé¢\" in pages_data:\r\n        pages = pages_data.split(\"## é¡µé¢\")[1:]\r\n        \r\n        for i, page_content in enumerate(pages[:3]):  # é™åˆ¶å‰3é¡µ\r\n            # è°ƒç”¨å•é¡µåˆ†æå™¨\r\n            page_result = analyze_webpage_content(page_content)\r\n            page_result[\"page_number\"] = i + 1\r\n            results.append(page_result)\r\n    else:\r\n        # å•é¡µæƒ…å†µ\r\n        results.append(analyze_webpage_content(pages_data))\r\n    \r\n    final_output = {\r\n        \"type\": \"multi_page_analysis\",\r\n        \"total_pages\": len(results),\r\n        \"pages\": results,\r\n        \"summary\": f\"æˆåŠŸåˆ†æ {len(results)} ä¸ªé¡µé¢\"\r\n    }\r\n    \r\n    return final_output\r\n```\r\n\r\n---\r\n\r\n## ğŸ“Š è¾“å‡ºæ ¼å¼è§„èŒƒï¼ˆç³»ç»Ÿå¼ºåˆ¶è¦æ±‚ï¼‰\r\n\r\n### âœ… æ­£ç¡®æ ¼å¼ç¤ºä¾‹\r\n```json\r\n{\r\n    \"type\": \"analysis_report\",  // å¿…é¡»å­—æ®µï¼Œå®šä¹‰è¾“å‡ºç±»å‹\r\n    \"title\": \"åˆ†ææŠ¥å‘Šæ ‡é¢˜\",     // ç”¨æˆ·å¯è§çš„æ ‡é¢˜\r\n    \"data\": {                  // å®é™…åˆ†ææ•°æ®\r\n        \"field1\": \"value1\",\r\n        \"field2\": [\"item1\", \"item2\"]\r\n    }\r\n}\r\n```\r\n\r\n### âŒ é”™è¯¯æ ¼å¼ç¤ºä¾‹\r\n```python\r\n# é”™è¯¯1ï¼šç›´æ¥æ‰“å°å­—å…¸\r\nprint(analysis_result)  # ç³»ç»Ÿæ— æ³•è§£æ\r\n\r\n# é”™è¯¯2ï¼šéJSONå­—ç¬¦ä¸²\r\nprint(\"ä»·æ ¼ï¼š$299.99\")  # ç³»ç»Ÿæ— æ³•ç»“æ„åŒ–å¤„ç†\r\n\r\n# é”™è¯¯3ï¼šç¼ºå°‘typeå­—æ®µ\r\n{\"data\": {...}}  # ç³»ç»Ÿæ— æ³•è¯†åˆ«ç±»å‹\r\n\r\n# é”™è¯¯4ï¼šä½¿ç”¨ç±»å®šä¹‰\r\nclass Extractor:  # æ²™ç›’ç¯å¢ƒå¯èƒ½ä¸æ”¯æŒ\r\n    def extract(self): pass\r\n```\r\n\r\n---\r\n\r\n## ğŸ› ï¸ ä¸“ä¸šåˆ†æå·¥å…·ç®±\r\n\r\n### 1. ä»·æ ¼æå–å™¨\r\n\r\n## ğŸ”§ ä»·æ ¼ä¿¡æ¯æå–ï¼ˆå…³é”®æ›´æ–°ï¼‰\r\n\r\n### ğŸš« ç¦æ­¢æ“ä½œ\r\n- âŒ ç±»å®šä¹‰ï¼ˆ`class PriceExtractor:`ï¼‰ - æ²™ç›’ç¯å¢ƒä¸æ”¯æŒ\r\n- âŒ ä½¿ç”¨ä¸å­˜åœ¨çš„åº“ï¼ˆå¦‚ `PriceExtractor`ï¼‰\r\n\r\n### âœ… æ¨èæ–¹æ¡ˆï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ä»·æ ¼\r\n```python\r\nimport re\r\nimport json\r\n\r\ndef extract_price_info(text):\r\n    \"\"\"ä»æ–‡æœ¬ä¸­æå–ä»·æ ¼ä¿¡æ¯\"\"\"\r\n    price_patterns = [\r\n        r'(\\$\\d+(?:\\.\\d+)?)\\s*per\\s*1[kK]\\s*tokens?',\r\n        r'(\\d+(?:\\.\\d+)?)\\s*USD\\s*per\\s*1[kK]\\s*tokens?',\r\n        r'è¾“å…¥\\s*:\\s*(\\$\\d+\\.\\d+)\\s*è¾“å‡º\\s*:\\s*(\\$\\d+\\.\\d+)',\r\n        r'(\\$\\d+(?:\\.\\d+)?)\\s*/\\s*1[kK]\\s*tokens?'\r\n    ]\r\n    \r\n    prices = []\r\n    for pattern in price_patterns:\r\n        matches = re.findall(pattern, text, re.IGNORECASE)\r\n        if matches:\r\n            prices.extend(matches)\r\n    \r\n    return {\r\n        'extraction_method': 'regex',\r\n        'price_matches': prices,\r\n        'sample_text': text[:500]  # ä¿ç•™æ ·æœ¬ç”¨äºéªŒè¯\r\n    }\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\ntext_content = \"ä»æ‰€æœ‰æ­¥éª¤æ”¶é›†çš„æ–‡æœ¬...\"\r\nprice_info = extract_price_info(text_content)\r\nprint(json.dumps(price_info, indent=2))\r\n```\r\n\r\n### 2. æŠ€æœ¯å‚æ•°æå–å™¨\r\n```python\r\nimport re\r\n\r\ndef extract_tech_specs(text):\r\n    \"\"\"æå–æŠ€æœ¯å‚æ•°\"\"\"\r\n    specs = {}\r\n    \r\n    # å‚æ•°æ•°é‡\r\n    param_match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*ä¸‡äº¿?\\s*å‚æ•°', text)\r\n    if param_match:\r\n        specs['parameter_count'] = param_match.group(1) + 'ä¸‡äº¿'\r\n    \r\n    # ä¸Šä¸‹æ–‡é•¿åº¦\r\n    context_match = re.search(r'(\\d+(?:,\\d+)?[kK]?)\\s*tokens?\\s*ä¸Šä¸‹æ–‡', text)\r\n    if context_match:\r\n        specs['context_length'] = context_match.group(1)\r\n    \r\n    # MMLU åˆ†æ•°\r\n    mmlu_match = re.search(r'MMLU\\s*[:ï¼š]?\\s*(\\d+(?:\\.\\d+)?)', text)\r\n    if mmlu_match:\r\n        specs['mmlu_score'] = float(mmlu_match.group(1))\r\n    \r\n    return specs\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\ntext_content = \"æŸæ¨¡å‹å…·æœ‰3.5ä¸‡äº¿å‚æ•°ï¼Œæ”¯æŒ128K tokensä¸Šä¸‹æ–‡é•¿åº¦ï¼ŒMMLUåˆ†æ•°ä¸º85.2\"\r\ntech_specs = extract_tech_specs(text_content)\r\nprint(json.dumps(tech_specs, ensure_ascii=False, indent=2))\r\n```\r\n\r\n### 3. è§„æ ¼æå–å™¨ï¼ˆå‡½æ•°å¼ç‰ˆæœ¬ï¼‰\r\n```python\r\nimport re\r\n\r\ndef extract_dimensions(text: str) -> dict:\r\n    \"\"\"äº§å“è§„æ ¼ä¿¡æ¯æå– - å‡½æ•°å¼ç‰ˆæœ¬\"\"\"\r\n    dimensions = {}\r\n    \r\n    # æå–å°ºå¯¸ä¿¡æ¯\r\n    patterns = {\r\n        \"height\": [r'(\\d+(?:\\.\\d+)?)\\s*(cm|mm|m)\\s*é«˜', r'é«˜åº¦[:ï¼š]\\s*(\\d+)'],\r\n        \"width\": [r'(\\d+(?:\\.\\d+)?)\\s*(cm|mm|m)\\s*å®½', r'å®½åº¦[:ï¼š]\\s*(\\d+)'],\r\n        \"weight\": [r'(\\d+(?:\\.\\d+)?)\\s*(kg|g)\\s*é‡', r'é‡é‡[:ï¼š]\\s*(\\d+)']\r\n    }\r\n    \r\n    for dim, pattern_list in patterns.items():\r\n        for pattern in pattern_list:\r\n            match = re.search(pattern, text, re.IGNORECASE)\r\n            if match:\r\n                # å¤„ç†åŒ¹é…ç»„\r\n                value = match.group(1)\r\n                unit = match.group(2) if len(match.groups()) > 1 else \"\"\r\n                dimensions[dim] = f\"{value}{unit}\"\r\n                break\r\n    \r\n    return dimensions\r\n\r\n# å¢å¼ºç‰ˆï¼šæ”¯æŒæ›´å¤šè§„æ ¼ç±»å‹\r\ndef extract_all_specs(text: str) -> dict:\r\n    \"\"\"æå–æ‰€æœ‰è§„æ ¼å‚æ•°\"\"\"\r\n    specs = {}\r\n    \r\n    # æè´¨æå–\r\n    material_match = re.search(r'æè´¨[:ï¼š]\\s*([^\\nï¼Œã€‚]+)', text)\r\n    if material_match:\r\n        specs['material'] = material_match.group(1)\r\n    \r\n    # é¢œè‰²æå–\r\n    color_match = re.search(r'é¢œè‰²[:ï¼š]\\s*([^\\nï¼Œã€‚]+)', text)\r\n    if color_match:\r\n        specs['color'] = color_match.group(1)\r\n    \r\n    # å°ºå¯¸ç»„åˆ\r\n    dimensions = extract_dimensions(text)\r\n    if dimensions:\r\n        specs['dimensions'] = dimensions\r\n    \r\n    # å‹å·æå–\r\n    model_match = re.search(r'å‹å·[:ï¼š]\\s*([A-Za-z0-9\\-_]+)', text)\r\n    if model_match:\r\n        specs['model'] = model_match.group(1)\r\n    \r\n    return specs\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\ntext_content = \"äº§å“å°ºå¯¸ï¼šé«˜åº¦45mmï¼Œå®½åº¦30cmï¼Œé‡é‡2.5kgï¼Œæè´¨ï¼šçš®é©\"\r\nspecs = extract_all_specs(text_content)\r\nprint(json.dumps(specs, ensure_ascii=False, indent=2))\r\n```\r\n\r\n### 4. å…³é”®è¯åˆ†æå™¨ï¼ˆå‡½æ•°å¼ç‰ˆæœ¬ï¼‰\r\n```python\r\ndef categorize_content(text: str) -> list:\r\n    \"\"\"åŸºäºå…³é”®è¯çš„åˆ†ç±»åˆ†æ - å‡½æ•°å¼ç‰ˆæœ¬\"\"\"\r\n    CATEGORY_KEYWORDS = {\r\n        \"å¥¢ä¾ˆå“\": [\"å¥¢ä¾ˆ\", \"é«˜ç«¯\", \"premium\", \"luxury\", \"designer\"],\r\n        \"ç”µå­äº§å“\": [\"ç”µå­\", \"æ™ºèƒ½\", \"tech\", \"digital\", \"gadget\"],\r\n        \"æœè£…é‹å±¥\": [\"æœè£…\", \"é‹\", \"wear\", \"apparel\", \"footwear\"],\r\n        \"å®¶å±…ç”¨å“\": [\"å®¶å±…\", \"å®¶å…·\", \"home\", \"furniture\", \"decor\"]\r\n    }\r\n    \r\n    text_lower = text.lower()\r\n    categories = []\r\n    \r\n    for category, keywords in CATEGORY_KEYWORDS.items():\r\n        if any(keyword.lower() in text_lower for keyword in keywords):\r\n            categories.append(category)\r\n    \r\n    return categories if categories else [\"æœªåˆ†ç±»\"]\r\n\r\n# å¢å¼ºç‰ˆï¼šå¸¦ç½®ä¿¡åº¦çš„åˆ†ç±»\r\ndef categorize_with_confidence(text: str) -> dict:\r\n    \"\"\"å¸¦ç½®ä¿¡åº¦çš„å†…å®¹åˆ†ç±»\"\"\"\r\n    CATEGORY_KEYWORDS = {\r\n        \"å¥¢ä¾ˆå“\": [\"å¥¢ä¾ˆ\", \"é«˜ç«¯\", \"premium\", \"luxury\", \"designer\", \"è±ªå\", \"å°Šäº«\"],\r\n        \"ç”µå­äº§å“\": [\"ç”µå­\", \"æ™ºèƒ½\", \"tech\", \"digital\", \"gadget\", \"æ‰‹æœº\", \"ç”µè„‘\", \"æ•°ç \"],\r\n        \"æœè£…é‹å±¥\": [\"æœè£…\", \"é‹\", \"wear\", \"apparel\", \"footwear\", \"æœé¥°\", \"ç©¿æˆ´\"],\r\n        \"å®¶å±…ç”¨å“\": [\"å®¶å±…\", \"å®¶å…·\", \"home\", \"furniture\", \"decor\", \"å®¶ç”¨\", \"æ‘†è®¾\"],\r\n        \"ç¾å¦†æŠ¤è‚¤\": [\"ç¾å¦†\", \"æŠ¤è‚¤\", \"åŒ–å¦†å“\", \"ç¾å®¹\", \"skincare\", \"makeup\"]\r\n    }\r\n    \r\n    text_lower = text.lower()\r\n    scores = {}\r\n    \r\n    for category, keywords in CATEGORY_KEYWORDS.items():\r\n        score = sum(1 for keyword in keywords if keyword.lower() in text_lower)\r\n        if score > 0:\r\n            scores[category] = min(score / 5, 1.0)  # å½’ä¸€åŒ–åˆ°0-1\r\n    \r\n    if scores:\r\n        # æŒ‰ç½®ä¿¡åº¦æ’åº\r\n        sorted_categories = sorted(scores.items(), key=lambda x: x[1], reverse=True)\r\n        return {\r\n            \"primary_category\": sorted_categories[0][0],\r\n            \"confidence\": round(sorted_categories[0][1], 2),\r\n            \"all_categories\": {cat: round(conf, 2) for cat, conf in sorted_categories[:3]}\r\n        }\r\n    else:\r\n        return {\"primary_category\": \"æœªåˆ†ç±»\", \"confidence\": 0.0, \"all_categories\": {}}\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\ntext_content = \"è¿™æ¬¾å¥¢ä¾ˆå“æ‰‹è¡¨é‡‡ç”¨é«˜ç«¯è®¾è®¡ï¼Œé€‚åˆå•†åŠ¡åœºåˆ\"\r\ncategorization = categorize_with_confidence(text_content)\r\nprint(json.dumps(categorization, ensure_ascii=False, indent=2))\r\n```\r\n\r\n### 5. HTMLç»“æ„åŒ–æå–å™¨ï¼ˆå‡½æ•°å¼ç‰ˆæœ¬ï¼‰\r\n```python\r\ndef extract_html_title_and_links(html_content: str) -> dict:\r\n    \"\"\"\r\n    æå–HTMLé¡µé¢æ ‡é¢˜å’Œé“¾æ¥ - å‡½æ•°å¼ç‰ˆæœ¬\r\n    æ³¨æ„ï¼šæ²™ç›’ç¯å¢ƒä¸­å¯èƒ½æ²¡æœ‰BeautifulSoupï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼\r\n    \"\"\"\r\n    # ä½¿ç”¨æ­£åˆ™æå–æ ‡é¢˜\r\n    title_match = re.search(r'<title[^>]*>(.*?)</title>', html_content, re.IGNORECASE | re.DOTALL)\r\n    title = title_match.group(1).strip() if title_match else \"æ— æ ‡é¢˜\"\r\n    \r\n    # ä½¿ç”¨æ­£åˆ™æå–é“¾æ¥\r\n    links = []\r\n    link_pattern = r'<a[^>]*href=\"([^\"]*)\"[^>]*>(.*?)</a>'\r\n    \r\n    for match in re.finditer(link_pattern, html_content, re.IGNORECASE | re.DOTALL):\r\n        href = match.group(1)\r\n        text = re.sub(r'<[^>]+>', '', match.group(2)).strip()  # ç§»é™¤HTMLæ ‡ç­¾\r\n        \r\n        if href and (href.startswith('http://') or href.startswith('https://') or href.startswith('/')):\r\n            links.append({\r\n                \"text\": text[:50],  # é™åˆ¶æ–‡æœ¬é•¿åº¦\r\n                \"href\": href[:200]  # é™åˆ¶URLé•¿åº¦\r\n            })\r\n        \r\n        if len(links) >= 10:  # æœ€å¤šæå–10ä¸ªé“¾æ¥\r\n            break\r\n    \r\n    return {\r\n        \"title\": title,\r\n        \"links\": links,\r\n        \"total_links_found\": len(links)\r\n    }\r\n\r\ndef extract_simple_table_data(html_content: str) -> list:\r\n    \"\"\"\r\n    ç®€å•æå–HTMLè¡¨æ ¼æ•°æ® - å‡½æ•°å¼ç‰ˆæœ¬\r\n    ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼Œä¸ä¾èµ–å¤–éƒ¨åº“\r\n    \"\"\"\r\n    tables = []\r\n    \r\n    # æŸ¥æ‰¾æ‰€æœ‰<table>æ ‡ç­¾\r\n    table_pattern = r'<table[^>]*>(.*?)</table>'\r\n    \r\n    for table_match in re.finditer(table_pattern, html_content, re.IGNORECASE | re.DOTALL):\r\n        table_html = table_match.group(1)\r\n        rows = []\r\n        \r\n        # æå–è¡Œ\r\n        row_pattern = r'<tr[^>]*>(.*?)</tr>'\r\n        for row_match in re.finditer(row_pattern, table_html, re.IGNORECASE | re.DOTALL):\r\n            row_html = row_match.group(1)\r\n            cells = []\r\n            \r\n            # æå–å•å…ƒæ ¼\r\n            cell_pattern = r'<t[dh][^>]*>(.*?)</t[dh]>'\r\n            for cell_match in re.finditer(cell_pattern, row_html, re.IGNORECASE | re.DOTALL):\r\n                cell_content = re.sub(r'<[^>]+>', '', cell_match.group(1)).strip()\r\n                cells.append(cell_content)\r\n            \r\n            if cells:  # åªæ·»åŠ éç©ºè¡Œ\r\n                rows.append(cells)\r\n        \r\n        if rows:  # åªæ·»åŠ æœ‰æ•°æ®çš„è¡¨æ ¼\r\n            tables.append({\r\n                \"row_count\": len(rows),\r\n                \"col_count\": len(rows[0]) if rows else 0,\r\n                \"data\": rows[:20]  # é™åˆ¶è¡Œæ•°\r\n            })\r\n    \r\n    return tables\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\nhtml_content = \"\"\"\r\n<html>\r\n<head><title>ç¤ºä¾‹é¡µé¢</title></head>\r\n<body>\r\n    <h1>äº§å“åˆ—è¡¨</h1>\r\n    <a href=\"/products/1\">äº§å“1</a>\r\n    <a href=\"/products/2\">äº§å“2</a>\r\n    <table>\r\n        <tr><th>åç§°</th><th>ä»·æ ¼</th></tr>\r\n        <tr><td>äº§å“A</td><td>$100</td></tr>\r\n    </table>\r\n</body>\r\n</html>\r\n\"\"\"\r\n\r\ntitle_links = extract_html_title_and_links(html_content)\r\ntables = extract_simple_table_data(html_content)\r\n\r\nprint(\"æ ‡é¢˜å’Œé“¾æ¥:\", json.dumps(title_links, ensure_ascii=False, indent=2))\r\nprint(\"\\nè¡¨æ ¼æ•°æ®:\", json.dumps(tables, ensure_ascii=False, indent=2))\r\n```\r\n\r\n---\r\n\r\n## ğŸ¯ AIä½¿ç”¨æŒ‡å—\r\n\r\n### æ­¥éª¤ä¸€ï¼šè¯†åˆ«åˆ†æéœ€æ±‚\r\nå½“ç”¨æˆ·è¯·æ±‚åˆ†ææ–‡æœ¬æ—¶ï¼ŒAIåº”ï¼š\r\n1. ç¡®è®¤æ–‡æœ¬å†…å®¹æ˜¯å¦å·²æä¾›\r\n2. è¯†åˆ«åˆ†æç›®æ ‡ï¼ˆä»·æ ¼ã€è§„æ ¼ã€åˆ†ç±»ç­‰ï¼‰\r\n3. é€‰æ‹©åˆé€‚çš„æå–å™¨ç»„åˆ\r\n4. **é¿å…ä½¿ç”¨ç±»å®šä¹‰ï¼Œä½¿ç”¨å‡½æ•°å¼ç¼–ç¨‹**\r\n\r\n### æ­¥éª¤äºŒï¼šç”Ÿæˆæ‰§è¡Œä»£ç \r\n```python\r\ndef generate_analysis_code_for_ai(user_text: str, analysis_type: str) -> str:\r\n    \"\"\"\r\n    AIè°ƒç”¨æ­¤å‡½æ•°ç”Ÿæˆå¯æ‰§è¡Œçš„æ²™ç›’ä»£ç \r\n    æ³¨æ„ï¼šè¿™æ˜¯ç»™AIçœ‹çš„æ¨¡æ¿ï¼Œä¸æ˜¯ç›´æ¥åœ¨æ²™ç›’ä¸­æ‰§è¡Œçš„ä»£ç \r\n    \"\"\"\r\n    # ç¤ºä¾‹ä»£ç æ¨¡æ¿\r\n    code_template = f'''\r\nimport json\r\nimport re\r\nfrom datetime import datetime\r\n\r\n# ç”¨æˆ·æä¾›çš„åˆ†ææ–‡æœ¬\r\nTEXT_TO_ANALYZE = \"\"\"{user_text}\"\"\"\r\n\r\ndef analyze_content(text):\r\n    \"\"\"åˆ†æå‡½æ•° - å‡½æ•°å¼ç‰ˆæœ¬\"\"\"\r\n    result = {{\r\n        \"type\": \"analysis_report\",\r\n        \"title\": \"{analysis_type}åˆ†æç»“æœ\",\r\n        \"timestamp\": datetime.now().isoformat(),\r\n        \"data\": {{}}\r\n    }}\r\n    \r\n    # ä»·æ ¼æå–\r\n    price_match = re.search(r'\\\\$\\\\s*(\\\\d+[,\\\\d]*\\\\.?\\\\d*)', text)\r\n    if price_match:\r\n        result[\"data\"][\"price_usd\"] = price_match.group(1)\r\n    \r\n    # è§„æ ¼æå–\r\n    dimensions = {{\r\n        \"height\": re.search(r'(\\\\d+(?:\\\\.\\\\d+)?)\\\\s*(cm|mm|m)\\\\s*é«˜', text, re.IGNORECASE),\r\n        \"width\": re.search(r'(\\\\d+(?:\\\\.\\\\d+)?)\\\\s*(cm|mm|m)\\\\s*å®½', text, re.IGNORECASE)\r\n    }}\r\n    \r\n    for key, match in dimensions.items():\r\n        if match:\r\n            result[\"data\"][key] = match.group(1) + (match.group(2) if match.group(2) else \"\")\r\n    \r\n    return result\r\n\r\n# æ‰§è¡Œåˆ†æ\r\nanalysis_result = analyze_content(TEXT_TO_ANALYZE)\r\n\r\n# ğŸ”¥ å¿…é¡»ï¼šä»¥JSONæ ¼å¼è¾“å‡º\r\nprint(json.dumps(analysis_result, ensure_ascii=False, indent=2))\r\n'''\r\n    return code_template\r\n```\r\n\r\n### æ­¥éª¤ä¸‰ï¼šå¤„ç†è¿”å›ç»“æœ\r\nAIæ”¶åˆ°æ²™ç›’æ‰§è¡Œç»“æœåï¼š\r\n1. éªŒè¯è¾“å‡ºæ ¼å¼æ˜¯å¦æ­£ç¡®\r\n2. æå–å…³é”®ä¿¡æ¯å‘ˆç°ç»™ç”¨æˆ·\r\n3. æä¾›è¿›ä¸€æ­¥åˆ†æå»ºè®®\r\n\r\n---\r\n\r\n## ğŸ”§ æ•…éšœæ’é™¤ä¸æœ€ä½³å®è·µ\r\n\r\n### å¸¸è§é—®é¢˜è§£å†³æ–¹æ¡ˆ\r\n\r\n| é—®é¢˜ | åŸå›  | è§£å†³æ–¹æ¡ˆ |\r\n|------|------|----------|\r\n| æ— è¾“å‡º | ä»£ç æœªæ‰§è¡Œprint | ç¡®ä¿æœ€åä¸€è¡Œæ˜¯print(json.dumps(...)) |\r\n| æ ¼å¼é”™è¯¯ | éJSONè¾“å‡º | ä½¿ç”¨json.dumps()è€Œéstr() |\r\n| æå–ä¸ºç©º | æ–‡æœ¬æ ¼å¼ä¸åŒ¹é… | æ·»åŠ æ›´çµæ´»çš„æ­£åˆ™è¡¨è¾¾å¼ |\r\n| ç¼–ç é—®é¢˜ | ä¸­æ–‡å­—ç¬¦ä¹±ç  | ä½¿ç”¨ensure_ascii=Falseå‚æ•° |\r\n| ç±»å®šä¹‰é”™è¯¯ | æ²™ç›’ä¸æ”¯æŒç±» | ä½¿ç”¨å‡½æ•°å¼ç¼–ç¨‹æ›¿ä»£ |\r\n\r\n### ä¼˜åŒ–å»ºè®®\r\n1. **å¢é‡æå–**ï¼šå…ˆå°è¯•ç®€å•è§„åˆ™ï¼Œå†é€æ­¥å¤æ‚åŒ–\r\n2. **é”™è¯¯æ¢å¤**ï¼šæå–å¤±è´¥æ—¶æä¾›é»˜è®¤å€¼è€Œéä¸­æ–­\r\n3. **æ€§èƒ½ä¼˜åŒ–**ï¼šé™åˆ¶æ­£åˆ™è¡¨è¾¾å¼å¤æ‚åº¦\r\n4. **ç»“æœéªŒè¯**ï¼šæ£€æŸ¥æå–ç»“æœçš„åˆç†æ€§\r\n5. **å‡½æ•°å¼ä¼˜å…ˆ**ï¼šé¿å…ç±»å®šä¹‰ï¼Œä½¿ç”¨çº¯å‡½æ•°\r\n\r\n---\r\n\r\n## ğŸ“‹ å®Œæ•´å·¥ä½œæµç¤ºä¾‹\r\n\r\n```python\r\n# ===================== å®Œæ•´åˆ†æå·¥ä½œæµï¼ˆå‡½æ•°å¼ç‰ˆæœ¬ï¼‰=====================\r\nimport json\r\nimport re\r\nfrom datetime import datetime\r\n\r\ndef complete_analysis_workflow(data_context: str) -> str:\r\n    \"\"\"\r\n    ç«¯åˆ°ç«¯çš„æ–‡æœ¬åˆ†æå·¥ä½œæµ - å‡½æ•°å¼ç‰ˆæœ¬\r\n    è¾“å…¥ï¼šçˆ¬è™«è·å–çš„æ–‡æœ¬æ•°æ®\r\n    è¾“å‡ºï¼šæ ‡å‡†åŒ–çš„åˆ†ææŠ¥å‘Š\r\n    \"\"\"\r\n    \r\n    # 1. å¹¶è¡Œæå–å„ç±»ä¿¡æ¯ï¼ˆä½¿ç”¨å‡½æ•°è€Œéç±»ï¼‰\r\n    price_info = extract_price_info(data_context)\r\n    dimensions = extract_dimensions(data_context)\r\n    categories = categorize_with_confidence(data_context)\r\n    \r\n    # 2. æ„å»ºç»“æœ\r\n    report = {\r\n        \"type\": \"comprehensive_analysis\",\r\n        \"title\": \"ç»¼åˆæ–‡æœ¬åˆ†ææŠ¥å‘Š\",\r\n        \"data\": {\r\n            \"ä»·æ ¼ä¿¡æ¯\": price_info,\r\n            \"è§„æ ¼å‚æ•°\": dimensions,\r\n            \"å†…å®¹åˆ†ç±»\": categories,\r\n            \"æ–‡æœ¬é•¿åº¦\": len(data_context),\r\n            \"å…³é”®å¥å­\": extract_key_sentences(data_context)\r\n        },\r\n        \"metadata\": {\r\n            \"åˆ†æå·¥å…·\": \"æ²™ç›’å†…ç½®åˆ†æå¥—ä»¶\",\r\n            \"åˆ†ææ—¶é—´\": datetime.now().isoformat(),\r\n            \"ç½®ä¿¡åº¦\": calculate_confidence(price_info, dimensions)\r\n        }\r\n    }\r\n    \r\n    return json.dumps(report, ensure_ascii=False, indent=2)\r\n\r\n# è¾…åŠ©å‡½æ•°\r\ndef extract_key_sentences(text: str, max_sentences: int = 3) -> list:\r\n    \"\"\"æå–å…³é”®å¥å­\"\"\"\r\n    # ç®€å•åˆ†å¥é€»è¾‘\r\n    sentences = []\r\n    current = \"\"\r\n    \r\n    for char in text:\r\n        current += char\r\n        if char in 'ã€‚ï¼ï¼Ÿ.!?':\r\n            sentence = current.strip()\r\n            if len(sentence) > 10:\r\n                sentences.append(sentence)\r\n            current = \"\"\r\n        \r\n        if len(sentences) >= max_sentences:\r\n            break\r\n    \r\n    # å¦‚æœæ²¡æ‰¾åˆ°è¶³å¤Ÿå¥å­ï¼ŒæŒ‰æ¢è¡Œåˆ†å‰²\r\n    if len(sentences) < max_sentences:\r\n        lines = [line.strip() for line in text.split('\\n') if len(line.strip()) > 10]\r\n        sentences.extend(lines[:max_sentences - len(sentences)])\r\n    \r\n    return sentences[:max_sentences]\r\n\r\ndef calculate_confidence(price_info: dict, dimensions: dict) -> str:\r\n    \"\"\"è®¡ç®—åˆ†æç½®ä¿¡åº¦\"\"\"\r\n    price_matches = price_info.get('price_matches', [])\r\n    has_dimensions = bool(dimensions)\r\n    \r\n    if price_matches and has_dimensions:\r\n        return \"é«˜\"\r\n    elif price_matches or has_dimensions:\r\n        return \"ä¸­\"\r\n    else:\r\n        return \"ä½\"\r\n\r\n# ä¸»æ‰§è¡Œé€»è¾‘\r\nif __name__ == \"__main__\":\r\n    # ç¤ºä¾‹æ–‡æœ¬\r\n    sample_text = \"\"\"\r\n    äº§å“ï¼šé«˜ç«¯æ™ºèƒ½æ‰‹è¡¨\r\n    ä»·æ ¼ï¼š$299.99\r\n    å°ºå¯¸ï¼šé«˜åº¦45mmï¼Œå®½åº¦38mm\r\n    æè´¨ï¼šä¸é”ˆé’¢è¡¨å£³ï¼Œè“å®çŸ³ç»ç’ƒ\r\n    åŠŸèƒ½ï¼šå¿ƒç‡ç›‘æµ‹ï¼ŒGPSå®šä½\r\n    \"\"\"\r\n    \r\n    result = complete_analysis_workflow(sample_text)\r\n    print(result)\r\n```\r\n\r\n---\r\n\r\n## âœ… éªŒè¯æµ‹è¯•\r\n\r\nè¿è¡Œä»¥ä¸‹ä»£ç éªŒè¯æ‚¨çš„åˆ†æå™¨ï¼š\r\n\r\n```python\r\n# æµ‹è¯•ç”¨ä¾‹ - å‡½æ•°å¼ç‰ˆæœ¬\r\nimport json\r\n\r\ntest_cases = [\r\n    (\"Jimmy Choo DIDI 45 ä»·æ ¼ $299.99 æè´¨çš®é© é«˜åº¦45mm\", \"äº§å“é¡µé¢åˆ†æ\"),\r\n    (\"iPhone 15 Pro Max å”®ä»· Â¥9999 é‡é‡ 221g å®½åº¦78mm\", \"ç”µå­äº§å“åˆ†æ\"),\r\n    (\"å®æœ¨é¤æ¡Œ å°ºå¯¸ 180x90cm ä»·æ ¼ â‚¬459 é«˜åº¦75cm\", \"å®¶å±…äº§å“åˆ†æ\")\r\n]\r\n\r\nfor test_text, expected_type in test_cases:\r\n    # ä½¿ç”¨å‡½æ•°å¼åˆ†æå™¨\r\n    dimensions = extract_dimensions(test_text)\r\n    categories = categorize_content(test_text)\r\n    \r\n    result = {\r\n        \"type\": \"test_result\",\r\n        \"test_case\": expected_type,\r\n        \"dimensions\": dimensions,\r\n        \"categories\": categories,\r\n        \"has_price\": \"$\" in test_text or \"Â¥\" in test_text or \"â‚¬\" in test_text\r\n    }\r\n    \r\n    print(f\"æµ‹è¯•: {expected_type}\")\r\n    print(f\"ç»“æœ: {json.dumps(result, ensure_ascii=False, indent=2)}\")\r\n    print(\"-\" * 50)\r\n```\r\n\r\n---\r\n\r\n## ğŸ“Œ æ€»ç»“è¦ç‚¹\r\n\r\n1. **å®‰å…¨ç¬¬ä¸€**ï¼šæ‰€æœ‰ä»£ç åœ¨æ²™ç›’ä¸­è¿è¡Œï¼Œæ— ç½‘ç»œæ— æ–‡ä»¶é£é™©\r\n2. **æ ¼å¼ä¸ºç‹**ï¼šè¾“å‡ºå¿…é¡»ç¬¦åˆæ ‡å‡†JSONç»“æ„ï¼ŒåŒ…å«typeå­—æ®µ\r\n3. **å‡½æ•°å¼ä¼˜å…ˆ**ï¼šé¿å…ç±»å®šä¹‰ï¼Œä½¿ç”¨çº¯å‡½æ•°è¿›è¡Œæ•°æ®æå–\r\n4. **æ¸è¿›æå–**ï¼šä»ç®€å•è§„åˆ™å¼€å§‹ï¼Œé€æ­¥å¢åŠ å¤æ‚æ€§\r\n5. **é”™è¯¯å¤„ç†**ï¼šæå–å¤±è´¥æ—¶æä¾›åˆç†é»˜è®¤å€¼\r\n6. **æ€§èƒ½æ„è¯†**ï¼šé¿å…å¤æ‚æ­£åˆ™å’Œæ— é™å¾ªç¯\r\n\r\n## ğŸ”„ ä»ç±»åˆ°å‡½æ•°çš„è½¬æ¢æŒ‡å—\r\n\r\n| åŸç±»å®šä¹‰ | è½¬æ¢åçš„å‡½æ•° | ä½¿ç”¨æ–¹å¼ |\r\n|---------|------------|---------|\r\n| `class Extractor:`<br>`def extract(self, text):` | `def extract_data(text):` | `result = extract_data(text)` |\r\n| `obj = Extractor()`<br>`obj.extract(text)` | ç›´æ¥è°ƒç”¨å‡½æ•° | `extract_data(text)` |\r\n| ç±»å±æ€§ï¼ˆ`self.config`ï¼‰ | å‡½æ•°å‚æ•°æˆ–å…¨å±€å¸¸é‡ | `def func(text, config={})` |\r\n| å¤šä¸ªç›¸å…³æ–¹æ³• | å¤šä¸ªç‹¬ç«‹å‡½æ•°æˆ–ä¸»å‡½æ•°è°ƒç”¨å­å‡½æ•° | `def main_func():`<br>`data1 = func1()`<br>`data2 = func2()` |\r\n\r\n## ğŸ¯ æœ€ç»ˆæ£€æŸ¥æ¸…å•\r\n\r\nåœ¨ç”Ÿæˆæ²™ç›’ä»£ç å‰ï¼Œè¯·ç¡®è®¤ï¼š\r\n- [ ] æ²¡æœ‰`class`å…³é”®å­—\r\n- [ ] æ‰€æœ‰åŠŸèƒ½éƒ½æ˜¯å‡½æ•°\r\n- [ ] è¾“å‡ºåŒ…å«`type`å­—æ®µ\r\n- [ ] ä½¿ç”¨`json.dumps()`è¾“å‡º\r\n- [ ] æ²¡æœ‰ç½‘ç»œè¯·æ±‚æˆ–æ–‡ä»¶ç³»ç»Ÿè®¿é—®\r\n- [ ] æ­£åˆ™è¡¨è¾¾å¼æœ‰é™åˆ¶ï¼ˆé¿å…ReDoSï¼‰\r\n\r\n---\r\n\n\n",
    "resources": {
      "references": {
        "matplotlib_cookbook.md": "# å¯è§†åŒ–å›¾è¡¨ç”ŸæˆæŒ‡å— (v3.0 - å®Œæ•´å·¥ä½œæµç‰ˆ)\n\n## ğŸš€ æ ¸å¿ƒä½¿ç”¨æ–¹æ³•\n\n**é‡è¦æç¤º**ï¼šæ‚¨åªéœ€è¦ä¸“æ³¨äºç»˜å›¾é€»è¾‘ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†å›¾åƒè¾“å‡ºã€‚\n\n### å¿…é¡»éµå¾ªçš„åŸåˆ™ï¼š\n1. **æ­£å¸¸å¯¼å…¥**ï¼š`import matplotlib.pyplot as plt`\n2. **æ­£å¸¸ç»˜å›¾**ï¼šä½¿ç”¨æ ‡å‡†çš„matplotlibå‡½æ•°\n3. **æ— éœ€ç¼–ç **ï¼šç¦æ­¢ä½¿ç”¨`io.BytesIO`ã€`base64`ç­‰æ‰‹åŠ¨ç¼–ç \n4. **æ¨èä½¿ç”¨**ï¼šåœ¨ä»£ç æœ«å°¾è°ƒç”¨`plt.show()`\n\n---\n\n## ğŸ“Š å¯ç›´æ¥ä½¿ç”¨çš„ä»£ç æ¨¡æ¿ï¼ˆä»æ•°æ®æ–‡ä»¶å¼€å§‹ï¼‰\n\n### æ¨¡æ¿1ï¼šè¯»å–ä¸Šä¼ æ–‡ä»¶å¹¶ç”Ÿæˆæ¡å½¢å›¾\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os\n\n# æ£€æŸ¥å¯ç”¨çš„æ•°æ®æ–‡ä»¶\ndata_dir = '/data'\nfiles = os.listdir(data_dir) if os.path.exists(data_dir) else []\nprint(f\"å¯ç”¨æ–‡ä»¶: {files}\")\n\nif files:\n    # é€‰æ‹©ç¬¬ä¸€ä¸ªCSVæ–‡ä»¶\n    csv_files = [f for f in files if f.endswith('.csv')]\n    if csv_files:\n        file_path = f'/data/{csv_files[0]}'\n        df = pd.read_csv(file_path)\n        print(f\"è¯»å–æ–‡ä»¶: {csv_files[0]}, å½¢çŠ¶: {df.shape}\")\n        print(df.head())\n        \n        # å‡è®¾æ•°æ®æœ‰categoryå’Œvalueåˆ—\n        if 'category' in df.columns and 'value' in df.columns:\n            plt.figure(figsize=(12, 7))\n            plt.bar(df['category'], df['value'], \n                   color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFE66D'])\n            plt.title(f'{csv_files[0]} - æ•°æ®åˆ†å¸ƒ')\n            plt.xlabel('ç±»åˆ«')\n            plt.ylabel('æ•°å€¼')\n            plt.xticks(rotation=45)\n            plt.grid(True, linestyle='--', alpha=0.3)\n            plt.tight_layout()\n            plt.show()\n        else:\n            print(\"æ•°æ®æ ¼å¼ä¸åŒ¹é…ï¼Œç”Ÿæˆç¤ºä¾‹å›¾è¡¨\")\n            generate_sample_chart()\n    else:\n        print(\"æ²¡æœ‰æ‰¾åˆ°CSVæ–‡ä»¶ï¼Œç”Ÿæˆç¤ºä¾‹å›¾è¡¨\")\n        generate_sample_chart()\nelse:\n    print(\"æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶ï¼Œç”Ÿæˆç¤ºä¾‹å›¾è¡¨\")\n    generate_sample_chart()\n\ndef generate_sample_chart():\n    \"\"\"ç”Ÿæˆç¤ºä¾‹å›¾è¡¨\"\"\"\n    import numpy as np\n    \n    # ç¤ºä¾‹æ•°æ®\n    categories = ['A', 'B', 'C', 'D', 'E']\n    values = np.random.randint(50, 200, 5)\n    \n    plt.figure(figsize=(10, 6))\n    bars = plt.bar(categories, values, \n                  color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFE66D'])\n    \n    # æ·»åŠ æ•°å€¼æ ‡ç­¾\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2., height + 3,\n                f'{int(height)}', ha='center', va='bottom')\n    \n    plt.title('ç¤ºä¾‹æ¡å½¢å›¾ - æ•°æ®åˆ†å¸ƒ')\n    plt.xlabel('äº§å“ç±»åˆ«')\n    plt.ylabel('é”€å”®é¢ (ä¸‡å…ƒ)')\n    plt.grid(True, axis='y', linestyle='--', alpha=0.6)\n    plt.tight_layout()\n    plt.show()\n```\n\n### æ¨¡æ¿2ï¼šæ—¶é—´åºåˆ—æŠ˜çº¿å›¾ï¼ˆé€‚åˆæœˆåº¦æŠ¥å‘Šï¼‰\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n# ç”Ÿæˆç¤ºä¾‹æ—¶é—´åºåˆ—æ•°æ®\ndef create_time_series_data():\n    dates = [datetime(2024, 1, 1) + timedelta(days=i*7) for i in range(12)]\n    values = [100, 120, 90, 150, 180, 200, 170, 220, 240, 210, 250, 280]\n    \n    df = pd.DataFrame({\n        'date': dates,\n        'value': values,\n        'target': [130] * 12\n    })\n    return df\n\ndf = create_time_series_data()\n\nplt.figure(figsize=(14, 8))\n\n# å®é™…å€¼æŠ˜çº¿\nplt.plot(df['date'], df['value'], \n         marker='o', \n         linestyle='-', \n         linewidth=3, \n         markersize=8,\n         color='#2E86AB',\n         label='å®é™…é”€å”®é¢')\n\n# ç›®æ ‡çº¿\nplt.plot(df['date'], df['target'], \n         linestyle='--', \n         linewidth=2,\n         color='#A23B72',\n         label='ç›®æ ‡çº¿')\n\n# å¡«å……åŒºåŸŸ\nplt.fill_between(df['date'], df['value'], df['target'], \n                 where=(df['value'] >= df['target']),\n                 alpha=0.3, color='#4ECDC4', label='è¶…é¢å®Œæˆ')\nplt.fill_between(df['date'], df['value'], df['target'],\n                 where=(df['value'] < df['target']),\n                 alpha=0.3, color='#FF6B6B', label='æœªè¾¾ç›®æ ‡')\n\nplt.title('2024å¹´é”€å”®é¢è¶‹åŠ¿åˆ†æ', fontsize=18, pad=20)\nplt.xlabel('æ—¥æœŸ', fontsize=14)\nplt.ylabel('é”€å”®é¢ (ä¸‡å…ƒ)', fontsize=14)\nplt.legend(fontsize=12, loc='upper left')\nplt.grid(True, alpha=0.3)\nplt.xticks(rotation=45)\nplt.tight_layout()\n\nplt.show()\n```\n\n### æ¨¡æ¿3ï¼šå¤šå­å›¾ä»ªè¡¨æ¿\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# åˆ›å»ºç¤ºä¾‹æ•°æ®\nnp.random.seed(42)\nn_points = 100\ndata = {\n    'x': np.random.randn(n_points),\n    'y': np.random.randn(n_points),\n    'category': np.random.choice(['A', 'B', 'C'], n_points),\n    'value': np.random.randint(1, 100, n_points)\n}\ndf = pd.DataFrame(data)\n\n# åˆ›å»º2x2çš„å­å›¾å¸ƒå±€\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('ç»¼åˆæ•°æ®åˆ†æä»ªè¡¨æ¿', fontsize=20, fontweight='bold')\n\n# 1. æ•£ç‚¹å›¾\nscatter = axes[0, 0].scatter(df['x'], df['y'], \n                             c=df['value'], \n                             s=df['value']*2,\n                             alpha=0.6,\n                             cmap='viridis')\naxes[0, 0].set_title('åˆ†å¸ƒæ•£ç‚¹å›¾ï¼ˆé¢œè‰²=æ•°å€¼ï¼Œå¤§å°=æ•°å€¼ï¼‰')\naxes[0, 0].set_xlabel('Xè½´')\naxes[0, 0].set_ylabel('Yè½´')\naxes[0, 0].grid(True, alpha=0.3)\nplt.colorbar(scatter, ax=axes[0, 0])\n\n# 2. ç®±çº¿å›¾\nbox_data = [df[df['category'] == cat]['value'].values for cat in ['A', 'B', 'C']]\nbp = axes[0, 1].boxplot(box_data, labels=['Aç±»', 'Bç±»', 'Cç±»'],\n                        patch_artist=True,\n                        boxprops=dict(facecolor='lightblue', color='darkblue'),\n                        medianprops=dict(color='red', linewidth=2))\naxes[0, 1].set_title('å„ç±»åˆ«æ•°æ®åˆ†å¸ƒ')\naxes[0, 1].set_ylabel('æ•°å€¼')\naxes[0, 1].grid(True, alpha=0.3, axis='y')\n\n# 3. é¥¼å›¾ï¼ˆç±»åˆ«å æ¯”ï¼‰\ncategory_counts = df['category'].value_counts()\naxes[1, 0].pie(category_counts.values, \n               labels=category_counts.index,\n               autopct='%1.1f%%',\n               colors=['#FF6B6B', '#4ECDC4', '#45B7D1'],\n               startangle=90,\n               explode=(0.05, 0, 0))\naxes[1, 0].set_title('ç±»åˆ«å æ¯”åˆ†å¸ƒ')\naxes[1, 0].axis('equal')  # ç¡®ä¿é¥¼å›¾æ˜¯åœ†å½¢\n\n# 4. ç›´æ–¹å›¾\naxes[1, 1].hist(df['value'], bins=20, \n                color='#96CEB4', \n                edgecolor='black',\n                alpha=0.7)\naxes[1, 1].axvline(df['value'].mean(), color='red', linestyle='--', linewidth=2)\naxes[1, 1].text(df['value'].mean()*1.05, axes[1, 1].get_ylim()[1]*0.9,\n               f'å‡å€¼: {df[\"value\"].mean():.1f}', \n               color='red', fontsize=10)\naxes[1, 1].set_title('æ•°å€¼åˆ†å¸ƒç›´æ–¹å›¾')\naxes[1, 1].set_xlabel('æ•°å€¼')\naxes[1, 1].set_ylabel('é¢‘æ•°')\naxes[1, 1].grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout(rect=[0, 0, 1, 0.96])  # ä¸ºæ ‡é¢˜ç•™å‡ºç©ºé—´\nplt.show()\n```\n\n---\n\n## ğŸ¨ å›¾è¡¨ç±»å‹é€‰æ‹©æŒ‡å—\n\n### æ ¹æ®åˆ†æç›®çš„é€‰æ‹©å›¾è¡¨ï¼š\n\n| åˆ†æç›®çš„ | æ¨èå›¾è¡¨ | ç¤ºä¾‹åœºæ™¯ |\n|---------|----------|----------|\n| **æ•°æ®æ¯”è¾ƒ** | æ¡å½¢å›¾ã€æŸ±çŠ¶å›¾ | äº§å“é”€å”®é¢å¯¹æ¯”ã€åœ°åŒºä¸šç»©æ’å |\n| **è¶‹åŠ¿åˆ†æ** | æŠ˜çº¿å›¾ã€é¢ç§¯å›¾ | æœˆåº¦é”€å”®è¶‹åŠ¿ã€ç”¨æˆ·å¢é•¿è¶‹åŠ¿ |\n| **åˆ†å¸ƒåˆ†æ** | ç›´æ–¹å›¾ã€ç®±çº¿å›¾ã€å¯†åº¦å›¾ | ç”¨æˆ·å¹´é¾„åˆ†å¸ƒã€æ”¶å…¥åˆ†å¸ƒ |\n| **æ¯”ä¾‹åˆ†æ** | é¥¼å›¾ã€ç¯å½¢å›¾ã€æ—­æ—¥å›¾ | å¸‚åœºä»½é¢ã€é¢„ç®—åˆ†é… |\n| **å…³ç³»åˆ†æ** | æ•£ç‚¹å›¾ã€æ°”æ³¡å›¾ã€çƒ­åŠ›å›¾ | å¹¿å‘ŠæŠ•å…¥ä¸é”€å”®å…³ç³»ã€ç›¸å…³æ€§åˆ†æ |\n| **ç»„æˆåˆ†æ** | å †å æ¡å½¢å›¾ã€ç€‘å¸ƒå›¾ | æ”¶å…¥æ„æˆåˆ†æã€æˆæœ¬ç»“æ„ |\n| **åœ°ç†åˆ†æ** | åœ°å›¾ã€ç­‰å€¼çº¿å›¾ | åœ°åŒºåˆ†å¸ƒã€äººå£å¯†åº¦ |\n\n---\n\n## ğŸ—ï¸ æµç¨‹å›¾ä¸æ¶æ„å›¾ç”ŸæˆæŒ‡å—\n\n### Graphviz ä¸“ä¸šæµç¨‹å›¾ï¼ˆä¿®æ­£ç‰ˆï¼‰\n\n#### åŸºç¡€æµç¨‹å›¾æ¨¡æ¿ - å¿…é¡»èµ‹å€¼ç»™å˜é‡å¹¶è°ƒç”¨\n```python\nfrom graphviz import Digraph\n\n# ğŸ¯ å…³é”®ï¼š1. åˆ›å»ºå›¾è¡¨å¯¹è±¡ 2. èµ‹å€¼ç»™å˜é‡ 3. ç¡®ä¿åœ¨å…¨å±€ä½œç”¨åŸŸ\ndef create_basic_flowchart():\n    dot = Digraph('BusinessProcess', comment='ä¸šåŠ¡æµç¨‹')\n    dot.attr(rankdir='LR', size='10,8')\n    \n    # è®¾ç½®èŠ‚ç‚¹æ ·å¼\n    dot.node('start', 'å¼€å§‹', shape='ellipse', color='green', style='filled', fillcolor='lightgreen')\n    dot.node('input', 'è¾“å…¥æ•°æ®', shape='box', style='filled', fillcolor='lightblue')\n    dot.node('process', 'æ•°æ®å¤„ç†', shape='box', style='filled', fillcolor='lightblue')\n    dot.node('analyze', 'åˆ†æç»“æœ', shape='box', style='filled', fillcolor='lightblue')\n    dot.node('decision', 'æ˜¯å¦é€šè¿‡ï¼Ÿ', shape='diamond', color='blue', style='filled', fillcolor='lightyellow')\n    dot.node('approve', 'å®¡æ‰¹é€šè¿‡', shape='box', style='filled', fillcolor='lightgreen')\n    dot.node('reject', 'è¿”å›ä¿®æ”¹', shape='box', style='filled', fillcolor='lightcoral')\n    dot.node('end', 'ç»“æŸ', shape='ellipse', color='red', style='filled', fillcolor='lightcoral')\n    \n    # æ·»åŠ è¾¹\n    dot.edge('start', 'input', label='å¯åŠ¨')\n    dot.edge('input', 'process', label='æ•°æ®éªŒè¯')\n    dot.edge('process', 'analyze', label='æ‰§è¡Œåˆ†æ')\n    dot.edge('analyze', 'decision', label='ç”ŸæˆæŠ¥å‘Š')\n    dot.edge('decision', 'approve', label='æ˜¯', color='green')\n    dot.edge('decision', 'reject', label='å¦', color='red')\n    dot.edge('approve', 'end', label='å®Œæˆ')\n    dot.edge('reject', 'process', label='é‡æ–°å¤„ç†', color='orange', style='dashed')\n    \n    return dot\n\n# ğŸ¯ å…³é”®ï¼šå°†å›¾è¡¨å¯¹è±¡èµ‹å€¼ç»™å…¨å±€å˜é‡\nflowchart = create_basic_flowchart()\n\n# ğŸ¯ å…³é”®ï¼šå›¾è¡¨å¯¹è±¡å¿…é¡»åœ¨å…¨å±€ä½œç”¨åŸŸä¸­å­˜åœ¨\n# ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹å¹¶æ•è·åä¸º 'flowchart' çš„Digraphå¯¹è±¡\n```\n\n#### ç³»ç»Ÿæ¶æ„å›¾æ¨¡æ¿\n```python\nfrom graphviz import Digraph\n\ndef create_system_architecture():\n    dot = Digraph('SystemArchitecture', format='png')\n    dot.attr(rankdir='TB', size='14,10', compound='true')\n    \n    # å‰ç«¯å±‚é›†ç¾¤\n    with dot.subgraph(name='cluster_frontend') as c:\n        c.attr(label='å‰ç«¯å±‚', style='filled', color='lightgrey', fontsize='16')\n        c.node('web_app', 'Webåº”ç”¨', shape='box3d', style='filled', fillcolor='lightblue')\n        c.node('mobile_app', 'ç§»åŠ¨ç«¯', shape='box3d', style='filled', fillcolor='lightblue')\n        c.node('api_gateway', 'APIç½‘å…³', shape='pentagon', style='filled', fillcolor='lightyellow')\n        \n    # åç«¯æœåŠ¡é›†ç¾¤\n    with dot.subgraph(name='cluster_backend') as c:\n        c.attr(label='åç«¯æœåŠ¡å±‚', style='filled', color='lightblue', fontsize='16')\n        c.node('auth_service', 'è®¤è¯æœåŠ¡', shape='component', style='filled', fillcolor='lightgreen')\n        c.node('user_service', 'ç”¨æˆ·æœåŠ¡', shape='component', style='filled', fillcolor='lightgreen')\n        c.node('product_service', 'äº§å“æœåŠ¡', shape='component', style='filled', fillcolor='lightgreen')\n        c.node('order_service', 'è®¢å•æœåŠ¡', shape='component', style='filled', fillcolor='lightgreen')\n        \n    # æ•°æ®å±‚é›†ç¾¤\n    with dot.subgraph(name='cluster_data') as c:\n        c.attr(label='æ•°æ®å­˜å‚¨å±‚', style='filled', color='lightgreen', fontsize='16')\n        c.node('main_db', 'ä¸»æ•°æ®åº“\\n(PostgreSQL)', shape='cylinder', style='filled', fillcolor='lightyellow')\n        c.node('cache', 'ç¼“å­˜\\n(Redis)', shape='cylinder', style='filled', fillcolor='lightcoral')\n        c.node('search_engine', 'æœç´¢å¼•æ“\\n(Elasticsearch)', shape='cylinder', style='filled', fillcolor='lightskyblue')\n        \n    # è¿æ¥å…³ç³»\n    dot.edge('web_app', 'api_gateway', label='HTTPS')\n    dot.edge('mobile_app', 'api_gateway', label='REST API')\n    dot.edge('api_gateway', 'auth_service', label='éªŒè¯è¯·æ±‚')\n    dot.edge('api_gateway', 'user_service', label='ç”¨æˆ·æ•°æ®')\n    dot.edge('api_gateway', 'product_service', label='äº§å“æ•°æ®')\n    dot.edge('api_gateway', 'order_service', label='è®¢å•å¤„ç†')\n    \n    dot.edge('user_service', 'main_db', label='CRUD')\n    dot.edge('product_service', 'main_db', label='æŸ¥è¯¢')\n    dot.edge('order_service', 'main_db', label='äº‹åŠ¡')\n    dot.edge('user_service', 'cache', label='ä¼šè¯ç¼“å­˜')\n    dot.edge('product_service', 'search_engine', label='å…¨æ–‡æœç´¢')\n    \n    return dot\n\n# åˆ›å»ºå¹¶èµ‹å€¼ç»™å…¨å±€å˜é‡\nsystem_arch = create_system_architecture()\n```\n\n### NetworkX ç½‘ç»œå…³ç³»å›¾ï¼ˆé€šè¿‡Matplotlibæ˜¾ç¤ºï¼‰\n\n#### å®Œæ•´çš„æ•°æ®æµæ°´çº¿ç½‘ç»œå›¾\n```python\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef create_data_pipeline_diagram():\n    # åˆ›å»ºæœ‰å‘å›¾\n    G = nx.DiGraph()\n    \n    # æ·»åŠ èŠ‚ç‚¹ï¼ˆæ•°æ®æµæ°´çº¿å„é˜¶æ®µï¼‰\n    nodes = {\n        'æ•°æ®æº': {'type': 'source', 'color': 'lightgreen'},\n        'æ•°æ®é‡‡é›†': {'type': 'process', 'color': 'lightblue'},\n        'æ•°æ®æ¸…æ´—': {'type': 'process', 'color': 'lightblue'},\n        'æ•°æ®è½¬æ¢': {'type': 'process', 'color': 'lightblue'},\n        'æ•°æ®å­˜å‚¨': {'type': 'storage', 'color': 'lightyellow'},\n        'æ•°æ®åˆ†æ': {'type': 'analysis', 'color': 'lightcoral'},\n        'æ•°æ®å¯è§†åŒ–': {'type': 'visualization', 'color': 'lightskyblue'},\n        'ä¸šåŠ¡å†³ç­–': {'type': 'decision', 'color': 'lightpink'}\n    }\n    \n    for node, attrs in nodes.items():\n        G.add_node(node, **attrs)\n    \n    # æ·»åŠ è¾¹ï¼ˆæ•°æ®æµå‘ï¼‰\n    edges = [\n        ('æ•°æ®æº', 'æ•°æ®é‡‡é›†', 'åŸå§‹æ•°æ®'),\n        ('æ•°æ®é‡‡é›†', 'æ•°æ®æ¸…æ´—', 'é¢„å¤„ç†'),\n        ('æ•°æ®æ¸…æ´—', 'æ•°æ®è½¬æ¢', 'æ ¼å¼åŒ–'),\n        ('æ•°æ®è½¬æ¢', 'æ•°æ®å­˜å‚¨', 'æŒä¹…åŒ–'),\n        ('æ•°æ®å­˜å‚¨', 'æ•°æ®åˆ†æ', 'æŸ¥è¯¢'),\n        ('æ•°æ®åˆ†æ', 'æ•°æ®å¯è§†åŒ–', 'ç»“æœ'),\n        ('æ•°æ®å¯è§†åŒ–', 'ä¸šåŠ¡å†³ç­–', 'æ´å¯Ÿ')\n    ]\n    \n    for src, dst, label in edges:\n        G.add_edge(src, dst, label=label)\n    \n    # å¸ƒå±€ç®—æ³•\n    pos = nx.spring_layout(G, k=2, iterations=50, seed=42)\n    \n    # ç»˜å›¾\n    plt.figure(figsize=(16, 10))\n    \n    # æŒ‰ç±»å‹ç€è‰²èŠ‚ç‚¹\n    node_colors = [nodes[node]['color'] for node in G.nodes()]\n    node_sizes = [3000 if nodes[node]['type'] in ['source', 'decision'] else 2000 for node in G.nodes()]\n    \n    nx.draw_networkx_nodes(G, pos, \n                          node_color=node_colors,\n                          node_size=node_sizes,\n                          edgecolors='black',\n                          linewidths=2,\n                          alpha=0.9)\n    \n    # ç»˜åˆ¶è¾¹\n    nx.draw_networkx_edges(G, pos, \n                          edge_color='gray',\n                          arrows=True,\n                          arrowsize=20,\n                          width=2,\n                          alpha=0.7,\n                          connectionstyle=\"arc3,rad=0.1\")\n    \n    # ç»˜åˆ¶èŠ‚ç‚¹æ ‡ç­¾\n    nx.draw_networkx_labels(G, pos, \n                           font_size=12,\n                           font_weight='bold',\n                           font_family='WenQuanYi Micro Hei')\n    \n    # ç»˜åˆ¶è¾¹æ ‡ç­¾\n    edge_labels = nx.get_edge_attributes(G, 'label')\n    nx.draw_networkx_edge_labels(G, pos, \n                                edge_labels=edge_labels,\n                                font_size=10,\n                                label_pos=0.5,\n                                font_family='WenQuanYi Micro Hei')\n    \n    # è®¾ç½®æ ‡é¢˜å’Œç½‘æ ¼\n    plt.title('æ•°æ®æµæ°´çº¿æ¶æ„å›¾', fontsize=20, pad=30, fontweight='bold')\n    plt.axis('off')\n    plt.tight_layout()\n    \n    # ğŸ¯ å…³é”®ï¼šè§¦å‘Matplotlibè‡ªåŠ¨æ•è·\n    plt.show()\n\n# è°ƒç”¨å‡½æ•°ç”Ÿæˆå›¾è¡¨\ncreate_data_pipeline_diagram()\n```\n\n---\n\n## âš™ï¸ æ ·å¼é…ç½®ä¸å­—ä½“è®¾ç½®ï¼ˆé‡è¦ï¼‰\n\n### ä¸­æ–‡å­—ä½“è‡ªåŠ¨é…ç½®ï¼ˆç³»ç»Ÿå·²å¤„ç†ï¼‰\n```python\nimport matplotlib.pyplot as plt\n\n# ç³»ç»Ÿå·²è‡ªåŠ¨é…ç½®ä¸­æ–‡å­—ä½“ï¼Œæ— éœ€æ‰‹åŠ¨è®¾ç½®\n# å¦‚æœé‡åˆ°å­—ä½“é—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹é…ç½®ï¼š\n\nplt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei', 'WenQuanYi Zen Hei']\nplt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜\n\n# å¯é€‰ï¼šè®¾ç½®å…¨å±€æ ·å¼\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams.update({\n    'figure.figsize': (12, 8),\n    'font.size': 12,\n    'axes.titlesize': 16,\n    'axes.labelsize': 14,\n    'xtick.labelsize': 11,\n    'ytick.labelsize': 11,\n    'legend.fontsize': 11,\n    'grid.alpha': 0.3\n})\n\nprint(\"å­—ä½“é…ç½®å®Œæˆï¼Œå¯ä»¥å¼€å§‹ç»˜å›¾\")\n```\n\n---\n\n## ğŸ“ˆ è¿›é˜¶åŠŸèƒ½ï¼šäº¤äº’å¼å›¾è¡¨ä¸åŠ¨ç”»\n\n### ç®€å•åŠ¨ç”»ç¤ºä¾‹\n```python\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport numpy as np\n\nfig, ax = plt.subplots(figsize=(10, 6))\nx = np.linspace(0, 2*np.pi, 100)\nline, = ax.plot(x, np.sin(x))\n\ndef animate(i):\n    line.set_ydata(np.sin(x + i/10.0))\n    return line,\n\nani = animation.FuncAnimation(fig, animate, interval=50, blit=True)\nplt.title('æ­£å¼¦æ³¢åŠ¨ç”»æ¼”ç¤º')\nplt.xlabel('Xè½´')\nplt.ylabel('Yè½´')\nplt.grid(True)\nplt.show()\n```\n\n---\n\n## âš ï¸ é‡è¦æ³¨æ„äº‹é¡¹\n\n### âœ… å¿…é¡»åŒ…å«ï¼š\n1. `import matplotlib.pyplot as plt`\n2. æœ‰æ„ä¹‰çš„å›¾è¡¨æ ‡é¢˜`plt.title()`\n3. `plt.show()`ï¼ˆMatplotlibå’ŒNetworkXå¿…é¡»è°ƒç”¨ï¼‰\n\n### âŒ ç¦æ­¢æ“ä½œï¼š\n1. ä¸è¦ä½¿ç”¨`base64.b64encode()`æ‰‹åŠ¨ç¼–ç å›¾ç‰‡\n2. ä¸è¦åˆ›å»º`io.BytesIO()`å¯¹è±¡\n3. ä¸è¦æ‰‹åŠ¨æ„å»ºJSONè¾“å‡ºï¼ˆç³»ç»Ÿè‡ªåŠ¨å¤„ç†ï¼‰\n4. **Graphvizå›¾è¡¨å¿…é¡»èµ‹å€¼ç»™å…¨å±€å˜é‡**\n\n### ğŸ”§ æœ€ä½³å®è·µï¼š\n1. **æ–‡ä»¶è¯»å–ä¼˜å…ˆ**ï¼šå…ˆä»`/data`ç›®å½•è¯»å–ç”¨æˆ·ä¸Šä¼ çš„æ–‡ä»¶\n2. **æä¾›å¤‡ç”¨æ–¹æ¡ˆ**ï¼šå¦‚æœæ²¡æœ‰æ–‡ä»¶ï¼Œç”Ÿæˆç¤ºä¾‹å›¾è¡¨\n3. **æ¸…æ™°çš„æ ‡ç­¾**ï¼šä¸ºå›¾è¡¨æ·»åŠ æ¸…æ™°çš„æ ‡é¢˜å’Œåæ ‡è½´æ ‡ç­¾\n4. **åˆç†çš„å°ºå¯¸**ï¼š`figsize`å»ºè®®(12, 8)æˆ–(10, 6)\n5. **å¸ƒå±€ä¼˜åŒ–**ï¼šä½¿ç”¨`plt.tight_layout()`é˜²æ­¢æ ‡ç­¾é‡å \n\n---\n\n## ğŸ¯ ç°åœ¨å®Œå…¨åŒ¹é…åç«¯ï¼\n\n### ç»Ÿä¸€çš„è‡ªåŠ¨æ•è·æœºåˆ¶ï¼š\n\n| å›¾è¡¨ç±»å‹ | æ­£ç¡®ä½¿ç”¨æ–¹æ³• | ç¤ºä¾‹ä»£ç  |\n|---------|-------------|----------|\n| **Matplotlib** | `plt.show()` | `plt.plot(); plt.show()` |\n| **Graphviz** | åˆ›å»ºå¹¶èµ‹å€¼ç»™å…¨å±€å˜é‡ | `dot = Digraph(); ...` |\n| **NetworkX** | `plt.show()` | `nx.draw(); plt.show()` |\n\n### ç»ˆæå·¥ä½œæµæ¨¡æ¿ï¼š\n```python\n# 1. æ£€æŸ¥æ•°æ®æ–‡ä»¶\nimport os, pandas as pd\nfiles = os.listdir('/data') if os.path.exists('/data') else []\n\n# 2. è¯»å–æ•°æ®ï¼ˆå¦‚æœæœ‰æ–‡ä»¶ï¼‰\nif files and 'data.csv' in files:\n    df = pd.read_csv('/data/data.csv')\n    # ä½¿ç”¨çœŸå®æ•°æ®ç»˜å›¾\nelse:\n    # ç”Ÿæˆç¤ºä¾‹æ•°æ®ç»˜å›¾\n    pass\n\n# 3. ç”Ÿæˆå›¾è¡¨ï¼ˆé€‰æ‹©ä¸€ç§ç±»å‹ï¼‰\n# Matplotlib: plt.plot(); plt.show()\n# Graphviz: dot = Digraph(); (è‡ªåŠ¨æ•è·)\n# NetworkX: nx.draw(); plt.show()\n\n# 4. å›¾è¡¨ä¼šè¢«è‡ªåŠ¨æ•è·å¹¶æ˜¾ç¤ºç»™ç”¨æˆ·\n```\n\n### æ•…éšœæ’é™¤ï¼š\n1. **å›¾è¡¨æœªæ˜¾ç¤º**ï¼š\n   - æ£€æŸ¥æ˜¯å¦è°ƒç”¨äº†`plt.show()`\n   - æ£€æŸ¥Graphvizå¯¹è±¡æ˜¯å¦èµ‹å€¼ç»™å…¨å±€å˜é‡\n   - æŸ¥çœ‹ç³»ç»Ÿé”™è¯¯è¾“å‡º\n\n2. **ä¸­æ–‡ä¹±ç **ï¼š\n   - ç³»ç»Ÿå·²å†…ç½®å­—ä½“ä¿®å¤\n   - å¯æ‰‹åŠ¨è®¾ç½®å­—ä½“é…ç½®\n\n3. **æ–‡ä»¶è¯»å–å¤±è´¥**ï¼š\n   - ç¡®ä¿æ–‡ä»¶å·²é€šè¿‡ä¸Šä¼ åŠŸèƒ½ä¸Šä¼ \n   - æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼š`/data/æ–‡ä»¶å`\n\n**è®°ä½**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨æ•è·æ‰€æœ‰å›¾è¡¨å¹¶è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼ï¼Œæ‚¨åªéœ€è¦ä¸“æ³¨äºç»˜å›¾é€»è¾‘å’Œæ•°æ®åˆ†æï¼\n",
        "ml_workflow.md": "# æœºå™¨å­¦ä¹ å·¥ä½œæµæŒ‡å— (v2.3)\r\n\r\n## ğŸ¯ å·¥å…·æ¦‚è¿°\r\n**åŠŸèƒ½**ï¼šæœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒã€è¯„ä¼°ã€ç»Ÿè®¡åˆ†æå’Œå¯è§†åŒ–  \r\n**è¾“å‡ºåŸåˆ™**ï¼šç›´æ¥æ‰“å°ç»“æœï¼Œç³»ç»Ÿè‡ªåŠ¨å¤„ç†è¾“å‡ºæ ¼å¼  \r\n\r\n**æ–°å¢åŠŸèƒ½**ï¼š\r\n- âœ… **XGBoost 1.7.6**ï¼šé«˜æ€§èƒ½æ¢¯åº¦æå‡æ ‘æ¨¡å‹\r\n- âœ… **pmdarima 2.0.4**ï¼šè‡ªåŠ¨åŒ–ARIMAæ—¶é—´åºåˆ—å»ºæ¨¡\r\n- âœ… å¢å¼ºçš„æ—¶é—´åºåˆ—åˆ†æèƒ½åŠ›\r\n- âœ… éçº¿æ€§æ¨¡å‹ä¸çº¿æ€§æ¨¡å‹çš„å¯¹æ¯”åˆ†æ\r\n\r\n## ğŸ“Š åŸºç¡€æœºå™¨å­¦ä¹ æ¨¡æ¿\r\n\r\n### æ•°æ®å‡†å¤‡ä¸é¢„å¤„ç†\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\n\r\ndef prepare_ml_data():\r\n    \"\"\"æœºå™¨å­¦ä¹ æ•°æ®å‡†å¤‡ç¤ºä¾‹\"\"\"\r\n    \r\n    # åˆ›å»ºç¤ºä¾‹æ•°æ®é›†\r\n    np.random.seed(42)\r\n    n_samples = 1000\r\n    \r\n    # å›å½’é—®é¢˜æ•°æ®\r\n    X_reg = np.random.normal(0, 1, (n_samples, 5))\r\n    y_reg = 2 * X_reg[:, 0] + 1.5 * X_reg[:, 1] - X_reg[:, 2] + np.random.normal(0, 0.5, n_samples)\r\n    \r\n    # åˆ†ç±»é—®é¢˜æ•°æ®\r\n    X_clf = np.random.normal(0, 1, (n_samples, 4))\r\n    y_clf = (X_clf[:, 0] + X_clf[:, 1] > 0).astype(int)\r\n    \r\n    print(\"=== æ•°æ®å‡†å¤‡å®Œæˆ ===\")\r\n    print(f\"æ ·æœ¬æ•°é‡: {n_samples}\")\r\n    print(f\"å›å½’ç‰¹å¾ç»´åº¦: {X_reg.shape[1]}\")\r\n    print(f\"åˆ†ç±»ç‰¹å¾ç»´åº¦: {X_clf.shape[1]}\")\r\n    print(f\"åˆ†ç±»æ ‡ç­¾åˆ†å¸ƒ: {np.unique(y_clf, return_counts=True)}\")\r\n    \r\n    return X_reg, y_reg, X_clf, y_clf\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\n# X_reg, y_reg, X_clf, y_clf = prepare_ml_data()\r\n```\r\n\r\n### æ ‡å‡†æœºå™¨å­¦ä¹ å·¥ä½œæµ\r\n```python\r\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\r\nfrom sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report\r\nfrom sklearn.model_selection import cross_val_score\r\n\r\ndef standard_ml_pipeline(X, y, problem_type='regression'):\r\n    \"\"\"æ ‡å‡†æœºå™¨å­¦ä¹ æµç¨‹\"\"\"\r\n    \r\n    print(f\"=== å¼€å§‹ {problem_type} æ¨¡å‹è®­ç»ƒ ===\")\r\n    \r\n    # æ•°æ®åˆ†å‰²\r\n    X_train, X_test, y_train, y_test = train_test_split(\r\n        X, y, test_size=0.2, random_state=42,\r\n        stratify=y if problem_type == 'classification' else None\r\n    )\r\n    \r\n    print(f\"è®­ç»ƒé›†å¤§å°: {X_train.shape}\")\r\n    print(f\"æµ‹è¯•é›†å¤§å°: {X_test.shape}\")\r\n    \r\n    # ç‰¹å¾æ ‡å‡†åŒ–\r\n    scaler = StandardScaler()\r\n    X_train_scaled = scaler.fit_transform(X_train)\r\n    X_test_scaled = scaler.transform(X_test)\r\n    \r\n    # é€‰æ‹©æ¨¡å‹\r\n    if problem_type == 'regression':\r\n        model = RandomForestRegressor(n_estimators=100, random_state=42)\r\n    else:\r\n        model = RandomForestClassifier(n_estimators=100, random_state=42)\r\n    \r\n    # è®­ç»ƒæ¨¡å‹\r\n    model.fit(X_train_scaled, y_train)\r\n    \r\n    # é¢„æµ‹\r\n    y_pred = model.predict(X_test_scaled)\r\n    \r\n    # æ¨¡å‹è¯„ä¼°\r\n    if problem_type == 'regression':\r\n        mse = mean_squared_error(y_test, y_pred)\r\n        rmse = np.sqrt(mse)\r\n        r2 = r2_score(y_test, y_pred)\r\n        \r\n        print(f\"å›å½’æ¨¡å‹æ€§èƒ½:\")\r\n        print(f\"  MSE: {mse:.4f}\")\r\n        print(f\"  RMSE: {rmse:.4f}\")\r\n        print(f\"  RÂ²: {r2:.4f}\")\r\n        \r\n        metrics = {'mse': mse, 'rmse': rmse, 'r2': r2}\r\n    else:\r\n        accuracy = accuracy_score(y_test, y_pred)\r\n        print(f\"åˆ†ç±»æ¨¡å‹æ€§èƒ½:\")\r\n        print(f\"  å‡†ç¡®ç‡: {accuracy:.4f}\")\r\n        print(\"\\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:\")\r\n        print(classification_report(y_test, y_pred))\r\n        \r\n        metrics = {'accuracy': accuracy}\r\n    \r\n    # äº¤å‰éªŒè¯\r\n    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, \r\n                               scoring='r2' if problem_type == 'regression' else 'accuracy')\r\n    print(f\"äº¤å‰éªŒè¯å¹³å‡å¾—åˆ†: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})\")\r\n    \r\n    return {\r\n        'model': model,\r\n        'metrics': metrics,\r\n        'X_test': X_test,\r\n        'y_test': y_test,\r\n        'y_pred': y_pred,\r\n        'cv_scores': cv_scores\r\n    }\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\n# X_reg, y_reg, X_clf, y_clf = prepare_ml_data()\r\n# regression_results = standard_ml_pipeline(X_reg, y_reg, 'regression')\r\n# classification_results = standard_ml_pipeline(X_clf, y_clf, 'classification')\r\n```\r\n\r\n## ğŸ“ˆ å›å½’åˆ†æå®Œæ•´å·¥ä½œæµ\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.ensemble import RandomForestRegressor\r\nfrom sklearn.metrics import mean_squared_error, r2_score\r\n\r\ndef complete_regression_analysis():\r\n    \"\"\"å®Œæ•´çš„å›å½’åˆ†æå·¥ä½œæµ\"\"\"\r\n    \r\n    print(\"=== å¼€å§‹å›å½’åˆ†æ ===\")\r\n    \r\n    # 1. æ•°æ®ç”Ÿæˆ\r\n    np.random.seed(42)\r\n    n_samples = 500\r\n    \r\n    # åˆ›å»ºæœ‰æ„ä¹‰çš„ç‰¹å¾\r\n    feature1 = np.random.normal(50, 15, n_samples)  # å¹´é¾„\r\n    feature2 = np.random.normal(100, 25, n_samples) # æ”¶å…¥\r\n    feature3 = np.random.normal(10, 3, n_samples)   # æ•™è‚²å¹´é™\r\n    feature4 = np.random.normal(0, 1, n_samples)    # å™ªå£°ç‰¹å¾\r\n    \r\n    # åˆ›å»ºç›®æ ‡å˜é‡ï¼ˆæ¨¡æ‹Ÿæˆ¿ä»·ï¼‰\r\n    target = (50 * feature1 + 80 * feature2 + 5000 * feature3 + \r\n              10 * feature1 * feature3 + np.random.normal(0, 10000, n_samples))\r\n    \r\n    df = pd.DataFrame({\r\n        'å¹´é¾„': feature1,\r\n        'æ”¶å…¥': feature2,\r\n        'æ•™è‚²å¹´é™': feature3,\r\n        'å™ªå£°ç‰¹å¾': feature4,\r\n        'æˆ¿ä»·': target\r\n    })\r\n    \r\n    print(\"æ•°æ®åŸºæœ¬ä¿¡æ¯:\")\r\n    print(f\"æ•°æ®é›†å½¢çŠ¶: {df.shape}\")\r\n    print(f\"ç‰¹å¾åˆ—è¡¨: {list(df.columns[:-1])}\")\r\n    print(f\"ç›®æ ‡å˜é‡: {df.columns[-1]}\")\r\n    \r\n    # 2. æ•°æ®æ¢ç´¢\r\n    print(\"\\n=== æ•°æ®æ¢ç´¢ ===\")\r\n    print(\"æ•°å€¼ç‰¹å¾ç»Ÿè®¡:\")\r\n    print(df.describe())\r\n    \r\n    # ç›¸å…³æ€§åˆ†æ\r\n    correlation = df.corr()['æˆ¿ä»·'].sort_values(ascending=False)\r\n    print(\"\\nç‰¹å¾ä¸ç›®æ ‡å˜é‡ç›¸å…³æ€§:\")\r\n    for feature, corr in correlation.items():\r\n        if feature != 'æˆ¿ä»·':\r\n            print(f\"  {feature}: {corr:.3f}\")\r\n    \r\n    # 3. æ¨¡å‹è®­ç»ƒ\r\n    X = df.drop('æˆ¿ä»·', axis=1)\r\n    y = df['æˆ¿ä»·']\r\n    \r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\r\n    \r\n    model = RandomForestRegressor(n_estimators=100, random_state=42)\r\n    model.fit(X_train, y_train)\r\n    \r\n    y_pred = model.predict(X_test)\r\n    \r\n    # 4. æ¨¡å‹è¯„ä¼°\r\n    mse = mean_squared_error(y_test, y_pred)\r\n    rmse = np.sqrt(mse)\r\n    r2 = r2_score(y_test, y_pred)\r\n    \r\n    print(f\"\\n=== æ¨¡å‹æ€§èƒ½ ===\")\r\n    print(f\"å‡æ–¹è¯¯å·® (MSE): {mse:,.2f}\")\r\n    print(f\"å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse:,.2f}\")\r\n    print(f\"å†³å®šç³»æ•° (RÂ²): {r2:.4f}\")\r\n    \r\n    # 5. ç‰¹å¾é‡è¦æ€§\r\n    feature_importance = pd.DataFrame({\r\n        'ç‰¹å¾': X.columns,\r\n        'é‡è¦æ€§': model.feature_importances_\r\n    }).sort_values('é‡è¦æ€§', ascending=False)\r\n    \r\n    print(f\"\\n=== ç‰¹å¾é‡è¦æ€§ ===\")\r\n    for _, row in feature_importance.iterrows():\r\n        print(f\"  {row['ç‰¹å¾']}: {row['é‡è¦æ€§']:.4f}\")\r\n    \r\n    # 6. å¯è§†åŒ–åˆ†æ\r\n    plt.figure(figsize=(15, 10))\r\n    \r\n    # å®é™…å€¼ vs é¢„æµ‹å€¼\r\n    plt.subplot(2, 3, 1)\r\n    plt.scatter(y_test, y_pred, alpha=0.6)\r\n    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\r\n    plt.xlabel('å®é™…å€¼')\r\n    plt.ylabel('é¢„æµ‹å€¼')\r\n    plt.title(f'é¢„æµ‹æ•ˆæœ (RÂ² = {r2:.3f})')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # æ®‹å·®åˆ†æ\r\n    plt.subplot(2, 3, 2)\r\n    residuals = y_test - y_pred\r\n    plt.scatter(y_pred, residuals, alpha=0.6)\r\n    plt.axhline(y=0, color='r', linestyle='--')\r\n    plt.xlabel('é¢„æµ‹å€¼')\r\n    plt.ylabel('æ®‹å·®')\r\n    plt.title('æ®‹å·®åˆ†æ')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–\r\n    plt.subplot(2, 3, 3)\r\n    top_features = feature_importance.head(5)\r\n    plt.barh(top_features['ç‰¹å¾'], top_features['é‡è¦æ€§'])\r\n    plt.xlabel('é‡è¦æ€§')\r\n    plt.title('Top 5 ç‰¹å¾é‡è¦æ€§')\r\n    plt.gca().invert_yaxis()\r\n    \r\n    # è¯¯å·®åˆ†å¸ƒ\r\n    plt.subplot(2, 3, 4)\r\n    plt.hist(residuals, bins=30, alpha=0.7, edgecolor='black')\r\n    plt.xlabel('æ®‹å·®')\r\n    plt.ylabel('é¢‘æ•°')\r\n    plt.title('è¯¯å·®åˆ†å¸ƒ')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # ç›¸å¯¹è¯¯å·®\r\n    plt.subplot(2, 3, 5)\r\n    relative_error = np.abs(residuals / y_test) * 100\r\n    plt.hist(relative_error, bins=30, alpha=0.7, edgecolor='black')\r\n    plt.xlabel('ç›¸å¯¹è¯¯å·® (%)')\r\n    plt.ylabel('é¢‘æ•°')\r\n    plt.title('ç›¸å¯¹è¯¯å·®åˆ†å¸ƒ')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # é¢„æµ‹è¯¯å·®ç®±çº¿å›¾\r\n    plt.subplot(2, 3, 6)\r\n    plt.boxplot(relative_error)\r\n    plt.ylabel('ç›¸å¯¹è¯¯å·® (%)')\r\n    plt.title('é¢„æµ‹è¯¯å·®åˆ†å¸ƒ')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    plt.tight_layout()\r\n    plt.show()\r\n    \r\n    # 7. æ¨¡å‹è§£é‡Š\r\n    print(f\"\\n=== æ¨¡å‹è§£é‡Š ===\")\r\n    print(f\"æ¨¡å‹æ€§èƒ½: {'ä¼˜ç§€' if r2 > 0.8 else 'è‰¯å¥½' if r2 > 0.6 else 'ä¸€èˆ¬'}\")\r\n    print(f\"æœ€é‡è¦çš„ç‰¹å¾: {feature_importance.iloc[0]['ç‰¹å¾']}\")\r\n    print(f\"å»ºè®®: å…³æ³¨{feature_importance.iloc[0]['ç‰¹å¾']}å’Œ{feature_importance.iloc[1]['ç‰¹å¾']}çš„ä¼˜åŒ–\")\r\n    \r\n    return {\r\n        'model': model,\r\n        'metrics': {'mse': mse, 'rmse': rmse, 'r2': r2},\r\n        'feature_importance': feature_importance,\r\n        'predictions': y_pred\r\n    }\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\n# regression_results = complete_regression_analysis()\r\n```\r\n\r\n## ğŸ” åˆ†ç±»åˆ†æå®Œæ•´å·¥ä½œæµ\r\n\r\n```python\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\r\nfrom sklearn.datasets import make_classification\r\n\r\ndef complete_classification_analysis():\r\n    \"\"\"å®Œæ•´çš„åˆ†ç±»åˆ†æå·¥ä½œæµ\"\"\"\r\n    \r\n    print(\"=== å¼€å§‹åˆ†ç±»åˆ†æ ===\")\r\n    \r\n    # 1. æ•°æ®ç”Ÿæˆ\r\n    X, y = make_classification(\r\n        n_samples=1000,\r\n        n_features=8,\r\n        n_informative=5,\r\n        n_redundant=2,\r\n        n_classes=3,\r\n        random_state=42\r\n    )\r\n    \r\n    feature_names = [f'ç‰¹å¾_{i+1}' for i in range(X.shape[1])]\r\n    df = pd.DataFrame(X, columns=feature_names)\r\n    df['ç±»åˆ«'] = y\r\n    \r\n    print(\"æ•°æ®åŸºæœ¬ä¿¡æ¯:\")\r\n    print(f\"æ•°æ®é›†å½¢çŠ¶: {df.shape}\")\r\n    print(f\"ç‰¹å¾æ•°é‡: {X.shape[1]}\")\r\n    print(f\"ç±»åˆ«æ•°é‡: {len(np.unique(y))}\")\r\n    print(f\"ç±»åˆ«åˆ†å¸ƒ: {np.unique(y, return_counts=True)}\")\r\n    \r\n    # 2. æ•°æ®æ¢ç´¢\r\n    print(\"\\n=== æ•°æ®æ¢ç´¢ ===\")\r\n    print(\"æ•°å€¼ç‰¹å¾ç»Ÿè®¡:\")\r\n    print(df.describe())\r\n    \r\n    # 3. æ¨¡å‹è®­ç»ƒ\r\n    X_data = df.drop('ç±»åˆ«', axis=1)\r\n    y_data = df['ç±»åˆ«']\r\n    \r\n    X_train, X_test, y_train, y_test = train_test_split(\r\n        X_data, y_data, test_size=0.2, random_state=42, stratify=y_data\r\n    )\r\n    \r\n    model = RandomForestClassifier(n_estimators=100, random_state=42)\r\n    model.fit(X_train, y_train)\r\n    \r\n    y_pred = model.predict(X_test)\r\n    \r\n    # 4. æ¨¡å‹è¯„ä¼°\r\n    accuracy = accuracy_score(y_test, y_pred)\r\n    \r\n    print(f\"\\n=== æ¨¡å‹æ€§èƒ½ ===\")\r\n    print(f\"å‡†ç¡®ç‡: {accuracy:.4f}\")\r\n    print(\"\\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:\")\r\n    print(classification_report(y_test, y_pred))\r\n    \r\n    # 5. ç‰¹å¾é‡è¦æ€§\r\n    feature_importance = pd.DataFrame({\r\n        'ç‰¹å¾': X_data.columns,\r\n        'é‡è¦æ€§': model.feature_importances_\r\n    }).sort_values('é‡è¦æ€§', ascending=False)\r\n    \r\n    print(f\"\\n=== ç‰¹å¾é‡è¦æ€§ ===\")\r\n    for _, row in feature_importance.iterrows():\r\n        print(f\"  {row['ç‰¹å¾']}: {row['é‡è¦æ€§']:.4f}\")\r\n    \r\n    # 6. å¯è§†åŒ–åˆ†æ\r\n    plt.figure(figsize=(15, 10))\r\n    \r\n    # æ··æ·†çŸ©é˜µ\r\n    plt.subplot(2, 3, 1)\r\n    cm = confusion_matrix(y_test, y_pred)\r\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\r\n    plt.xlabel('é¢„æµ‹æ ‡ç­¾')\r\n    plt.ylabel('çœŸå®æ ‡ç­¾')\r\n    plt.title('æ··æ·†çŸ©é˜µ')\r\n    \r\n    # ç‰¹å¾é‡è¦æ€§\r\n    plt.subplot(2, 3, 2)\r\n    top_features = feature_importance.head(8)\r\n    plt.barh(top_features['ç‰¹å¾'], top_features['é‡è¦æ€§'])\r\n    plt.xlabel('é‡è¦æ€§')\r\n    plt.title('ç‰¹å¾é‡è¦æ€§æ’å')\r\n    plt.gca().invert_yaxis()\r\n    \r\n    # ç±»åˆ«åˆ†å¸ƒ\r\n    plt.subplot(2, 3, 3)\r\n    unique, counts = np.unique(y, return_counts=True)\r\n    plt.pie(counts, labels=[f'ç±»åˆ« {cls}' for cls in unique], autopct='%1.1f%%')\r\n    plt.title('ç±»åˆ«åˆ†å¸ƒ')\r\n    \r\n    # åˆ†ç±»æŠ¥å‘Šçƒ­åŠ›å›¾\r\n    plt.subplot(2, 3, 4)\r\n    report_dict = classification_report(y_test, y_pred, output_dict=True)\r\n    report_df = pd.DataFrame(report_dict).transpose().iloc[:-3, :-1]\r\n    sns.heatmap(report_df, annot=True, cmap='YlOrRd', fmt='.3f')\r\n    plt.title('åˆ†ç±»æŒ‡æ ‡çƒ­åŠ›å›¾')\r\n    \r\n    # å­¦ä¹ æ›²çº¿ï¼ˆç®€åŒ–ç‰ˆï¼‰\r\n    plt.subplot(2, 3, 5)\r\n    train_sizes = np.linspace(0.1, 1.0, 10)\r\n    train_scores = []\r\n    test_scores = []\r\n    \r\n    for size in train_sizes:\r\n        n_train = int(size * len(X_train))\r\n        X_train_sub = X_train.iloc[:n_train]\r\n        y_train_sub = y_train.iloc[:n_train]\r\n        \r\n        model_temp = RandomForestClassifier(n_estimators=50, random_state=42)\r\n        model_temp.fit(X_train_sub, y_train_sub)\r\n        \r\n        train_score = model_temp.score(X_train_sub, y_train_sub)\r\n        test_score = model_temp.score(X_test, y_test)\r\n        \r\n        train_scores.append(train_score)\r\n        test_scores.append(test_score)\r\n    \r\n    plt.plot(train_sizes, train_scores, 'o-', label='è®­ç»ƒå¾—åˆ†')\r\n    plt.plot(train_sizes, test_scores, 'o-', label='æµ‹è¯•å¾—åˆ†')\r\n    plt.xlabel('è®­ç»ƒæ ·æœ¬æ¯”ä¾‹')\r\n    plt.ylabel('å‡†ç¡®ç‡')\r\n    plt.title('å­¦ä¹ æ›²çº¿')\r\n    plt.legend()\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # ç±»åˆ«é¢„æµ‹åˆ†å¸ƒ\r\n    plt.subplot(2, 3, 6)\r\n    pred_counts = pd.Series(y_pred).value_counts().sort_index()\r\n    true_counts = pd.Series(y_test).value_counts().sort_index()\r\n    \r\n    x = np.arange(len(true_counts))\r\n    width = 0.35\r\n    \r\n    plt.bar(x - width/2, true_counts, width, label='çœŸå®åˆ†å¸ƒ', alpha=0.7)\r\n    plt.bar(x + width/2, pred_counts, width, label='é¢„æµ‹åˆ†å¸ƒ', alpha=0.7)\r\n    plt.xlabel('ç±»åˆ«')\r\n    plt.ylabel('æ ·æœ¬æ•°')\r\n    plt.title('ç±»åˆ«åˆ†å¸ƒå¯¹æ¯”')\r\n    plt.legend()\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    plt.tight_layout()\r\n    plt.show()\r\n    \r\n    # 7. æ¨¡å‹è§£é‡Š\r\n    print(f\"\\n=== æ¨¡å‹è§£é‡Š ===\")\r\n    print(f\"æ¨¡å‹æ€§èƒ½: {'ä¼˜ç§€' if accuracy > 0.9 else 'è‰¯å¥½' if accuracy > 0.8 else 'ä¸€èˆ¬'}\")\r\n    print(f\"æœ€é‡è¦çš„ç‰¹å¾: {feature_importance.iloc[0]['ç‰¹å¾']}\")\r\n    print(f\"æœ€å®¹æ˜“æ··æ·†çš„ç±»åˆ«: æŸ¥çœ‹æ··æ·†çŸ©é˜µå¯¹è§’çº¿å¤–çš„æœ€å¤§å€¼\")\r\n    \r\n    return {\r\n        'model': model,\r\n        'metrics': {'accuracy': accuracy},\r\n        'feature_importance': feature_importance,\r\n        'predictions': y_pred\r\n    }\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\n# classification_results = complete_classification_analysis()\r\n```\r\n\r\n## ğŸ“Š ç»Ÿè®¡å»ºæ¨¡åˆ†æ\r\n\r\n```python\r\nimport statsmodels.api as sm\r\nimport statsmodels.formula.api as smf\r\n\r\ndef statistical_modeling_analysis():\r\n    \"\"\"ç»Ÿè®¡å»ºæ¨¡åˆ†æ\"\"\"\r\n    \r\n    print(\"=== å¼€å§‹ç»Ÿè®¡å»ºæ¨¡åˆ†æ ===\")\r\n    \r\n    # åˆ›å»ºç¤ºä¾‹æ•°æ®\r\n    np.random.seed(42)\r\n    n_samples = 200\r\n    \r\n    data = pd.DataFrame({\r\n        'å¹¿å‘ŠæŠ•å…¥': np.random.normal(1000, 300, n_samples),\r\n        'ä»·æ ¼': np.random.normal(50, 15, n_samples),\r\n        'ä¿ƒé”€æ´»åŠ¨': np.random.choice([0, 1], n_samples, p=[0.7, 0.3]),\r\n        'å­£èŠ‚æ€§': np.random.choice([0, 1], n_samples, p=[0.5, 0.5])\r\n    })\r\n    \r\n    # ç”Ÿæˆé”€å”®é¢ï¼ˆä¸ç‰¹å¾æœ‰çœŸå®å…³ç³»ï¼‰\r\n    data['é”€å”®é¢'] = (\r\n        500 + 0.8 * data['å¹¿å‘ŠæŠ•å…¥'] - 5 * data['ä»·æ ¼'] + \r\n        200 * data['ä¿ƒé”€æ´»åŠ¨'] + 150 * data['å­£èŠ‚æ€§'] + \r\n        np.random.normal(0, 100, n_samples)\r\n    )\r\n    \r\n    print(\"æ•°æ®åŸºæœ¬ä¿¡æ¯:\")\r\n    print(f\"æ ·æœ¬æ•°é‡: {len(data)}\")\r\n    print(f\"ç‰¹å¾: {list(data.columns[:-1])}\")\r\n    print(\"\\næ•°æ®æè¿°:\")\r\n    print(data.describe())\r\n    \r\n    # 1. OLS å›å½’åˆ†æ\r\n    print(\"\\n=== OLS å›å½’åˆ†æ ===\")\r\n    model = smf.ols('é”€å”®é¢ ~ å¹¿å‘ŠæŠ•å…¥ + ä»·æ ¼ + ä¿ƒé”€æ´»åŠ¨ + å­£èŠ‚æ€§', data=data).fit()\r\n    \r\n    print(\"å›å½’ç»“æœæ‘˜è¦:\")\r\n    print(model.summary())\r\n    \r\n    # 2. å…³é”®ç»Ÿè®¡æŒ‡æ ‡\r\n    print(f\"\\n=== å…³é”®ç»Ÿè®¡æŒ‡æ ‡ ===\")\r\n    print(f\"RÂ²: {model.rsquared:.4f}\")\r\n    print(f\"è°ƒæ•´RÂ²: {model.rsquared_adj:.4f}\")\r\n    print(f\"Fç»Ÿè®¡é‡: {model.fvalue:.2f}\")\r\n    print(f\"Fç»Ÿè®¡é‡på€¼: {model.f_pvalue:.4f}\")\r\n    \r\n    # 3. ç³»æ•°è§£é‡Š\r\n    print(f\"\\n=== ç³»æ•°è§£é‡Š ===\")\r\n    for feature, coef in model.params.items():\r\n        p_value = model.pvalues[feature]\r\n        significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"\"\r\n        print(f\"{feature}: {coef:.2f} {significance} (på€¼: {p_value:.4f})\")\r\n    \r\n    # 4. æ®‹å·®åˆ†æ\r\n    print(f\"\\n=== æ®‹å·®åˆ†æ ===\")\r\n    residuals = model.resid\r\n    print(f\"æ®‹å·®å‡å€¼: {residuals.mean():.4f}\")\r\n    print(f\"æ®‹å·®æ ‡å‡†å·®: {residuals.std():.4f}\")\r\n    \r\n    # 5. å¯è§†åŒ–åˆ†æ\r\n    plt.figure(figsize=(15, 10))\r\n    \r\n    # å®é™…å€¼ vs é¢„æµ‹å€¼\r\n    plt.subplot(2, 3, 1)\r\n    y_pred_ols = model.predict(data[['å¹¿å‘ŠæŠ•å…¥', 'ä»·æ ¼', 'ä¿ƒé”€æ´»åŠ¨', 'å­£èŠ‚æ€§']])\r\n    plt.scatter(data['é”€å”®é¢'], y_pred_ols, alpha=0.6)\r\n    plt.plot([data['é”€å”®é¢'].min(), data['é”€å”®é¢'].max()], \r\n             [data['é”€å”®é¢'].min(), data['é”€å”®é¢'].max()], 'r--', lw=2)\r\n    plt.xlabel('å®é™…é”€å”®é¢')\r\n    plt.ylabel('é¢„æµ‹é”€å”®é¢')\r\n    plt.title(f'OLSé¢„æµ‹æ•ˆæœ (RÂ² = {model.rsquared:.3f})')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # æ®‹å·®å›¾\r\n    plt.subplot(2, 3, 2)\r\n    plt.scatter(y_pred_ols, residuals, alpha=0.6)\r\n    plt.axhline(y=0, color='r', linestyle='--')\r\n    plt.xlabel('é¢„æµ‹å€¼')\r\n    plt.ylabel('æ®‹å·®')\r\n    plt.title('æ®‹å·®åˆ†æ')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # Q-Qå›¾\r\n    plt.subplot(2, 3, 3)\r\n    sm.qqplot(residuals, line='45', ax=plt.gca())\r\n    plt.title('Q-Qå›¾ï¼ˆæ®‹å·®æ­£æ€æ€§æ£€éªŒï¼‰')\r\n    \r\n    # ç‰¹å¾ä¸ç›®æ ‡å˜é‡å…³ç³»\r\n    plt.subplot(2, 3, 4)\r\n    plt.scatter(data['å¹¿å‘ŠæŠ•å…¥'], data['é”€å”®é¢'], alpha=0.6)\r\n    plt.xlabel('å¹¿å‘ŠæŠ•å…¥')\r\n    plt.ylabel('é”€å”®é¢')\r\n    plt.title('å¹¿å‘ŠæŠ•å…¥ vs é”€å”®é¢')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    plt.subplot(2, 3, 5)\r\n    plt.scatter(data['ä»·æ ¼'], data['é”€å”®é¢'], alpha=0.6)\r\n    plt.xlabel('ä»·æ ¼')\r\n    plt.ylabel('é”€å”®é¢')\r\n    plt.title('ä»·æ ¼ vs é”€å”®é¢')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # ç³»æ•°å¯è§†åŒ–\r\n    plt.subplot(2, 3, 6)\r\n    coefficients = model.params.iloc[1:]  # æ’é™¤æˆªè·é¡¹\r\n    colors = ['green' if p < 0.05 else 'red' for p in model.pvalues.iloc[1:]]\r\n    plt.barh(coefficients.index, coefficients.values, color=colors)\r\n    plt.axvline(x=0, color='black', linestyle='-')\r\n    plt.xlabel('ç³»æ•°å€¼')\r\n    plt.title('ç‰¹å¾ç³»æ•°ï¼ˆç»¿è‰²è¡¨ç¤ºæ˜¾è‘—ï¼‰')\r\n    \r\n    plt.tight_layout()\r\n    plt.show()\r\n    \r\n    # 6. ä¸šåŠ¡è§£é‡Š\r\n    print(f\"\\n=== ä¸šåŠ¡è§£é‡Š ===\")\r\n    print(f\"æ¨¡å‹è§£é‡ŠåŠ›: {'å¼º' if model.rsquared > 0.7 else 'ä¸­ç­‰' if model.rsquared > 0.5 else 'å¼±'}\")\r\n    \r\n    significant_features = []\r\n    for feature in model.params.index[1:]:  # æ’é™¤æˆªè·\r\n        if model.pvalues[feature] < 0.05:\r\n            significant_features.append(feature)\r\n    \r\n    if significant_features:\r\n        print(f\"æ˜¾è‘—å½±å“ç‰¹å¾: {', '.join(significant_features)}\")\r\n    else:\r\n        print(\"æ²¡æœ‰å‘ç°ç»Ÿè®¡æ˜¾è‘—çš„ç‰¹å¾\")\r\n    \r\n    return {\r\n        'model': model,\r\n        'rsquared': model.rsquared,\r\n        'significant_features': significant_features,\r\n        'residuals': residuals\r\n    }\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\n# stats_results = statistical_modeling_analysis()\r\n```\r\n\r\n## â° æ—¶é—´åºåˆ—åˆ†æï¼ˆv2.3æ–°å¢ï¼‰\r\n\r\n### ä½¿ç”¨pmdarimaè¿›è¡Œè‡ªåŠ¨åŒ–ARIMAå»ºæ¨¡\r\n\r\n```python\r\nfrom pmdarima import auto_arima\r\nimport xgboost as xgb\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\r\n\r\ndef time_series_arima_analysis(series, seasonal_period=7, forecast_steps=30):\r\n    \"\"\"è‡ªåŠ¨åŒ–ARIMAæ—¶é—´åºåˆ—åˆ†æ\"\"\"\r\n    \r\n    print(\"=== å¼€å§‹æ—¶é—´åºåˆ—ARIMAåˆ†æ ===\")\r\n    \r\n    # 1. æ•°æ®æ£€æŸ¥\r\n    print(f\"æ—¶é—´åºåˆ—é•¿åº¦: {len(series)}\")\r\n    print(f\"æ•°æ®ç±»å‹: {type(series)}\")\r\n    \r\n    # 2. è‡ªåŠ¨ARIMAå»ºæ¨¡\r\n    print(\"\\n=== è‡ªåŠ¨ARIMAå‚æ•°é€‰æ‹© ===\")\r\n    try:\r\n        model = auto_arima(\r\n            series,\r\n            seasonal=True,\r\n            m=seasonal_period,  # å­£èŠ‚æ€§å‘¨æœŸï¼ˆ7å¤©ä¸ºå‘¨å­£èŠ‚æ€§ï¼‰\r\n            stepwise=True,      # ä½¿ç”¨é€æ­¥æœç´¢ï¼ŒèŠ‚çœå†…å­˜\r\n            suppress_warnings=True,\r\n            error_action='ignore',\r\n            trace=True,         # æ˜¾ç¤ºæœç´¢è¿‡ç¨‹\r\n            random_state=42\r\n        )\r\n        \r\n        print(f\"æœ€ä½³ARIMAå‚æ•°: {model.order}\")\r\n        print(f\"æœ€ä½³å­£èŠ‚æ€§å‚æ•°: {model.seasonal_order}\")\r\n        print(f\"æ¨¡å‹AIC: {model.aic():.2f}\")\r\n        \r\n    except Exception as e:\r\n        print(f\"è‡ªåŠ¨ARIMAå¤±è´¥: {e}\")\r\n        return None\r\n    \r\n    # 3. æ¨¡å‹æ‘˜è¦\r\n    print(\"\\n=== æ¨¡å‹æ‘˜è¦ ===\")\r\n    print(model.summary())\r\n    \r\n    # 4. é¢„æµ‹\r\n    print(f\"\\n=== æœªæ¥{forecast_steps}æœŸé¢„æµ‹ ===\")\r\n    forecast, conf_int = model.predict(\r\n        n_periods=forecast_steps,\r\n        return_conf_int=True,\r\n        alpha=0.05  # 95%ç½®ä¿¡åŒºé—´\r\n    )\r\n    \r\n    # 5. æ¨¡å‹è¯„ä¼°ï¼ˆä½¿ç”¨è®­ç»ƒé›†æœ€åéƒ¨åˆ†ä½œä¸ºéªŒè¯ï¼‰\r\n    train_size = int(len(series) * 0.8)\r\n    train = series[:train_size]\r\n    test = series[train_size:]\r\n    \r\n    # åœ¨è®­ç»ƒé›†ä¸Šé‡æ–°æ‹Ÿåˆæ¨¡å‹\r\n    model.fit(train)\r\n    predictions = model.predict(n_periods=len(test))\r\n    \r\n    # è®¡ç®—æŒ‡æ ‡\r\n    mae = mean_absolute_error(test, predictions)\r\n    rmse = np.sqrt(mean_squared_error(test, predictions))\r\n    mape = np.mean(np.abs((test - predictions) / test)) * 100\r\n    \r\n    print(f\"\\n=== æ¨¡å‹æ€§èƒ½è¯„ä¼° ===\")\r\n    print(f\"MAE (å¹³å‡ç»å¯¹è¯¯å·®): {mae:.2f}\")\r\n    print(f\"RMSE (å‡æ–¹æ ¹è¯¯å·®): {rmse:.2f}\")\r\n    print(f\"MAPE (å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®): {mape:.2f}%\")\r\n    \r\n    # 6. å¯è§†åŒ–\r\n    plt.figure(figsize=(15, 10))\r\n    \r\n    # åŸå§‹åºåˆ—ä¸æ‹Ÿåˆå€¼\r\n    plt.subplot(2, 2, 1)\r\n    plt.plot(series.index, series, label='åŸå§‹åºåˆ—', alpha=0.7)\r\n    plt.plot(series.index, model.predict_in_sample(), label='æ‹Ÿåˆå€¼', alpha=0.7)\r\n    plt.xlabel('æ—¶é—´')\r\n    plt.ylabel('å€¼')\r\n    plt.title('åŸå§‹åºåˆ—ä¸æ¨¡å‹æ‹Ÿåˆ')\r\n    plt.legend()\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # æ®‹å·®åˆ†æ\r\n    plt.subplot(2, 2, 2)\r\n    residuals = series - model.predict_in_sample()\r\n    plt.plot(residuals.index, residuals, alpha=0.7)\r\n    plt.axhline(y=0, color='r', linestyle='--')\r\n    plt.xlabel('æ—¶é—´')\r\n    plt.ylabel('æ®‹å·®')\r\n    plt.title('æ¨¡å‹æ®‹å·®')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # é¢„æµ‹ç»“æœ\r\n    plt.subplot(2, 2, 3)\r\n    last_n = min(100, len(series))\r\n    plt.plot(series.index[-last_n:], series.values[-last_n:], label='å†å²æ•°æ®')\r\n    \r\n    # åˆ›å»ºæœªæ¥æ—¶é—´ç´¢å¼•\r\n    if hasattr(series.index, 'freq'):\r\n        future_index = pd.date_range(start=series.index[-1], periods=forecast_steps+1, freq=series.index.freq)[1:]\r\n    else:\r\n        future_index = range(len(series), len(series) + forecast_steps)\r\n    \r\n    plt.plot(future_index, forecast, label='é¢„æµ‹å€¼', color='red')\r\n    plt.fill_between(future_index, conf_int[:, 0], conf_int[:, 1], color='pink', alpha=0.3, label='95%ç½®ä¿¡åŒºé—´')\r\n    plt.xlabel('æ—¶é—´')\r\n    plt.ylabel('å€¼')\r\n    plt.title(f'æœªæ¥{forecast_steps}æœŸé¢„æµ‹')\r\n    plt.legend()\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # æ®‹å·®åˆ†å¸ƒ\r\n    plt.subplot(2, 2, 4)\r\n    plt.hist(residuals.dropna(), bins=30, alpha=0.7, edgecolor='black')\r\n    plt.xlabel('æ®‹å·®å€¼')\r\n    plt.ylabel('é¢‘æ•°')\r\n    plt.title('æ®‹å·®åˆ†å¸ƒ')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    plt.tight_layout()\r\n    plt.show()\r\n    \r\n    return {\r\n        'model': model,\r\n        'order': model.order,\r\n        'seasonal_order': model.seasonal_order,\r\n        'forecast': forecast,\r\n        'confidence_interval': conf_int,\r\n        'metrics': {'mae': mae, 'rmse': rmse, 'mape': mape},\r\n        'residuals': residuals\r\n    }\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\n# å‡è®¾dfæ˜¯ä¸€ä¸ªæ—¶é—´åºåˆ—DataFrameï¼Œindexä¸ºæ—¥æœŸï¼Œæœ‰ä¸€åˆ—'é”€å”®é¢'\r\n# results = time_series_arima_analysis(df['é”€å”®é¢'], seasonal_period=7, forecast_steps=30)\r\n```\r\n\r\n### ä½¿ç”¨XGBoostè¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹\r\n\r\n```python\r\ndef time_series_xgboost_analysis(df, target_col, lag_features=7, forecast_steps=30):\r\n    \"\"\"ä½¿ç”¨XGBoostè¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹\"\"\"\r\n    \r\n    print(\"=== å¼€å§‹æ—¶é—´åºåˆ—XGBooståˆ†æ ===\")\r\n    \r\n    # 1. å‡†å¤‡ç‰¹å¾\r\n    print(\"å‡†å¤‡æ—¶é—´åºåˆ—ç‰¹å¾...\")\r\n    features_df = pd.DataFrame(index=df.index)\r\n    \r\n    # æ»åç‰¹å¾\r\n    for lag in range(1, lag_features + 1):\r\n        features_df[f'lag_{lag}'] = df[target_col].shift(lag)\r\n    \r\n    # æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾\r\n    for window in [3, 7, 14, 30]:\r\n        features_df[f'ma_{window}'] = df[target_col].rolling(window).mean().shift(1)\r\n        features_df[f'std_{window}'] = df[target_col].rolling(window).std().shift(1)\r\n    \r\n    # æ—¥æœŸç‰¹å¾\r\n    if hasattr(df.index, 'month'):\r\n        features_df['month'] = df.index.month\r\n        features_df['dayofweek'] = df.index.dayofweek\r\n        features_df['dayofmonth'] = df.index.day\r\n        features_df['quarter'] = df.index.quarter\r\n    \r\n    # å¤–éƒ¨ç‰¹å¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰\r\n    external_features = ['Temperature', 'Promotion', 'Competitor_Price', 'Holiday']\r\n    for feat in external_features:\r\n        if feat in df.columns:\r\n            features_df[feat] = df[feat]\r\n    \r\n    # ç›®æ ‡å˜é‡\r\n    features_df['target'] = df[target_col]\r\n    \r\n    # ç§»é™¤ç¼ºå¤±å€¼\r\n    features_df = features_df.dropna()\r\n    \r\n    print(f\"ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {features_df.shape}\")\r\n    \r\n    # 2. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\r\n    X = features_df.drop('target', axis=1)\r\n    y = features_df['target']\r\n    \r\n    split_idx = int(len(X) * 0.8)\r\n    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\r\n    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\r\n    \r\n    print(f\"è®­ç»ƒé›†å¤§å°: {X_train.shape}\")\r\n    print(f\"æµ‹è¯•é›†å¤§å°: {X_test.shape}\")\r\n    \r\n    # 3. è®­ç»ƒXGBoostæ¨¡å‹\r\n    print(\"\\nè®­ç»ƒXGBoostæ¨¡å‹...\")\r\n    \r\n    xgb_model = xgb.XGBRegressor(\r\n        n_estimators=100,\r\n        max_depth=5,\r\n        learning_rate=0.05,\r\n        subsample=0.8,\r\n        colsample_bytree=0.8,\r\n        tree_method='hist',  # å†…å­˜å‹å¥½\r\n        n_jobs=2,           # 6GBå†…å­˜ä¸‹ä½¿ç”¨2ä¸ªçº¿ç¨‹\r\n        random_state=42,\r\n        verbosity=0\r\n    )\r\n    \r\n    xgb_model.fit(X_train, y_train)\r\n    \r\n    # 4. æ¨¡å‹è¯„ä¼°\r\n    y_pred = xgb_model.predict(X_test)\r\n    \r\n    mae = mean_absolute_error(y_test, y_pred)\r\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\r\n    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\r\n    \r\n    print(f\"\\n=== XGBoostæ¨¡å‹æ€§èƒ½ ===\")\r\n    print(f\"MAE (å¹³å‡ç»å¯¹è¯¯å·®): {mae:.2f}\")\r\n    print(f\"RMSE (å‡æ–¹æ ¹è¯¯å·®): {rmse:.2f}\")\r\n    print(f\"MAPE (å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®): {mape:.2f}%\")\r\n    \r\n    # 5. ç‰¹å¾é‡è¦æ€§\r\n    feature_importance = pd.DataFrame({\r\n        'ç‰¹å¾': X.columns,\r\n        'é‡è¦æ€§': xgb_model.feature_importances_\r\n    }).sort_values('é‡è¦æ€§', ascending=False)\r\n    \r\n    print(f\"\\n=== ç‰¹å¾é‡è¦æ€§ï¼ˆTop 10ï¼‰===\")\r\n    for _, row in feature_importance.head(10).iterrows():\r\n        print(f\"  {row['ç‰¹å¾']}: {row['é‡è¦æ€§']:.4f}\")\r\n    \r\n    # 6. å¯è§†åŒ–\r\n    plt.figure(figsize=(15, 10))\r\n    \r\n    # é¢„æµ‹ vs å®é™…\r\n    plt.subplot(2, 3, 1)\r\n    plt.scatter(y_test, y_pred, alpha=0.6)\r\n    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\r\n    plt.xlabel('å®é™…å€¼')\r\n    plt.ylabel('é¢„æµ‹å€¼')\r\n    plt.title(f'XGBoosté¢„æµ‹æ•ˆæœ (MAE = {mae:.2f})')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # ç‰¹å¾é‡è¦æ€§\r\n    plt.subplot(2, 3, 2)\r\n    top_features = feature_importance.head(10)\r\n    plt.barh(top_features['ç‰¹å¾'], top_features['é‡è¦æ€§'])\r\n    plt.xlabel('é‡è¦æ€§')\r\n    plt.title('Top 10 ç‰¹å¾é‡è¦æ€§')\r\n    plt.gca().invert_yaxis()\r\n    \r\n    # æ—¶é—´åºåˆ—é¢„æµ‹å¯¹æ¯”\r\n    plt.subplot(2, 3, 3)\r\n    plt.plot(y_test.index, y_test.values, label='å®é™…å€¼', alpha=0.7)\r\n    plt.plot(y_test.index, y_pred, label='é¢„æµ‹å€¼', alpha=0.7)\r\n    plt.xlabel('æ—¶é—´')\r\n    plt.ylabel('å€¼')\r\n    plt.title('æ—¶é—´åºåˆ—é¢„æµ‹å¯¹æ¯”')\r\n    plt.legend()\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # æ®‹å·®åˆ†æ\r\n    plt.subplot(2, 3, 4)\r\n    residuals = y_test - y_pred\r\n    plt.scatter(y_pred, residuals, alpha=0.6)\r\n    plt.axhline(y=0, color='r', linestyle='--')\r\n    plt.xlabel('é¢„æµ‹å€¼')\r\n    plt.ylabel('æ®‹å·®')\r\n    plt.title('æ®‹å·®åˆ†æ')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # è¯¯å·®åˆ†å¸ƒ\r\n    plt.subplot(2, 3, 5)\r\n    plt.hist(residuals, bins=30, alpha=0.7, edgecolor='black')\r\n    plt.xlabel('æ®‹å·®')\r\n    plt.ylabel('é¢‘æ•°')\r\n    plt.title('è¯¯å·®åˆ†å¸ƒ')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # æ»šåŠ¨é¢„æµ‹\r\n    plt.subplot(2, 3, 6)\r\n    # å–æœ€å100ä¸ªç‚¹å±•ç¤º\r\n    last_n = min(100, len(y_test))\r\n    plt.plot(y_test.index[-last_n:], y_test.values[-last_n:], label='å®é™…å€¼')\r\n    plt.plot(y_test.index[-last_n:], y_pred[-last_n:], label='é¢„æµ‹å€¼')\r\n    plt.xlabel('æ—¶é—´')\r\n    plt.ylabel('å€¼')\r\n    plt.title('æ»šåŠ¨é¢„æµ‹å¯¹æ¯”ï¼ˆæœ€å100ç‚¹ï¼‰')\r\n    plt.legend()\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    plt.tight_layout()\r\n    plt.show()\r\n    \r\n    # 7. æœªæ¥é¢„æµ‹ï¼ˆå¦‚æœéœ€è¦ï¼‰\r\n    if forecast_steps > 0:\r\n        print(f\"\\n=== æœªæ¥{forecast_steps}æœŸé¢„æµ‹ ===\")\r\n        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“ä¸šåŠ¡é€»è¾‘å®ç°æ»šåŠ¨é¢„æµ‹\r\n        # ç®€åŒ–ç‰ˆï¼šä½¿ç”¨æœ€ålag_featuresä¸ªç‚¹ä½œä¸ºåˆå§‹ç‰¹å¾\r\n        \r\n        last_features = X.iloc[-1:].copy()\r\n        future_predictions = []\r\n        \r\n        for i in range(forecast_steps):\r\n            # é¢„æµ‹ä¸‹ä¸€æ­¥\r\n            pred = xgb_model.predict(last_features)[0]\r\n            future_predictions.append(pred)\r\n            \r\n            # æ›´æ–°ç‰¹å¾ï¼ˆå¦‚æœæ˜¯æ—¶é—´åºåˆ—é¢„æµ‹ï¼Œéœ€è¦æ›´æ–°æ»åç‰¹å¾ï¼‰\r\n            # è¿™é‡Œç®€åŒ–ä¸ºåªä½¿ç”¨æœ€æ–°é¢„æµ‹å€¼\r\n            # å®é™…åº”ç”¨ä¸­éœ€è¦æ ¹æ®ç‰¹å¾å·¥ç¨‹é€»è¾‘æ›´æ–°\r\n            \r\n        print(f\"æœªæ¥é¢„æµ‹å€¼: {future_predictions}\")\r\n    \r\n    return {\r\n        'model': xgb_model,\r\n        'metrics': {'mae': mae, 'rmse': rmse, 'mape': mape},\r\n        'feature_importance': feature_importance,\r\n        'predictions': y_pred,\r\n        'future_predictions': future_predictions if forecast_steps > 0 else None\r\n    }\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\n# å‡è®¾dfæ˜¯ä¸€ä¸ªDataFrameï¼ŒåŒ…å«æ—¶é—´åºåˆ—å’Œå¤–éƒ¨ç‰¹å¾\r\n# results = time_series_xgboost_analysis(df, target_col='Sales', lag_features=14, forecast_steps=30)\r\n```\r\n\r\n### æ—¶é—´åºåˆ—æ¨¡å‹å¯¹æ¯”\r\n\r\n```python\r\ndef compare_time_series_models(df, target_col, seasonal_period=7, lag_features=14):\r\n    \"\"\"å¯¹æ¯”ä¸åŒæ—¶é—´åºåˆ—æ¨¡å‹æ€§èƒ½\"\"\"\r\n    \r\n    print(\"=== æ—¶é—´åºåˆ—æ¨¡å‹å¯¹æ¯”åˆ†æ ===\")\r\n    \r\n    # å‡†å¤‡æ•°æ®\r\n    series = df[target_col]\r\n    \r\n    # 1. ARIMAæ¨¡å‹\r\n    print(\"\\n1. è®­ç»ƒARIMAæ¨¡å‹...\")\r\n    arima_results = time_series_arima_analysis(series, seasonal_period, forecast_steps=0)\r\n    \r\n    # 2. XGBoostæ¨¡å‹\r\n    print(\"\\n2. è®­ç»ƒXGBoostæ¨¡å‹...\")\r\n    xgb_results = time_series_xgboost_analysis(df, target_col, lag_features, forecast_steps=0)\r\n    \r\n    # 3. LightGBMæ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰\r\n    try:\r\n        import lightgbm as lgb\r\n        print(\"\\n3. è®­ç»ƒLightGBMæ¨¡å‹...\")\r\n        \r\n        # å‡†å¤‡ç‰¹å¾ï¼ˆå¤ç”¨XGBoostçš„ç‰¹å¾ï¼‰\r\n        features_df = pd.DataFrame(index=df.index)\r\n        for lag in range(1, lag_features + 1):\r\n            features_df[f'lag_{lag}'] = df[target_col].shift(lag)\r\n        \r\n        for window in [3, 7, 14, 30]:\r\n            features_df[f'ma_{window}'] = df[target_col].rolling(window).mean().shift(1)\r\n        \r\n        if hasattr(df.index, 'month'):\r\n            features_df['month'] = df.index.month\r\n            features_df['dayofweek'] = df.index.dayofweek\r\n        \r\n        external_features = ['Temperature', 'Promotion', 'Competitor_Price', 'Holiday']\r\n        for feat in external_features:\r\n            if feat in df.columns:\r\n                features_df[feat] = df[feat]\r\n        \r\n        features_df['target'] = df[target_col]\r\n        features_df = features_df.dropna()\r\n        \r\n        X = features_df.drop('target', axis=1)\r\n        y = features_df['target']\r\n        \r\n        split_idx = int(len(X) * 0.8)\r\n        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\r\n        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\r\n        \r\n        # è®­ç»ƒLightGBM\r\n        lgb_model = lgb.LGBMRegressor(\r\n            num_leaves=31,\r\n            learning_rate=0.05,\r\n            n_estimators=100,\r\n            n_jobs=2,\r\n            random_state=42,\r\n            verbose=-1\r\n        )\r\n        \r\n        lgb_model.fit(X_train, y_train)\r\n        y_pred_lgb = lgb_model.predict(X_test)\r\n        \r\n        mae_lgb = mean_absolute_error(y_test, y_pred_lgb)\r\n        rmse_lgb = np.sqrt(mean_squared_error(y_test, y_pred_lgb))\r\n        \r\n        print(f\"LightGBMæ€§èƒ½: MAE={mae_lgb:.2f}, RMSE={rmse_lgb:.2f}\")\r\n        \r\n        lgb_results = {\r\n            'model': lgb_model,\r\n            'metrics': {'mae': mae_lgb, 'rmse': rmse_lgb}\r\n        }\r\n        \r\n    except ImportError:\r\n        print(\"LightGBMä¸å¯ç”¨ï¼Œè·³è¿‡\")\r\n        lgb_results = None\r\n    \r\n    # 4. æ¨¡å‹å¯¹æ¯”\r\n    print(\"\\n=== æ¨¡å‹æ€§èƒ½å¯¹æ¯” ===\")\r\n    \r\n    comparison_data = []\r\n    \r\n    if arima_results:\r\n        comparison_data.append({\r\n            'æ¨¡å‹': 'ARIMA',\r\n            'MAE': arima_results['metrics']['mae'],\r\n            'RMSE': arima_results['metrics']['rmse'],\r\n            'MAPE': arima_results['metrics']['mape']\r\n        })\r\n    \r\n    if xgb_results:\r\n        comparison_data.append({\r\n            'æ¨¡å‹': 'XGBoost',\r\n            'MAE': xgb_results['metrics']['mae'],\r\n            'RMSE': xgb_results['metrics']['rmse'],\r\n            'MAPE': xgb_results['metrics']['mape']\r\n        })\r\n    \r\n    if lgb_results:\r\n        comparison_data.append({\r\n            'æ¨¡å‹': 'LightGBM',\r\n            'MAE': lgb_results['metrics']['mae'],\r\n            'RMSE': lgb_results['metrics']['rmse'],\r\n            'MAPE': None\r\n        })\r\n    \r\n    comparison_df = pd.DataFrame(comparison_data)\r\n    print(comparison_df.to_string(index=False))\r\n    \r\n    # 5. å¯è§†åŒ–å¯¹æ¯”\r\n    if len(comparison_data) > 1:\r\n        plt.figure(figsize=(12, 5))\r\n        \r\n        # MAEå¯¹æ¯”\r\n        plt.subplot(1, 2, 1)\r\n        models = [d['æ¨¡å‹'] for d in comparison_data]\r\n        maes = [d['MAE'] for d in comparison_data]\r\n        \r\n        bars = plt.bar(models, maes, alpha=0.7)\r\n        plt.xlabel('æ¨¡å‹')\r\n        plt.ylabel('MAE')\r\n        plt.title('æ¨¡å‹MAEå¯¹æ¯”')\r\n        plt.grid(True, alpha=0.3)\r\n        \r\n        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼\r\n        for bar, mae in zip(bars, maes):\r\n            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \r\n                    f'{mae:.2f}', ha='center', va='bottom')\r\n        \r\n        # RMSEå¯¹æ¯”\r\n        plt.subplot(1, 2, 2)\r\n        rmses = [d['RMSE'] for d in comparison_data]\r\n        \r\n        bars = plt.bar(models, rmses, alpha=0.7, color='orange')\r\n        plt.xlabel('æ¨¡å‹')\r\n        plt.ylabel('RMSE')\r\n        plt.title('æ¨¡å‹RMSEå¯¹æ¯”')\r\n        plt.grid(True, alpha=0.3)\r\n        \r\n        for bar, rmse in zip(bars, rmses):\r\n            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \r\n                    f'{rmse:.2f}', ha='center', va='bottom')\r\n        \r\n        plt.tight_layout()\r\n        plt.show()\r\n        \r\n        # æ¨èæ¨¡å‹\r\n        best_model_idx = np.argmin(maes)\r\n        best_model = models[best_model_idx]\r\n        print(f\"\\n=== æ¨èæ¨¡å‹ ===\")\r\n        print(f\"æ ¹æ®MAEæŒ‡æ ‡ï¼Œæ¨èä½¿ç”¨: {best_model}æ¨¡å‹\")\r\n        print(f\"ç†ç”±: åœ¨æµ‹è¯•é›†ä¸Šè¡¨ç°æœ€ä½³ (MAE = {maes[best_model_idx]:.2f})\")\r\n    \r\n    return {\r\n        'arima': arima_results,\r\n        'xgboost': xgb_results,\r\n        'lightgbm': lgb_results,\r\n        'comparison': comparison_df\r\n    }\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\n# å‡è®¾æœ‰å®Œæ•´çš„æ—¶é—´åºåˆ—æ•°æ®é›†df\r\n# model_comparison = compare_time_series_models(df, target_col='Sales', seasonal_period=7, lag_features=14)\r\n```\r\n\r\n## ğŸ”§ æ¨¡å‹ä¼˜åŒ–ä¸è°ƒå‚\r\n\r\n```python\r\nfrom sklearn.model_selection import GridSearchCV\r\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\r\n\r\ndef model_optimization_pipeline(X, y, problem_type='regression'):\r\n    \"\"\"æ¨¡å‹è¶…å‚æ•°ä¼˜åŒ–æµç¨‹\"\"\"\r\n    \r\n    print(f\"=== å¼€å§‹ {problem_type} æ¨¡å‹ä¼˜åŒ– ===\")\r\n    \r\n    # æ•°æ®åˆ†å‰²\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\r\n    \r\n    # é€‰æ‹©æ¨¡å‹å’Œå‚æ•°ç½‘æ ¼\r\n    if problem_type == 'regression':\r\n        model = RandomForestRegressor(random_state=42)\r\n        param_grid = {\r\n            'n_estimators': [50, 100, 200],\r\n            'max_depth': [None, 10, 20],\r\n            'min_samples_split': [2, 5, 10],\r\n            'min_samples_leaf': [1, 2, 4]\r\n        }\r\n        scoring = 'r2'\r\n    else:\r\n        model = RandomForestClassifier(random_state=42)\r\n        param_grid = {\r\n            'n_estimators': [50, 100, 200],\r\n            'max_depth': [None, 10, 20],\r\n            'min_samples_split': [2, 5, 10],\r\n            'min_samples_leaf': [1, 2, 4]\r\n        }\r\n        scoring = 'accuracy'\r\n    \r\n    # ç½‘æ ¼æœç´¢\r\n    print(\"æ­£åœ¨è¿›è¡Œç½‘æ ¼æœç´¢...\")\r\n    grid_search = GridSearchCV(\r\n        model, param_grid, cv=5, scoring=scoring, \r\n        n_jobs=-1, verbose=1\r\n    )\r\n    grid_search.fit(X_train, y_train)\r\n    \r\n    # è¾“å‡ºæœ€ä¼˜å‚æ•°\r\n    print(f\"\\n=== æœ€ä¼˜å‚æ•° ===\")\r\n    for param, value in grid_search.best_params_.items():\r\n        print(f\"  {param}: {value}\")\r\n    \r\n    print(f\"æœ€ä¼˜æ¨¡å‹å¾—åˆ†: {grid_search.best_score_:.4f}\")\r\n    \r\n    # æµ‹è¯•é›†æ€§èƒ½\r\n    best_model = grid_search.best_estimator_\r\n    y_pred = best_model.predict(X_test)\r\n    \r\n    if problem_type == 'regression':\r\n        test_score = r2_score(y_test, y_pred)\r\n        print(f\"æµ‹è¯•é›† RÂ²: {test_score:.4f}\")\r\n    else:\r\n        test_score = accuracy_score(y_test, y_pred)\r\n        print(f\"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_score:.4f}\")\r\n    \r\n    return {\r\n        'best_model': best_model,\r\n        'best_params': grid_search.best_params_,\r\n        'best_score': grid_search.best_score_,\r\n        'test_score': test_score\r\n    }\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\n# X_reg, y_reg, X_clf, y_clf = prepare_ml_data()\r\n# optimized_regression = model_optimization_pipeline(X_reg, y_reg, 'regression')\r\n# optimized_classification = model_optimization_pipeline(X_clf, y_clf, 'classification')\r\n```\r\n\r\n## æœºå™¨å­¦ä¹ å¢å¼º(v2.5æ–°å¢)\r\n\r\n### LightGBM - é«˜æ•ˆæ¢¯åº¦æå‡\r\n\r\n**ç”¨é€”**: é«˜æ€§èƒ½æ¢¯åº¦æå‡æ ‘ç®—æ³•  \r\n**ä¼˜åŠ¿**: æ¯”XGBoostè®­ç»ƒæ›´å¿«ï¼Œå†…å­˜å ç”¨æ›´å°‘  \r\n\r\n```python\r\nimport lightgbm as lgb\r\nfrom sklearn.model_selection import train_test_split\r\nimport pandas as pd\r\n\r\n# å‡†å¤‡æ•°æ®\r\ndata = pd.read_csv('/data/train.csv')\r\nX = data.drop('target', axis=1)\r\ny = data['target']\r\n\r\n# åˆ’åˆ†æ•°æ®é›†\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\r\n\r\n# åˆ›å»ºæ•°æ®é›†\r\ntrain_data = lgb.Dataset(X_train, label=y_train)\r\ntest_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\r\n\r\n# å‚æ•°è®¾ç½®ï¼ˆä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼‰\r\nparams = {\r\n    'boosting_type': 'gbdt',\r\n    'objective': 'binary',\r\n    'metric': 'binary_logloss',\r\n    'num_leaves': 31,\r\n    'learning_rate': 0.05,\r\n    'feature_fraction': 0.9,\r\n    'bagging_fraction': 0.8,\r\n    'bagging_freq': 5,\r\n    'verbose': -1,\r\n    'num_threads': 2  # é™åˆ¶çº¿ç¨‹æ•°\r\n}\r\n\r\n# è®­ç»ƒæ¨¡å‹\r\ngbm = lgb.train(params, train_data, num_boost_round=100)\r\n```\r\n\r\n### Category Encoders - åˆ†ç±»ç‰¹å¾ç¼–ç \r\n\r\n**ç”¨é€”**: å„ç§åˆ†ç±»ç¼–ç æ–¹æ³•  \r\n**ä¼˜åŠ¿**: æå‡åˆ†ç±»æ¨¡å‹æ€§èƒ½ï¼Œæ”¯æŒå¤šç§ç¼–ç ç­–ç•¥  \r\n\r\n```python\r\nimport pandas as pd\r\nimport category_encoders as ce\r\n\r\n# åˆ›å»ºç¤ºä¾‹æ•°æ®\r\ndf = pd.DataFrame({\r\n    'category': ['A', 'B', 'A', 'C', 'B', 'A'],\r\n    'value': [1, 2, 3, 4, 5, 6]\r\n})\r\n\r\n# ä½¿ç”¨Target Encoding\r\nencoder = ce.TargetEncoder(cols=['category'])\r\ndf_encoded = encoder.fit_transform(df['category'], df['value'])\r\n\r\nprint(df_encoded)\r\n```\r\n\r\n### XGBoost - é«˜æ€§èƒ½æ¢¯åº¦æå‡æ ‘ (v2.3æ–°å¢)\r\n\r\n**ç”¨é€”**: é«˜çº§æ¢¯åº¦æå‡æ ‘ç®—æ³•ï¼Œæ”¯æŒå›å½’ã€åˆ†ç±»ã€æ’åºä»»åŠ¡  \r\n**ä¼˜åŠ¿**: ç²¾åº¦é«˜ï¼Œæ”¯æŒè‡ªå®šä¹‰ç›®æ ‡å‡½æ•°ï¼Œå¯è§£é‡Šæ€§å¥½  \r\n\r\n```python\r\nimport xgboost as xgb\r\nimport pandas as pd\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.metrics import accuracy_score, mean_squared_error\r\n\r\n# å‡†å¤‡æ•°æ®\r\ndata = pd.read_csv('/data/train.csv')\r\nX = data.drop('target', axis=1)\r\ny = data['target']\r\n\r\n# åˆ’åˆ†æ•°æ®é›†\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\r\n\r\n# åˆ›å»ºDMatrixï¼ˆXGBoosté«˜æ•ˆæ•°æ®ç»“æ„ï¼‰\r\ndtrain = xgb.DMatrix(X_train, label=y_train)\r\ndtest = xgb.DMatrix(X_test, label=y_test)\r\n\r\n# å‚æ•°è®¾ç½®ï¼ˆå›å½’é—®é¢˜ç¤ºä¾‹ï¼‰\r\nparams = {\r\n    'objective': 'reg:squarederror',  # å›å½’ä»»åŠ¡\r\n    'max_depth': 5,\r\n    'eta': 0.1,  # å­¦ä¹ ç‡\r\n    'subsample': 0.8,\r\n    'colsample_bytree': 0.8,\r\n    'tree_method': 'hist',  # å†…å­˜å‹å¥½çš„ç›´æ–¹å›¾ç®—æ³•\r\n    'n_jobs': 2,  # 6GBå†…å­˜ä¸‹ä½¿ç”¨2ä¸ªçº¿ç¨‹\r\n    'random_state': 42\r\n}\r\n\r\n# è®­ç»ƒæ¨¡å‹\r\nnum_rounds = 100\r\nmodel = xgb.train(params, dtrain, num_rounds)\r\n\r\n# é¢„æµ‹\r\ny_pred = model.predict(dtest)\r\n\r\n# è¯„ä¼°\r\nif data['target'].dtype == 'object':  # åˆ†ç±»ä»»åŠ¡\r\n    accuracy = accuracy_score(y_test, y_pred.round())\r\n    print(f\"å‡†ç¡®ç‡: {accuracy:.4f}\")\r\nelse:  # å›å½’ä»»åŠ¡\r\n    mse = mean_squared_error(y_test, y_pred)\r\n    print(f\"MSE: {mse:.4f}\")\r\n\r\n# ç‰¹å¾é‡è¦æ€§\r\nimportance = model.get_score(importance_type='weight')\r\nprint(\"ç‰¹å¾é‡è¦æ€§:\", importance)\r\n\r\n# ä¿å­˜æ¨¡å‹\r\nmodel.save_model('/data/xgboost_model.json')\r\n```\r\n\r\n### scikit-optimize - è´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ–\r\n\r\n**ç”¨é€”**: è‡ªåŠ¨åŒ–è¶…å‚æ•°ä¼˜åŒ–  \r\n**ä¼˜åŠ¿**: æ¯”ç½‘æ ¼æœç´¢æ›´é«˜æ•ˆï¼Œæ‰¾åˆ°æ›´å¥½å‚æ•°ç»„åˆ  \r\n\r\n```python\r\nfrom skopt import BayesSearchCV\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nimport pandas as pd\r\n\r\n# å‡†å¤‡æ•°æ®\r\ndata = pd.read_csv('/data/train.csv')\r\nX = data.drop('target', axis=1)\r\ny = data['target']\r\n\r\n# å®šä¹‰å‚æ•°æœç´¢ç©ºé—´\r\nparam_space = {\r\n    'n_estimators': (50, 200),\r\n    'max_depth': (3, 10),\r\n    'min_samples_split': (2, 10),\r\n    'min_samples_leaf': (1, 4)\r\n}\r\n\r\n# è´å¶æ–¯ä¼˜åŒ–æœç´¢\r\nopt = BayesSearchCV(\r\n    RandomForestClassifier(),\r\n    param_space,\r\n    n_iter=50,\r\n    cv=5,\r\n    n_jobs=2  # é™åˆ¶å¹¶è¡Œçº¿ç¨‹\r\n)\r\n\r\nopt.fit(X, y)\r\nprint(f\"æœ€ä½³å‚æ•°: {opt.best_params_}\")\r\nprint(f\"æœ€ä½³åˆ†æ•°: {opt.best_score_:.4f}\")\r\n```\r\n\r\n## âš ï¸ ä½¿ç”¨æ³¨æ„äº‹é¡¹\r\n\r\n### âœ… æ¨èåšæ³•ï¼š\r\n- ä½¿ç”¨æ ‡å‡†çš„ scikit-learn å’Œ statsmodels æ¥å£\r\n- ç›´æ¥ä½¿ç”¨ `print()` è¾“å‡ºç»“æœå’ŒæŒ‡æ ‡\r\n- ä½¿ç”¨ `plt.show()` æ˜¾ç¤ºå›¾è¡¨\r\n- å¯¹æ•°æ®è¿›è¡Œé€‚å½“çš„é¢„å¤„ç†å’Œæ ‡å‡†åŒ–\r\n- æ—¶é—´åºåˆ—åˆ†æä¼˜å…ˆä½¿ç”¨pmdarimaè‡ªåŠ¨é€‰æ‹©ARIMAå‚æ•°\r\n- éçº¿æ€§å»ºæ¨¡ä¼˜å…ˆä½¿ç”¨XGBoostæˆ–LightGBM\r\n\r\n### âŒ é¿å…çš„æ“ä½œï¼š\r\n- ä¸è¦æ‰‹åŠ¨æ„å»º JSON è¾“å‡º\r\n- ä¸è¦ä½¿ç”¨ `base64` ç¼–ç \r\n- ä¸è¦åˆ›å»ºå¤æ‚çš„è‡ªå®šä¹‰è¾“å‡ºæ ¼å¼\r\n- ä¸è¦å¯¹æ˜æ˜¾å­£èŠ‚æ€§æ•°æ®ä½¿ç”¨éå­£èŠ‚æ€§ARIMA\r\n\r\n### ğŸ”§ é”™è¯¯å¤„ç†ï¼š\r\n```python\r\ntry:\r\n    from sklearn.ensemble import RandomForestRegressor\r\n    # æ¨¡å‹è®­ç»ƒä»£ç \r\nexcept ImportError:\r\n    print(\"scikit-learn ä¸å¯ç”¨\")\r\n\r\ntry:\r\n    import statsmodels.api as sm\r\n    # ç»Ÿè®¡å»ºæ¨¡ä»£ç \r\nexcept ImportError:\r\n    print(\"statsmodels ä¸å¯ç”¨\")\r\n\r\ntry:\r\n    import xgboost as xgb\r\n    # XGBoostä»£ç \r\nexcept ImportError:\r\n    print(\"XGBoost ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®\")\r\n\r\ntry:\r\n    import pmdarima as pm\r\n    # ARIMAä»£ç \r\nexcept ImportError:\r\n    print(\"pmdarima ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®\")\r\n```\r\n\r\n### ğŸ’¡ å®ç”¨æŠ€å·§ï¼š\r\n```python\r\n# å¿«é€Ÿæ¨¡å‹è¯„ä¼°å‡½æ•°\r\ndef quick_model_evaluation(model, X_test, y_test, problem_type='regression'):\r\n    \"\"\"å¿«é€Ÿæ¨¡å‹è¯„ä¼°\"\"\"\r\n    y_pred = model.predict(X_test)\r\n    \r\n    if problem_type == 'regression':\r\n        r2 = r2_score(y_test, y_pred)\r\n        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\r\n        print(f\"RÂ²: {r2:.4f}, RMSE: {rmse:.4f}\")\r\n    else:\r\n        accuracy = accuracy_score(y_test, y_pred)\r\n        print(f\"å‡†ç¡®ç‡: {accuracy:.4f}\")\r\n    \r\n    return y_pred\r\n\r\n# æ—¶é—´åºåˆ—åˆ†æå¿«é€Ÿæ¨¡æ¿\r\ndef quick_time_series_analysis(series, model_type='auto_arima'):\r\n    \"\"\"å¿«é€Ÿæ—¶é—´åºåˆ—åˆ†ææ¨¡æ¿\"\"\"\r\n    if model_type == 'auto_arima':\r\n        from pmdarima import auto_arima\r\n        model = auto_arima(series, seasonal=True, m=7, suppress_warnings=True)\r\n        forecast = model.predict(n_periods=30)\r\n    elif model_type == 'xgboost':\r\n        # ä½¿ç”¨time_series_xgboost_analysiså‡½æ•°\r\n        pass\r\n    \r\n    return model, forecast\r\n```\r\n\r\n**è®°ä½**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰è¾“å‡ºæ ¼å¼ï¼Œæ‚¨åªéœ€è¦ä¸“æ³¨äºæœºå™¨å­¦ä¹ å»ºæ¨¡å’Œåˆ†æé€»è¾‘ï¼\r\n\r\n## ğŸ“ æ²™ç›’ç¯å¢ƒæ–‡ä»¶æ“ä½œæŒ‡å—\r\n\r\n### æ–‡ä»¶ä¸Šä¼ ï¼ˆå¿…é¡»æ­¥éª¤ï¼‰\r\nåœ¨æ²™ç›’ä¸­è¿è¡Œä»£ç å‰ï¼Œ**å¿…é¡»å…ˆä¸Šä¼ æ•°æ®æ–‡ä»¶**ï¼š\r\n\r\n```python\r\n# ç¤ºä¾‹ï¼šå¦‚ä½•å¼•ç”¨å·²ä¸Šä¼ çš„æ–‡ä»¶\r\n# å‡è®¾æ‚¨å·²ç»é€šè¿‡å‰ç«¯ç•Œé¢ä¸Šä¼ äº†ä»¥ä¸‹æ–‡ä»¶ï¼š\r\n# - /data/train.csv      ï¼ˆé€šè¿‡æ–‡ä»¶ä¸Šä¼ APIä¸Šä¼ ï¼‰\r\n# - /data/dataset.xlsx   ï¼ˆé€šè¿‡æ–‡ä»¶ä¸Šä¼ APIä¸Šä¼ ï¼‰\r\n# - /data/sales.parquet  ï¼ˆé€šè¿‡æ–‡ä»¶ä¸Šä¼ APIä¸Šä¼ ï¼‰\r\n\r\nimport pandas as pd\r\nimport os\r\n\r\ndef list_uploaded_files():\r\n    \"\"\"åˆ—å‡ºæ‰€æœ‰å·²ä¸Šä¼ çš„æ–‡ä»¶\"\"\"\r\n    data_dir = '/data'\r\n    if os.path.exists(data_dir):\r\n        files = os.listdir(data_dir)\r\n        print(f\"å·²ä¸Šä¼ çš„æ–‡ä»¶: {files}\")\r\n        return files\r\n    else:\r\n        print(\"æ²¡æœ‰æ‰¾åˆ°/dataç›®å½•\")\r\n        return []\r\n\r\n# åˆ—å‡ºæ–‡ä»¶\r\navailable_files = list_uploaded_files()\r\n\r\n# è¯»å–ç‰¹å®šæ–‡ä»¶\r\nif 'train.csv' in available_files:\r\n    df = pd.read_csv('/data/train.csv')\r\n    print(f\"æˆåŠŸè¯»å– train.csvï¼Œå½¢çŠ¶: {df.shape}\")\r\n    \r\nif 'dataset.xlsx' in available_files:\r\n    df = pd.read_excel('/data/dataset.xlsx')\r\n    print(f\"æˆåŠŸè¯»å– dataset.xlsxï¼Œå½¢çŠ¶: {df.shape}\")\r\n```\r\n\r\n### æ”¯æŒçš„æ–‡ä»¶æ ¼å¼\r\næ ¹æ®code_interpreter.pyï¼Œç³»ç»Ÿæ”¯æŒä»¥ä¸‹æ–‡ä»¶æ ¼å¼ï¼š\r\n- ğŸ“Š æ•°æ®æ–‡ä»¶ï¼š`.csv`, `.xlsx`, `.xls`, `.parquet`, `.json`\r\n\r\n### æ–‡ä»¶è¯»å–æœ€ä½³å®è·µ\r\n```python\r\ndef safe_read_data(filename):\r\n    \"\"\"å®‰å…¨è¯»å–æ•°æ®æ–‡ä»¶ï¼Œå¸¦é”™è¯¯å¤„ç†\"\"\"\r\n    try:\r\n        filepath = f'/data/{filename}'\r\n        \r\n        # æ ¹æ®æ‰©å±•åé€‰æ‹©è¯»å–æ–¹æ³•\r\n        if filename.endswith('.csv'):\r\n            df = pd.read_csv(filepath)\r\n        elif filename.endswith('.parquet'):\r\n            df = pd.read_parquet(filepath)\r\n        elif filename.endswith(('.xlsx', '.xls')):\r\n            df = pd.read_excel(filepath)\r\n        elif filename.endswith('.json'):\r\n            df = pd.read_json(filepath)\r\n        else:\r\n            raise ValueError(f\"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {filename}\")\r\n        \r\n        print(f\"âœ… æˆåŠŸè¯»å– {filename}\")\r\n        print(f\"   è¡Œæ•°: {len(df)}, åˆ—æ•°: {len(df.columns)}\")\r\n        print(f\"   åˆ—å: {list(df.columns)}\")\r\n        \r\n        return df\r\n        \r\n    except FileNotFoundError:\r\n        print(f\"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filename}\")\r\n        print(\"è¯·å…ˆé€šè¿‡æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½ä¸Šä¼ æ–‡ä»¶\")\r\n        return None\r\n    except Exception as e:\r\n        print(f\"âŒ è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}\")\r\n        return None\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\nif __name__ == \"__main__\":\r\n    # æ£€æŸ¥å¯ç”¨çš„æ–‡ä»¶\r\n    files = list_uploaded_files()\r\n    if files:\r\n        for file in files:\r\n            print(f\"å‘ç°æ–‡ä»¶: {file}\")\r\n        \r\n        # è¯»å–ç¬¬ä¸€ä¸ªCSVæ–‡ä»¶\r\n        csv_files = [f for f in files if f.endswith('.csv')]\r\n        if csv_files:\r\n            df = safe_read_data(csv_files[0])\r\n            if df is not None:\r\n                # è¿›è¡Œæœºå™¨å­¦ä¹ åˆ†æ\r\n                pass\r\n```\r\n\r\n### å·¥ä½œæµæ•´åˆç¤ºä¾‹\r\n```python\r\n# å®Œæ•´çš„MLå·¥ä½œæµï¼ŒåŒ…å«æ–‡ä»¶æ£€æŸ¥\r\ndef complete_ml_workflow_with_file_check():\r\n    \"\"\"åŒ…å«æ–‡ä»¶æ£€æŸ¥çš„å®Œæ•´MLå·¥ä½œæµ\"\"\"\r\n    \r\n    print(\"=== æœºå™¨å­¦ä¹ å·¥ä½œæµå¼€å§‹ ===\")\r\n    \r\n    # 1. æ£€æŸ¥æ•°æ®æ–‡ä»¶\r\n    files = list_uploaded_files()\r\n    if not files:\r\n        print(\"è­¦å‘Šï¼šæ²¡æœ‰æ‰¾åˆ°ä¸Šä¼ çš„æ–‡ä»¶ï¼Œå°†ä½¿ç”¨ç¤ºä¾‹æ•°æ®\")\r\n        # ä½¿ç”¨generate_sample_data()å‡½æ•°åˆ›å»ºç¤ºä¾‹æ•°æ®\r\n        from sklearn.datasets import make_regression\r\n        X, y = make_regression(n_samples=1000, n_features=10, random_state=42)\r\n    else:\r\n        print(f\"æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶: {files}\")\r\n        \r\n        # è¯»å–ç¬¬ä¸€ä¸ªæ•°æ®æ–‡ä»¶\r\n        data_file = files[0]\r\n        df = safe_read_data(data_file)\r\n        \r\n        if df is None:\r\n            print(\"æ— æ³•è¯»å–æ–‡ä»¶ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®\")\r\n            from sklearn.datasets import make_regression\r\n            X, y = make_regression(n_samples=1000, n_features=10, random_state=42)\r\n        else:\r\n            # å‡è®¾æœ€åä¸€åˆ—æ˜¯ç›®æ ‡å˜é‡\r\n            X = df.iloc[:, :-1].values\r\n            y = df.iloc[:, -1].values\r\n    \r\n    # 2. æ‰§è¡ŒMLåˆ†æï¼ˆä½¿ç”¨æ–‡æ¡£ä¸­çš„å‡½æ•°ï¼‰\r\n    results = standard_ml_pipeline(X, y, problem_type='regression')\r\n    \r\n    return results\r\n```\r\n\r\n### âš¡ å¿«é€Ÿä½¿ç”¨æ¨¡æ¿\r\n```python\r\n# åœ¨æ²™ç›’ä¸­è¿è¡Œæœºå™¨å­¦ä¹ åˆ†æçš„å®Œæ•´ç¤ºä¾‹\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.ensemble import RandomForestRegressor\r\nfrom sklearn.metrics import mean_squared_error, r2_score\r\nimport matplotlib.pyplot as plt\r\n\r\n# æ­¥éª¤1ï¼šè¯»å–æ•°æ®ï¼ˆæ›¿æ¢ä¸ºæ‚¨çš„æ–‡ä»¶åï¼‰\r\ntry:\r\n    # å¦‚æœæ‚¨ä¸Šä¼ äº†train.csv\r\n    df = pd.read_csv('/data/train.csv')\r\n    print(f\"æ•°æ®å½¢çŠ¶: {df.shape}\")\r\n    \r\n    # æ­¥éª¤2ï¼šå‡†å¤‡ç‰¹å¾å’Œç›®æ ‡\r\n    X = df.drop('target_column', axis=1)  # æ›¿æ¢ä¸ºæ‚¨çš„ç›®æ ‡åˆ—å\r\n    y = df['target_column']\r\n    \r\n    # æ­¥éª¤3ï¼šè®­ç»ƒæ¨¡å‹\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\r\n    \r\n    model = RandomForestRegressor(n_estimators=100, random_state=42)\r\n    model.fit(X_train, y_train)\r\n    \r\n    # æ­¥éª¤4ï¼šè¯„ä¼°\r\n    y_pred = model.predict(X_test)\r\n    r2 = r2_score(y_test, y_pred)\r\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\r\n    \r\n    print(f\"æ¨¡å‹æ€§èƒ½: RÂ²={r2:.4f}, RMSE={rmse:.4f}\")\r\n    \r\n    # æ­¥éª¤5ï¼šå¯è§†åŒ–\r\n    plt.figure(figsize=(10, 5))\r\n    plt.scatter(y_test, y_pred, alpha=0.6)\r\n    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\r\n    plt.xlabel('å®é™…å€¼')\r\n    plt.ylabel('é¢„æµ‹å€¼')\r\n    plt.title(f'é¢„æµ‹æ•ˆæœ (RÂ² = {r2:.3f})')\r\n    plt.grid(True, alpha=0.3)\r\n    plt.show()\r\n    \r\nexcept FileNotFoundError:\r\n    print(\"âŒ æœªæ‰¾åˆ°æ–‡ä»¶ã€‚è¯·ç¡®ä¿ï¼š\")\r\n    print(\"   1. å·²é€šè¿‡æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½ä¸Šä¼ train.csv\")\r\n    print(\"   2. æ–‡ä»¶ä½äº/dataç›®å½•ä¸‹\")\r\n    print(\"   3. æ–‡ä»¶åæ‹¼å†™æ­£ç¡®\")\r\n    \r\n    # æä¾›ç¤ºä¾‹æ•°æ®ä½œä¸ºå¤‡é€‰\r\n    print(\"\\nğŸ”§ æ­£åœ¨ç”Ÿæˆç¤ºä¾‹æ•°æ®è¿›è¡Œåˆ†æ...\")\r\n    from sklearn.datasets import make_regression\r\n    X, y = make_regression(n_samples=1000, n_features=5, random_state=42)\r\n    \r\n    # ç»§ç»­æ‰§è¡Œåˆ†æ...\r\n```\r\n",
        "pandas_cheatsheet.md": "# ä»£ç è§£é‡Šå™¨ä½¿ç”¨æŒ‡å— v2.5 (æœ€ç»ˆèåˆç‰ˆ)\n\n## ğŸ¯ æ ¸å¿ƒåŸåˆ™ï¼šåç«¯è‡ªåŠ¨åŒ–ï¼Œä»£ç è¦ç®€æ´\n\n### âœ… **åç«¯å·²è‡ªåŠ¨å¤„ç†çš„åŠŸèƒ½ï¼š**\n1. **å›¾è¡¨æ•è·**ï¼š`plt.show()` è‡ªåŠ¨ç”Ÿæˆå›¾ç‰‡ï¼Œæ— éœ€æ‰‹åŠ¨ç¼–ç \n2. **æ–‡ä»¶ç®¡ç†**ï¼š`/data` ç›®å½•å·²é…ç½®å¥½ï¼Œæ”¯æŒä¼šè¯æŒä¹…åŒ–\n3. **è¾“å‡ºå¤„ç†**ï¼šç³»ç»Ÿè‡ªåŠ¨å¤„ç†æ‰€æœ‰ `print()` è¾“å‡º\n4. **é”™è¯¯æ•è·**ï¼šåç«¯æœ‰å®Œæ•´çš„é”™è¯¯å¤„ç†ç³»ç»Ÿ\n\n### âš ï¸ **èµ„æºé™åˆ¶ï¼š**\n1. **å†…å­˜é™åˆ¶**ï¼šå¯ç”¨å†…å­˜ä¸Šé™ä¸º6GB\n2. **æ—¶é—´é™åˆ¶**ï¼šä»£ç æ‰§è¡Œæœ‰90ç§’è¶…æ—¶é™åˆ¶\n\n### âŒ **æ¨¡å‹ä¸éœ€è¦åšçš„ï¼š**\n1. ä¸è¦æ‰‹åŠ¨ç¼–ç å›¾è¡¨ä¸º base64\n2. ä¸è¦ç¼–å†™å¤æ‚çš„é”™è¯¯å¤„ç†åŒ…è£…å™¨\n3. ä¸è¦ç®¡ç†æ–‡ä»¶æ ¼å¼è½¬æ¢ï¼ˆåç«¯è‡ªåŠ¨å¤„ç†ï¼‰\n4. ä¸è¦å¤„ç†å›¾è¡¨æ ‡é¢˜å’Œæ ¼å¼ï¼ˆç³»ç»Ÿè‡ªåŠ¨ä¼˜åŒ–ï¼‰\n\n---\n\n## ğŸ“‚ æ–‡ä»¶æ“ä½œï¼ˆä¼šè¯å·¥ä½œåŒºï¼š`/data`ï¼‰\n\n### ä»å·¥ä½œåŒºè¯»å–æ–‡ä»¶\n```python\nimport pandas as pd\n\n# æœ€ç®€å•çš„æ–‡ä»¶è¯»å–ï¼ˆæ”¯æŒ CSVã€Excelã€Parquet ç­‰ï¼‰\ndf = pd.read_csv('/data/your_file.csv')\n\n# å¿«é€ŸæŸ¥çœ‹æ•°æ®\nprint(f\"æ•°æ®å½¢çŠ¶: {df.shape}\")\nprint(df.head())\n```\n\n### ä¿å­˜æ–‡ä»¶åˆ°å·¥ä½œåŒº\n```python\n# ä¿å­˜å¤„ç†ç»“æœ\ndf_processed.to_csv('/data/processed_data.csv', index=False)\n\n# ä¿å­˜ä¸ºé«˜æ•ˆæ ¼å¼ï¼ˆä¾›åç»­ä½¿ç”¨ï¼‰\nimport pyarrow.feather as feather\nfeather.write_feather(df_processed, '/data/processed_data.feather')\n```\n\n### ğŸ“ é‡è¦è¯´æ˜\n- **æ–‡ä»¶ä½ç½®**ï¼šæ‰€æœ‰æ–‡ä»¶éƒ½åœ¨ `/data` ç›®å½•ä¸‹\n- **ä¼šè¯æŒä¹…**ï¼šæ–‡ä»¶åœ¨åŒä¸€ä¼šè¯çš„å¤šæ¬¡æ‰§è¡Œä¸­ä¿æŒå¯ç”¨\n- **è‡ªåŠ¨æ¸…ç†**ï¼š24å°æ—¶åä¼šè¯æ–‡ä»¶è‡ªåŠ¨æ¸…ç†\n\n---\n\n## ğŸ“Š æ•°æ®å¯è§†åŒ–ï¼ˆè‡ªåŠ¨æ•è·ï¼‰\n\n### åŸºç¡€å›¾è¡¨\n```python\nimport matplotlib.pyplot as plt\n\n# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆåç«¯å·²é…ç½®ï¼Œè¿™é‡Œåªæ˜¯ç¡®ä¿ï¼‰\nplt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei']\n\n# 1. æŠ˜çº¿å›¾\nplt.figure(figsize=(10, 6))\nplt.plot(df['date'], df['value'], marker='o', linewidth=2)\nplt.title('é”€å”®è¶‹åŠ¿å›¾')  # æ ‡é¢˜ä¼šè¢«è‡ªåŠ¨æ•è·\nplt.xlabel('æ—¥æœŸ')\nplt.ylabel('é”€å”®é¢')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()  # ğŸ¯ å…³é”®ï¼šç›´æ¥ show()ï¼Œç³»ç»Ÿè‡ªåŠ¨æ•è·ï¼\n\n# 2. æŸ±çŠ¶å›¾\nplt.figure(figsize=(10, 6))\ndf.groupby('category')['sales'].sum().plot(kind='bar')\nplt.title('å„å“ç±»é”€å”®é¢')\nplt.tight_layout()\nplt.show()  # ğŸ¯ å…³é”®ï¼šç³»ç»Ÿè‡ªåŠ¨å¤„ç†ï¼\n\n# 3. æ•£ç‚¹å›¾\nplt.figure(figsize=(10, 6))\nplt.scatter(df['x'], df['y'], alpha=0.6, c=df['value'], cmap='viridis')\nplt.title('æ•£ç‚¹åˆ†å¸ƒå›¾')\nplt.colorbar(label='å€¼å¤§å°')\nplt.tight_layout()\nplt.show()  # ğŸ¯ å…³é”®ï¼šç³»ç»Ÿè‡ªåŠ¨æ•è·ï¼\n```\n\n### é«˜çº§å›¾è¡¨\n```python\n# 4. å­å›¾ï¼ˆå¤šå›¾è¡¨ï¼‰\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\naxes[0, 0].plot(df['date'], df['value1'])\naxes[0, 0].set_title('å›¾è¡¨1')\n\naxes[0, 1].hist(df['value2'], bins=30)\naxes[0, 1].set_title('å›¾è¡¨2')\n\naxes[1, 0].scatter(df['x'], df['y'])\naxes[1, 0].set_title('å›¾è¡¨3')\n\naxes[1, 1].boxplot([df['group1'], df['group2']])\naxes[1, 1].set_title('å›¾è¡¨4')\n\nplt.tight_layout()\nplt.show()  # ğŸ¯ ç³»ç»Ÿè‡ªåŠ¨æ•è·æ•´ä¸ªå›¾å½¢ï¼\n\n# 5. çƒ­åŠ›å›¾ï¼ˆç›¸å…³æ€§çŸ©é˜µï¼‰\nimport seaborn as sns\n\ncorr_matrix = df.corr()\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\nplt.title('ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾')\nplt.tight_layout()\nplt.show()  # ğŸ¯ ç³»ç»Ÿè‡ªåŠ¨æ•è·ï¼\n```\n\n### ğŸ“ å›¾è¡¨è¯´æ˜\n- **åç«¯è‡ªåŠ¨å¤„ç†**ï¼šæ‰€æœ‰å›¾è¡¨ç±»å‹ï¼ˆMatplotlibã€Seabornã€Graphvizã€NetworkXï¼‰\n- **æ ‡é¢˜æ•è·**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨æå–å›¾è¡¨æ ‡é¢˜æ˜¾ç¤ºç»™ç”¨æˆ·\n- **ä¸­æ–‡å­—ä½“**ï¼šåç«¯å·²é…ç½®ä¸­æ–‡æ”¯æŒï¼Œæ— éœ€æ‹…å¿ƒä¹±ç \n\n---\n\n## ğŸ§¹ æ•°æ®å¤„ç†ï¼ˆç®€æ´å®ç”¨ç‰ˆï¼‰\n\n### åŸºç¡€æ¸…æ´—\n```python\nimport pandas as pd\nimport numpy as np\n\n# è¯»å–æ•°æ®\ndf = pd.read_csv('/data/raw_data.csv')\n\n# æ‰“å°åŸºæœ¬ä¿¡æ¯\nprint(f\"åŸå§‹æ•°æ®: {df.shape[0]}è¡Œ Ã— {df.shape[1]}åˆ—\")\nprint(f\"ç¼ºå¤±å€¼æ€»æ•°: {df.isnull().sum().sum()}\")\n\n# å¤„ç†ç¼ºå¤±å€¼ï¼ˆæ•°å€¼åˆ—ç”¨ä¸­ä½æ•°ï¼‰\nnumeric_cols = df.select_dtypes(include=[np.number]).columns\nfor col in numeric_cols:\n    if df[col].isnull().any():\n        df[col].fillna(df[col].median(), inplace=True)\n\n# å¤„ç†ç¼ºå¤±å€¼ï¼ˆæ–‡æœ¬åˆ—ç”¨ä¼—æ•°ï¼‰\ntext_cols = df.select_dtypes(include=['object']).columns\nfor col in text_cols:\n    if df[col].isnull().any():\n        if not df[col].mode().empty:\n            df[col].fillna(df[col].mode()[0], inplace=True)\n\n# åˆ é™¤é‡å¤è¡Œ\ndf = df.drop_duplicates()\nprint(f\"æ¸…æ´—åæ•°æ®: {df.shape[0]}è¡Œ Ã— {df.shape[1]}åˆ—\")\n```\n\n### ç»Ÿè®¡åˆ†æ\n```python\n# åŸºç¡€ç»Ÿè®¡\nprint(\"æ•°å€¼åˆ—ç»Ÿè®¡:\")\nprint(df.describe())\n\n# åˆ†ç»„ç»Ÿè®¡\nprint(\"\\nåˆ†ç»„ç»Ÿè®¡:\")\ngroup_stats = df.groupby('category').agg({\n    'value': ['mean', 'sum', 'count', 'std']\n}).round(2)\nprint(group_stats)\n\n# é€è§†è¡¨\nprint(\"\\né€è§†è¡¨:\")\npivot = pd.pivot_table(df, \n                      values='sales', \n                      index='region', \n                      columns='month',\n                      aggfunc='sum')\nprint(pivot)\n```\n\n---\n\n## ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼ˆé’ˆå¯¹å¤§æ–‡ä»¶ï¼‰\n\n### æ–¹æ³•1ï¼šDuckDBï¼ˆSQLæŸ¥è¯¢ï¼Œæ¯”Pandaså¿«3-10å€ï¼‰\n```python\nimport duckdb\n\n# ç›´æ¥æŸ¥è¯¢CSV/Parquetæ–‡ä»¶ï¼ˆä¸åŠ è½½åˆ°å†…å­˜ï¼‰\nresult = duckdb.sql(\"\"\"\n    SELECT department, \n           AVG(salary) as avg_salary,\n           COUNT(*) as employee_count\n    FROM read_csv_auto('/data/employees.csv')\n    WHERE department IS NOT NULL\n    GROUP BY department\n    ORDER BY avg_salary DESC\n\"\"\").df()\n\nprint(\"éƒ¨é—¨è–ªèµ„ç»Ÿè®¡:\")\nprint(result)\n```\n\n### æ–¹æ³•2ï¼šåˆ†å—å¤„ç†ï¼ˆå¤§CSVæ–‡ä»¶ï¼‰\n```python\n# åˆ†å—è¯»å–å¤§æ–‡ä»¶\nchunks = []\nfor chunk in pd.read_csv('/data/large_file.csv', chunksize=50000):\n    # å¤„ç†æ¯ä¸ªæ•°æ®å—\n    processed = chunk[chunk['value'] > 0]  # ç¤ºä¾‹ç­›é€‰\n    chunks.append(processed)\n\n# åˆå¹¶ç»“æœ\nfinal_df = pd.concat(chunks, ignore_index=True)\nprint(f\"å¤„ç†å®Œæˆ: {len(final_df)}è¡Œ\")\n```\n\n### æ–¹æ³•3ï¼šé«˜æ•ˆæ ¼å¼è½¬æ¢\n```python\n# å°†CSVè½¬æ¢ä¸ºFeatheræ ¼å¼ï¼ˆæé€Ÿ10-100å€ï¼‰\nimport pyarrow.feather as feather\n\ndf = pd.read_csv('/data/large.csv')\nfeather.write_feather(df, '/data/large.feather')\n\n# ä¸‹æ¬¡è¯»å–æ—¶ï¼ˆæé€Ÿï¼‰\ndf_fast = feather.read_feather('/data/large.feather')\n```\n\n---\n\n## ğŸ’¡ å®ç”¨ä»£ç ç‰‡æ®µ\n\n### æ¨¡æ¿1ï¼šåŸºç¡€åˆ†æ\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# 1. è¯»å–æ•°æ®\ndf = pd.read_csv('/data/data.csv')\n\n# 2. å¿«é€Ÿåˆ†æ\nprint(f\"æ•°æ®å½¢çŠ¶: {df.shape}\")\nprint(df.describe())\n\n# 3. ç®€å•å¯è§†åŒ–\ndf.groupby('category')['value'].mean().plot(kind='bar')\nplt.title('å„åˆ†ç±»å¹³å‡å€¼')\nplt.tight_layout()\nplt.show()\n```\n\n### æ¨¡æ¿2ï¼šæ•°æ®æ¸…æ´—æµæ°´çº¿\n```python\n# 1. è¯»å–\ndf = pd.read_csv('/data/raw.csv')\n\n# 2. æ¸…æ´—\ndf = df.dropna().drop_duplicates()\n\n# 3. åˆ†æ\nprint(f\"æ¸…æ´—å: {df.shape}\")\nprint(df.groupby('group')['value'].mean())\n\n# 4. ä¿å­˜\ndf.to_csv('/data/cleaned.csv', index=False)\n```\n\n### æ¨¡æ¿3ï¼šå®Œæ•´æŠ¥å‘Šç”Ÿæˆ\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n# æ·»åŠ èµ„æºä½¿ç”¨æç¤º\nprint(f\"å¯ç”¨å†…å­˜é™åˆ¶: 6GB\")\nprint(f\"å»ºè®®å¤§æ–‡ä»¶å¤„ç†: ä½¿ç”¨åˆ†å—æˆ–DuckDB\")\nprint(\"æ³¨æ„ï¼šä»£ç æ‰§è¡Œæœ‰90ç§’è¶…æ—¶é™åˆ¶ï¼Œå¤æ‚è®¡ç®—è¯·ä¼˜åŒ–\")\n\nprint(\"=\" * 50)\nprint(f\"æ•°æ®åˆ†ææŠ¥å‘Š - {datetime.now().strftime('%Y-%m-%d')}\")\nprint(\"=\" * 50)\n\n# 1. æ•°æ®æ¦‚è§ˆ\ndf = pd.read_csv('/data/sales.csv')\nprint(f\"æ•°æ®é›†: {df.shape[0]}è¡Œ Ã— {df.shape[1]}åˆ—\")\nprint(f\"æ—¶é—´èŒƒå›´: {df['date'].min()} è‡³ {df['date'].max()}\")\n\n# 2. å…³é”®æŒ‡æ ‡\ntotal_sales = df['amount'].sum()\navg_sale = df['amount'].mean()\nprint(f\"\\nå…³é”®æŒ‡æ ‡:\")\nprint(f\"  æ€»é”€å”®é¢: Â¥{total_sales:,.2f}\")\nprint(f\"  å¹³å‡äº¤æ˜“é¢: Â¥{avg_sale:,.2f}\")\n\n# 3. å¯è§†åŒ–\nplt.figure(figsize=(12, 5))\n\n# é”€å”®é¢è¶‹åŠ¿\nplt.subplot(1, 2, 1)\ndf.groupby('date')['amount'].sum().plot()\nplt.title('æ¯æ—¥é”€å”®é¢')\nplt.grid(True, alpha=0.3)\n\n# å“ç±»åˆ†å¸ƒ\nplt.subplot(1, 2, 2)\ndf['category'].value_counts().head(10).plot(kind='bar')\nplt.title('Top 10 å“ç±»')\nplt.xticks(rotation=45)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nâœ… åˆ†æå®Œæˆï¼\")\n```\n\n---\n\n## âš ï¸ é‡è¦æé†’ï¼ˆåŸºäºåç«¯ç‰¹æ€§ï¼‰\n\n### åç«¯å·²é…ç½®ï¼Œæ— éœ€æ‹…å¿ƒï¼š\n1. **ä¸­æ–‡å­—ä½“**ï¼šå·²å®‰è£… WenQuanYi å­—ä½“ï¼Œå›¾è¡¨æ— ä¹±ç \n2. **å›¾è¡¨æ•è·**ï¼šæ‰€æœ‰ `plt.show()` è‡ªåŠ¨è½¬æ¢ä¸ºå›¾ç‰‡\n3. **å†…å­˜ç®¡ç†**ï¼šå®¹å™¨é™åˆ¶ 6GBï¼Œè‡ªåŠ¨å¤„ç†å†…å­˜æº¢å‡º\n4. **æ–‡ä»¶æƒé™**ï¼š`/data` ç›®å½•æœ‰è¯»å†™æƒé™\n\n### ä»£ç æ‰§è¡Œé™åˆ¶ï¼š\n1. **å†…å­˜é™åˆ¶**ï¼šå¯ç”¨å†…å­˜ä¸Šé™ä¸º6GBï¼Œå¤„ç†å¤§æ–‡ä»¶æ—¶å»ºè®®ä½¿ç”¨åˆ†å—å¤„ç†æˆ–DuckDB\n2. **è¶…æ—¶é™åˆ¶**ï¼šä»£ç æ‰§è¡Œæœ‰90ç§’è¶…æ—¶é™åˆ¶ï¼Œå¤æ‚è®¡ç®—è¯·ä¼˜åŒ–ç®—æ³•æˆ–åˆ†æ­¥æ‰§è¡Œ\n\n### ä»£ç ç¼–å†™åŸåˆ™ï¼š\n1. **ä¿æŒç®€æ´**ï¼šå†™ç›´ç™½çš„ Python ä»£ç ï¼Œæ— éœ€å¤æ‚åŒ…è£…\n2. **ç›¸ä¿¡åç«¯**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†å›¾è¡¨ã€é”™è¯¯ã€è¾“å‡ºæ ¼å¼\n3. **ä½¿ç”¨æ ‡å‡†åº“**ï¼šPandasã€Matplotlibã€NumPy ç­‰å·²é¢„è£…\n4. **å…³æ³¨ä¸šåŠ¡é€»è¾‘**ï¼šè®©åç«¯å¤„ç†æŠ€æœ¯ç»†èŠ‚\n\n---\n\n## ğŸ”§ æ•…éšœæ’é™¤\n\n### å¸¸è§é—®é¢˜ï¼š\n1. **æ–‡ä»¶ä¸å­˜åœ¨**ï¼šæ£€æŸ¥æ–‡ä»¶åæ˜¯å¦æ­£ç¡®ï¼Œæ³¨æ„å¤§å°å†™\n2. **å†…å­˜ä¸è¶³**ï¼šä½¿ç”¨åˆ†å—å¤„ç†æˆ– DuckDB æŸ¥è¯¢\n3. **å›¾è¡¨ä¸æ˜¾ç¤º**ï¼šç¡®ä¿è°ƒç”¨äº† `plt.show()`\n4. **ä¸­æ–‡ä¹±ç **ï¼šåç«¯å·²é…ç½®å­—ä½“ï¼Œæ— éœ€é¢å¤–å¤„ç†\n5. **æ‰§è¡Œè¶…æ—¶**ï¼šä»£ç æ‰§è¡Œè¶…è¿‡90ç§’é™åˆ¶ï¼Œè¯·ä¼˜åŒ–ç®—æ³•æˆ–åˆ†æ­¥æ‰§è¡Œ\n\n### æ€§èƒ½å»ºè®®ï¼š\n- **å°æ–‡ä»¶**ï¼šç›´æ¥ä½¿ç”¨ Pandas\n- **å¤§æ–‡ä»¶**ï¼šä½¿ç”¨ DuckDB æˆ–åˆ†å—å¤„ç†\n- **é‡å¤è®¡ç®—**ï¼šä¿å­˜ä¸­é—´ç»“æœåˆ° `/data` ç›®å½•\n- **å¤æ‚å›¾è¡¨**ï¼šåç«¯ä¼šè‡ªåŠ¨ä¼˜åŒ–æ¸²æŸ“\n- **å†…å­˜ä½¿ç”¨**ï¼šå¯ç”¨å†…å­˜é™åˆ¶ä¸º6GBï¼Œå¤„ç†å¤§å‹æ•°æ®é›†æ—¶è¯·æ³¨æ„ä½¿ç”¨åˆ†å—æˆ–DuckDBä»¥é™ä½å†…å­˜å ç”¨\n- **æ‰§è¡Œæ—¶é—´**ï¼šä»£ç æ‰§è¡Œæœ‰90ç§’è¶…æ—¶é™åˆ¶ï¼Œå¯¹äºå¤æ‚è®¡ç®—å»ºè®®ä¼˜åŒ–ç®—æ³•æˆ–åˆ†è§£ä¸ºå¤šä¸ªæ­¥éª¤æ‰§è¡Œ\n\n---\n\n## ğŸ“‹ å¿«é€Ÿå‚è€ƒå¡\n\n```python\n# è¯»å–æ–‡ä»¶\ndf = pd.read_csv('/data/file.csv')\n\n# ä¿å­˜æ–‡ä»¶\ndf.to_csv('/data/output.csv', index=False)\n\n# æ˜¾ç¤ºå›¾è¡¨\nplt.plot(x, y)\nplt.show()  # ğŸ¯ å…³é”®ï¼\n\n# æ‰“å°ç»“æœ\nprint(df.describe())\n\n# é«˜æ•ˆæŸ¥è¯¢ï¼ˆå¤§æ–‡ä»¶ï¼‰\nimport duckdb\nresult = duckdb.sql(\"SELECT * FROM read_csv_auto('/data/big.csv')\").df()\n\n# æ·»åŠ èµ„æºä½¿ç”¨æç¤º\nprint(f\"å¯ç”¨å†…å­˜é™åˆ¶: 6GB\")\nprint(f\"å»ºè®®å¤§æ–‡ä»¶å¤„ç†: ä½¿ç”¨åˆ†å—æˆ–DuckDB\")\nprint(\"æ³¨æ„ï¼šä»£ç æ‰§è¡Œæœ‰90ç§’è¶…æ—¶é™åˆ¶ï¼Œå¤æ‚è®¡ç®—è¯·ä¼˜åŒ–\")\n```\n\n---\n\n**æœ€ç»ˆåŸåˆ™**ï¼šå†™ä½ **æƒ³å†™**çš„ä»£ç ï¼Œåç«¯ä¼šå¤„ç†**è¯¥å¤„ç†**çš„ç»†èŠ‚ï¼å›¾è¡¨ã€æ–‡ä»¶ã€è¾“å‡ºéƒ½äº¤ç»™ç³»ç»Ÿï¼Œä½ åªéœ€è¦å…³æ³¨æ•°æ®åˆ†æå’Œä¸šåŠ¡é€»è¾‘ã€‚",
        "report_generator_workflow.md": "# è‡ªåŠ¨åŒ–æŠ¥å‘Šç”ŸæˆæŒ‡å— (v3.0 - å®Œæ•´ç‰ˆ)\r\n\r\n## ğŸš€ æ ¸å¿ƒè¾“å‡ºåè®® (å¼ºåˆ¶éµå¾ª)\r\n\r\n**é‡è¦æç¤º**: è¦ç”Ÿæˆä¸€ä¸ªå¯ä¾›ç”¨æˆ·ä¸‹è½½çš„æ–‡ä»¶ï¼ˆWord, Excel, PDF, PPTç­‰ï¼‰ï¼Œä½ çš„Pythonä»£ç **å¿…é¡»**å°†æ–‡ä»¶å†…å®¹è¿›è¡ŒBase64ç¼–ç ï¼Œå¹¶å°†å…¶åŒ…è£¹åœ¨ä¸€ä¸ªç‰¹å®šæ ¼å¼çš„JSONå¯¹è±¡ä¸­ï¼Œç„¶å `print` è¿™ä¸ªJSONå¯¹è±¡ã€‚\r\n\r\n**å·¥ä½œæµ**:\r\n1. **å¯¼å…¥å¿…è¦åº“**: `io`, `base64`, `json`ã€‚\r\n2. **åœ¨å†…å­˜ä¸­åˆ›å»ºæ–‡ä»¶**: ä½¿ç”¨ `io.BytesIO()` åˆ›å»ºä¸€ä¸ªå†…å­˜ç¼“å†²åŒºã€‚\r\n3. **ä¿å­˜åˆ°å†…å­˜**: è°ƒç”¨ç›¸åº”åº“çš„ `.save(buffer)` æ–¹æ³•å°†æ–‡ä»¶å†…å®¹å†™å…¥å†…å­˜ç¼“å†²åŒºã€‚\r\n4. **ç¼–ç **: å°†ç¼“å†²åŒºä¸­çš„äºŒè¿›åˆ¶æ•°æ®ç¼–ç ä¸ºBase64å­—ç¬¦ä¸²ã€‚\r\n5. **æ‰“åŒ…å¹¶æ‰“å°**: æ„å»ºä¸€ä¸ªåŒ…å« `type` å’Œ `data_base64` å­—æ®µçš„å­—å…¸ï¼Œå¹¶ä½¿ç”¨ `json.dumps()` æ‰“å°å‡ºæ¥ã€‚\r\n\r\n**JSONæ ¼å¼è§„èŒƒ**:\r\n```json\r\n{\r\n    \"type\": \"æ–‡ä»¶ç±»å‹\",  // å¿…é¡»æ˜¯ï¼šword, excel, pdf, ppt ä¹‹ä¸€\r\n    \"title\": \"æ–‡ä»¶å.åç¼€\",\r\n    \"data_base64\": \"Base64ç¼–ç çš„äºŒè¿›åˆ¶æ•°æ®\"\r\n}\r\n```\r\n\r\n---\r\n\r\n## ğŸ“Š Word æŠ¥å‘Šç”Ÿæˆ (.docx)\r\n\r\n### âœ… å¯ç›´æ¥ä½¿ç”¨çš„ä»£ç æ¨¡æ¿\r\n```python\r\nimport io\r\nimport base64\r\nimport json\r\nfrom docx import Document\r\nfrom docx.shared import Inches, Pt\r\nfrom docx.enum.text import WD_ALIGN_PARAGRAPH\r\nfrom docx.enum.table import WD_TABLE_ALIGNMENT\r\nfrom datetime import datetime\r\n\r\n# --- 1. åœ¨å†…å­˜ä¸­æ„å»º Word æ–‡æ¡£ ---\r\ndoc = Document()\r\ndoc.add_heading('ä¸šåŠ¡åˆ†ææŠ¥å‘Š', 0)\r\n\r\n# æ·»åŠ æŠ¥å‘Šä¿¡æ¯\r\np = doc.add_paragraph()\r\np.add_run(f'ç”Ÿæˆæ—¶é—´: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}').bold = True\r\ndoc.add_paragraph('è¿™æ˜¯ä¸€ä¸ªç”±ä»£ç è§£é‡Šå™¨ç”Ÿæˆçš„Wordæ–‡æ¡£ç¤ºä¾‹ã€‚')\r\n\r\n# æ·»åŠ è¡¨æ ¼\r\ntable = doc.add_table(rows=3, cols=3)\r\ntable.style = 'Light Grid Accent 1'\r\ntable.alignment = WD_TABLE_ALIGNMENT.CENTER\r\n\r\n# è®¾ç½®è¡¨å¤´\r\nheader_cells = table.rows[0].cells\r\nheader_cells[0].text = 'é¡¹ç›®'\r\nheader_cells[1].text = 'é¢„ç®—(å…ƒ)'\r\nheader_cells[2].text = 'å®é™…æ”¯å‡º(å…ƒ)'\r\n\r\n# æ·»åŠ æ•°æ®\r\ndata_rows = [\r\n    ('è¥é”€æ´»åŠ¨', '50,000', '48,200'),\r\n    ('ç ”å‘æŠ•å…¥', '200,000', '198,500'),\r\n    ('è¡Œæ”¿è´¹ç”¨', '30,000', '31,200')\r\n]\r\n\r\nfor i, (item, budget, actual) in enumerate(data_rows, 1):\r\n    row_cells = table.rows[i].cells\r\n    row_cells[0].text = item\r\n    row_cells[1].text = budget\r\n    row_cells[2].text = actual\r\n\r\n# æ·»åŠ æ€»ç»“æ®µè½\r\ndoc.add_heading('æ€»ç»“', level=2)\r\ndoc.add_paragraph('æ€»ä½“æ¥çœ‹ï¼Œå„éƒ¨é—¨é¢„ç®—æ‰§è¡Œæƒ…å†µè‰¯å¥½ï¼Œå®é™…æ”¯å‡ºåŸºæœ¬æ§åˆ¶åœ¨é¢„ç®—èŒƒå›´å†…ã€‚')\r\n\r\n# --- 2. ä¿å­˜åˆ°å†…å­˜ç¼“å†²åŒº ---\r\nbuffer = io.BytesIO()\r\ndoc.save(buffer)\r\nbuffer.seek(0)  # é‡ç½®æŒ‡é’ˆåˆ°å¼€å¤´\r\n\r\n# --- 3. Base64 ç¼–ç å¹¶æ‰“åŒ…ä¸º JSON ---\r\ndata_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\r\nresult = {\r\n    \"type\": \"word\",\r\n    \"title\": f\"ä¸šåŠ¡åˆ†ææŠ¥å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx\",\r\n    \"data_base64\": data_base64\r\n}\r\n\r\n# --- 4. æ‰“å°æœ€ç»ˆçš„ JSON å¯¹è±¡ ---\r\nprint(json.dumps(result))\r\n```\r\n\r\n---\r\n\r\n## ğŸ“ˆ Excel æŠ¥å‘Šç”Ÿæˆ (.xlsx)\r\n\r\n### âœ… å¯ç›´æ¥ä½¿ç”¨çš„ä»£ç æ¨¡æ¿\r\n```python\r\nimport io\r\nimport base64\r\nimport json\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom datetime import datetime\r\nfrom openpyxl.styles import Font, Alignment, PatternFill\r\n\r\n# --- 1. åˆ›å»º DataFrame å¹¶å‡†å¤‡ Excel å†…å®¹ ---\r\ndata = {\r\n    'éƒ¨é—¨': ['é”€å”®éƒ¨', 'ç ”å‘éƒ¨', 'å¸‚åœºéƒ¨', 'äººåŠ›èµ„æºéƒ¨', 'è´¢åŠ¡éƒ¨'],\r\n    'é¢„ç®—(å…ƒ)': [500000, 800000, 300000, 200000, 150000],\r\n    'å®é™…æ”¯å‡º(å…ƒ)': [485000, 795000, 310000, 195000, 148000],\r\n    'å·®å¼‚ç‡(%)': [-3.0, -0.6, 3.3, -2.5, -1.3]\r\n}\r\ndf = pd.DataFrame(data)\r\n\r\n# è®¡ç®—æ€»è®¡\r\nsummary_data = {\r\n    'éƒ¨é—¨': ['æ€»è®¡'],\r\n    'é¢„ç®—(å…ƒ)': [df['é¢„ç®—(å…ƒ)'].sum()],\r\n    'å®é™…æ”¯å‡º(å…ƒ)': [df['å®é™…æ”¯å‡º(å…ƒ)'].sum()],\r\n    'å·®å¼‚ç‡(%)': [round((df['å®é™…æ”¯å‡º(å…ƒ)'].sum() - df['é¢„ç®—(å…ƒ)'].sum()) / df['é¢„ç®—(å…ƒ)'].sum() * 100, 2)]\r\n}\r\nsummary_df = pd.DataFrame(summary_data)\r\n\r\n# --- 2. ä½¿ç”¨ ExcelWriter å°† DataFrame å†™å…¥å†…å­˜ç¼“å†²åŒº ---\r\noutput_buffer = io.BytesIO()\r\nwith pd.ExcelWriter(output_buffer, engine='openpyxl') as writer:\r\n    # å†™å…¥è¯¦ç»†æ•°æ®è¡¨\r\n    df.to_excel(writer, sheet_name='éƒ¨é—¨é¢„ç®—è¯¦æƒ…', index=False)\r\n    \r\n    # å†™å…¥æ±‡æ€»è¡¨\r\n    summary_df.to_excel(writer, sheet_name='é¢„ç®—æ±‡æ€»', index=False)\r\n    \r\n    # è·å–å·¥ä½œç°¿å’Œå·¥ä½œè¡¨ä»¥è¿›è¡Œæ ¼å¼è®¾ç½®\r\n    workbook = writer.book\r\n    detail_sheet = writer.sheets['éƒ¨é—¨é¢„ç®—è¯¦æƒ…']\r\n    summary_sheet = writer.sheets['é¢„ç®—æ±‡æ€»']\r\n    \r\n    # è®¾ç½®åˆ—å®½\r\n    for column in detail_sheet.columns:\r\n        max_length = 0\r\n        column_letter = column[0].column_letter\r\n        for cell in column:\r\n            try:\r\n                if len(str(cell.value)) > max_length:\r\n                    max_length = len(str(cell.value))\r\n            except:\r\n                pass\r\n        adjusted_width = min(max_length + 2, 30)\r\n        detail_sheet.column_dimensions[column_letter].width = adjusted_width\r\n        \r\n    # è®¾ç½®æ ‡é¢˜æ ·å¼\r\n    for cell in detail_sheet[1]:\r\n        cell.font = Font(bold=True, size=12)\r\n        cell.fill = PatternFill(start_color=\"C6EFCE\", end_color=\"C6EFCE\", fill_type=\"solid\")\r\n        cell.alignment = Alignment(horizontal='center')\r\n    \r\n    for cell in summary_sheet[1]:\r\n        cell.font = Font(bold=True, size=14, color=\"FFFFFF\")\r\n        cell.fill = PatternFill(start_color=\"366092\", end_color=\"366092\", fill_type=\"solid\")\r\n        cell.alignment = Alignment(horizontal='center')\r\n\r\noutput_buffer.seek(0)\r\n\r\n# --- 3. Base64 ç¼–ç å¹¶æ‰“åŒ…ä¸º JSON ---\r\ndata_base64 = base64.b64encode(output_buffer.getvalue()).decode('utf-8')\r\nresult = {\r\n    \"type\": \"excel\",\r\n    \"title\": f\"éƒ¨é—¨é¢„ç®—æŠ¥å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\",\r\n    \"data_base64\": data_base64\r\n}\r\n\r\n# --- 4. æ‰“å°æœ€ç»ˆçš„ JSON å¯¹è±¡ ---\r\nprint(json.dumps(result))\r\n```\r\n\r\n---\r\n\r\n## ğŸ“Š é«˜çº§Excelæ“ä½œï¼ˆv2.5æ–°å¢ï¼‰\r\n\r\n### ä½¿ç”¨å¤šä¸ªå·¥ä½œè¡¨å’Œæ•°æ®é€è§†è¡¨\r\n```python\r\nimport pandas as pd\r\nimport io\r\nimport base64\r\nimport json\r\nfrom datetime import datetime\r\n\r\ndef create_advanced_excel_report():\r\n    \"\"\"åˆ›å»ºåŒ…å«å¤šä¸ªå·¥ä½œè¡¨å’Œå¤æ‚åˆ†æçš„ExcelæŠ¥å‘Š\"\"\"\r\n    \r\n    # åˆ›å»ºç¤ºä¾‹æ•°æ®\r\n    sales_data = {\r\n        'æ—¥æœŸ': pd.date_range('2024-01-01', periods=30, freq='D'),\r\n        'äº§å“': np.random.choice(['A', 'B', 'C', 'D'], 30),\r\n        'é”€å”®é¢': np.random.randint(1000, 10000, 30),\r\n        'æ•°é‡': np.random.randint(10, 100, 30)\r\n    }\r\n    \r\n    customer_data = {\r\n        'å®¢æˆ·ID': [f'C{1000+i}' for i in range(10)],\r\n        'å®¢æˆ·åç§°': [f'å®¢æˆ·_{i}' for i in range(10)],\r\n        'åœ°åŒº': np.random.choice(['åä¸œ', 'åå—', 'ååŒ—', 'è¥¿å—'], 10),\r\n        'ä¿¡ç”¨è¯„çº§': np.random.choice(['A', 'B', 'C'], 10)\r\n    }\r\n    \r\n    df_sales = pd.DataFrame(sales_data)\r\n    df_customers = pd.DataFrame(customer_data)\r\n    \r\n    # åˆ›å»ºæ•°æ®é€è§†è¡¨\r\n    pivot_table = pd.pivot_table(df_sales, \r\n                                 values='é”€å”®é¢', \r\n                                 index='äº§å“', \r\n                                 columns=df_sales['æ—¥æœŸ'].dt.strftime('%Y-%m-%d'), \r\n                                 aggfunc='sum',\r\n                                 fill_value=0)\r\n    \r\n    # åˆ›å»ºç¼“å†²åŒº\r\n    buffer = io.BytesIO()\r\n    \r\n    with pd.ExcelWriter(buffer, engine='openpyxl') as writer:\r\n        # å†™å…¥åŸå§‹æ•°æ®\r\n        df_sales.to_excel(writer, sheet_name='é”€å”®æ•°æ®', index=False)\r\n        df_customers.to_excel(writer, sheet_name='å®¢æˆ·æ•°æ®', index=False)\r\n        \r\n        # å†™å…¥æ•°æ®é€è§†è¡¨\r\n        pivot_table.to_excel(writer, sheet_name='é”€å”®æ±‡æ€»')\r\n        \r\n        # å†™å…¥åˆ†æç»“æœ\r\n        analysis_data = {\r\n            'æŒ‡æ ‡': ['æ€»é”€å”®é¢', 'å¹³å‡é”€å”®é¢', 'æœ€é«˜é”€å”®é¢', 'æœ€ä½é”€å”®é¢'],\r\n            'æ•°å€¼': [\r\n                df_sales['é”€å”®é¢'].sum(),\r\n                df_sales['é”€å”®é¢'].mean(),\r\n                df_sales['é”€å”®é¢'].max(),\r\n                df_sales['é”€å”®é¢'].min()\r\n            ]\r\n        }\r\n        df_analysis = pd.DataFrame(analysis_data)\r\n        df_analysis.to_excel(writer, sheet_name='åˆ†æç»“æœ', index=False)\r\n    \r\n    buffer.seek(0)\r\n    \r\n    # ç¼–ç å¹¶è¾“å‡º\r\n    data_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\r\n    result = {\r\n        \"type\": \"excel\",\r\n        \"title\": f\"é«˜çº§é”€å”®åˆ†ææŠ¥å‘Š_{datetime.now().strftime('%Y%m%d')}.xlsx\",\r\n        \"data_base64\": data_base64\r\n    }\r\n    \r\n    print(json.dumps(result))\r\n```\r\n\r\n---\r\n\r\n## ğŸ“„ PDF æŠ¥å‘Šç”Ÿæˆ (.pdf)\r\n\r\n### âœ… å¯ç›´æ¥ä½¿ç”¨çš„ä»£ç æ¨¡æ¿\r\n```python\r\nimport io\r\nimport base64\r\nimport json\r\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, Image\r\nfrom reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\r\nfrom reportlab.lib.units import inch\r\nfrom reportlab.lib import colors\r\nfrom reportlab.lib.pagesizes import letter, A4\r\nfrom datetime import datetime\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\n# --- åˆ›å»ºå›¾è¡¨å¹¶ä¿å­˜åˆ°å†…å­˜ ---\r\ndef create_chart_image():\r\n    \"\"\"åˆ›å»ºä¸€ä¸ªç¤ºä¾‹å›¾è¡¨å¹¶è¿”å›Base64ç¼–ç \"\"\"\r\n    plt.figure(figsize=(8, 4))\r\n    categories = ['Q1', 'Q2', 'Q3', 'Q4']\r\n    values = [120, 145, 180, 160]\r\n    plt.bar(categories, values, color=['#2E86AB', '#A23B72', '#F18F01', '#C73E1D'])\r\n    plt.title('å­£åº¦é”€å”®é¢è¶‹åŠ¿')\r\n    plt.xlabel('å­£åº¦')\r\n    plt.ylabel('é”€å”®é¢(ä¸‡å…ƒ)')\r\n    plt.grid(True, alpha=0.3)\r\n    \r\n    # å°†å›¾è¡¨ä¿å­˜åˆ°å†…å­˜\r\n    chart_buffer = io.BytesIO()\r\n    plt.savefig(chart_buffer, format='png', dpi=100, bbox_inches='tight')\r\n    plt.close()\r\n    chart_buffer.seek(0)\r\n    return chart_buffer\r\n\r\n# --- 1. åœ¨å†…å­˜ä¸­æ„å»º PDF æ–‡æ¡£ ---\r\nbuffer = io.BytesIO()\r\ndoc = SimpleDocTemplate(buffer, pagesize=A4, rightMargin=72, leftMargin=72, topMargin=72, bottomMargin=18)\r\nstyles = getSampleStyleSheet()\r\n\r\n# è‡ªå®šä¹‰æ ·å¼\r\ntitle_style = ParagraphStyle(\r\n    'CustomTitle',\r\n    parent=styles['Title'],\r\n    fontSize=24,\r\n    spaceAfter=30,\r\n    alignment=1  # å±…ä¸­\r\n)\r\n\r\nheading_style = ParagraphStyle(\r\n    'CustomHeading',\r\n    parent=styles['Heading2'],\r\n    fontSize=16,\r\n    spaceBefore=20,\r\n    spaceAfter=10\r\n)\r\n\r\n# æ„å»ºå†…å®¹\r\nstory = []\r\nstory.append(Paragraph(\"å…¬å¸å¹´åº¦è´¢åŠ¡æŠ¥å‘Š\", title_style))\r\nstory.append(Paragraph(f\"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}\", styles['Normal']))\r\nstory.append(Spacer(1, 20))\r\n\r\n# æ·»åŠ æ‘˜è¦\r\nstory.append(Paragraph(\"æ‰§è¡Œæ‘˜è¦\", heading_style))\r\nstory.append(Paragraph(\"æœ¬æŠ¥å‘Šè¯¦ç»†åˆ†æäº†å…¬å¸2024å¹´åº¦çš„è´¢åŠ¡çŠ¶å†µå’Œä¸šåŠ¡è¡¨ç°ï¼ŒåŒ…æ‹¬æ”¶å…¥ã€æ”¯å‡ºã€åˆ©æ¶¦ç­‰å…³é”®æŒ‡æ ‡ã€‚\", styles['BodyText']))\r\nstory.append(Spacer(1, 15))\r\n\r\n# æ·»åŠ å›¾è¡¨\r\nchart_buffer = create_chart_image()\r\nstory.append(Paragraph(\"å­£åº¦é”€å”®è¶‹åŠ¿\", heading_style))\r\nstory.append(Image(chart_buffer, width=6*inch, height=3*inch))\r\nstory.append(Spacer(1, 15))\r\n\r\n# æ·»åŠ è¡¨æ ¼\r\nstory.append(Paragraph(\"è´¢åŠ¡æ•°æ®æ±‡æ€»\", heading_style))\r\ndata = [\r\n    ['é¡¹ç›®', 'Q1', 'Q2', 'Q3', 'Q4', 'å¹´åº¦æ€»è®¡'],\r\n    ['æ”¶å…¥(ä¸‡å…ƒ)', '450', '520', '610', '580', '2160'],\r\n    ['æˆæœ¬(ä¸‡å…ƒ)', '280', '310', '350', '320', '1260'],\r\n    ['åˆ©æ¶¦(ä¸‡å…ƒ)', '170', '210', '260', '260', '900'],\r\n    ['åˆ©æ¶¦ç‡(%)', '37.8', '40.4', '42.6', '44.8', '41.7']\r\n]\r\n\r\ntable = Table(data, colWidths=[1.5*inch, 1*inch, 1*inch, 1*inch, 1*inch, 1.2*inch])\r\ntable.setStyle(TableStyle([\r\n    ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\r\n    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\r\n    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\r\n    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\r\n    ('FONTSIZE', (0, 0), (-1, 0), 12),\r\n    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\r\n    ('BACKGROUND', (0, 1), (-1, -1), colors.beige),\r\n    ('GRID', (0, 0), (-1, -1), 1, colors.black),\r\n    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\r\n    ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),\r\n]))\r\n\r\nstory.append(table)\r\nstory.append(Spacer(1, 20))\r\n\r\n# æ·»åŠ ç»“è®º\r\nstory.append(Paragraph(\"ç»“è®ºä¸å»ºè®®\", heading_style))\r\nstory.append(Paragraph(\"1. å…¬å¸å…¨å¹´æ”¶å…¥ç¨³æ­¥å¢é•¿ï¼Œç¬¬å››å­£åº¦ç•¥æœ‰å›è½ä½†æ•´ä½“è¡¨ç°è‰¯å¥½ã€‚\", styles['BodyText']))\r\nstory.append(Paragraph(\"2. åˆ©æ¶¦ç‡é€å­£åº¦æå‡ï¼Œæ˜¾ç¤ºæˆæœ¬æ§åˆ¶æªæ–½æ•ˆæœæ˜¾è‘—ã€‚\", styles['BodyText']))\r\nstory.append(Paragraph(\"3. å»ºè®®æ˜å¹´åŠ å¤§ç ”å‘æŠ•å…¥ï¼Œä¼˜åŒ–äº§å“ç»“æ„ï¼Œè¿›ä¸€æ­¥æå‡ç›ˆåˆ©èƒ½åŠ›ã€‚\", styles['BodyText']))\r\n\r\n# æ„å»ºæ–‡æ¡£\r\ndoc.build(story)\r\nbuffer.seek(0)\r\n\r\n# --- 2. Base64 ç¼–ç å¹¶æ‰“åŒ…ä¸º JSON ---\r\ndata_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\r\nresult = {\r\n    \"type\": \"pdf\",\r\n    \"title\": f\"å…¬å¸å¹´åº¦è´¢åŠ¡æŠ¥å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf\",\r\n    \"data_base64\": data_base64\r\n}\r\n\r\n# --- 3. æ‰“å°æœ€ç»ˆçš„ JSON å¯¹è±¡ ---\r\nprint(json.dumps(result))\r\n```\r\n\r\n---\r\n\r\n## ğŸ¤ PowerPoint æŠ¥å‘Šç”Ÿæˆ (.pptx) - v2.5æ–°å¢\r\n\r\n### âœ… å¯ç›´æ¥ä½¿ç”¨çš„ä»£ç æ¨¡æ¿\r\n```python\r\nimport io\r\nimport base64\r\nimport json\r\nfrom pptx import Presentation\r\nfrom pptx.util import Inches, Pt\r\nfrom pptx.enum.text import PP_ALIGN\r\nfrom pptx.dml.color import RGBColor\r\nfrom datetime import datetime\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\n# --- 1. åœ¨å†…å­˜ä¸­æ„å»º PowerPoint æ–‡æ¡£ ---\r\nprs = Presentation()\r\n\r\n# åˆ›å»ºæ ‡é¢˜é¡µ\r\ntitle_slide_layout = prs.slide_layouts[0]\r\nslide = prs.slides.add_slide(title_slide_layout)\r\ntitle = slide.shapes.title\r\nsubtitle = slide.placeholders[1]\r\n\r\ntitle.text = \"å­£åº¦ä¸šåŠ¡æ±‡æŠ¥\"\r\nsubtitle.text = f\"{datetime.now().strftime('%Yå¹´%mæœˆ')}\\næ•°æ®åˆ†æå›¢é˜Ÿ\"\r\n\r\n# åˆ›å»ºç›®å½•é¡µ\r\nbullet_slide_layout = prs.slide_layouts[1]\r\nslide = prs.slides.add_slide(bullet_slide_layout)\r\nshapes = slide.shapes\r\n\r\ntitle_shape = shapes.title\r\ntitle_shape.text = 'æ±‡æŠ¥ç›®å½•'\r\n\r\nbody_shape = shapes.placeholders[1]\r\ntf = body_shape.text_frame\r\ntf.text = '1. ä¸šç»©æ¦‚è§ˆ'\r\np = tf.add_paragraph()\r\np.text = '2. å¸‚åœºåˆ†æ'\r\np = tf.add_paragraph()\r\np.text = '3. è´¢åŠ¡æ•°æ®'\r\np = tf.add_paragraph()\r\np.text = '4. æœªæ¥å±•æœ›'\r\n\r\n# åˆ›å»ºå›¾è¡¨é¡µ - ä¸šç»©æ¦‚è§ˆ\r\nslide = prs.slides.add_slide(prs.slide_layouts[5])\r\ntitle = slide.shapes.title\r\ntitle.text = \"ä¸šç»©æ¦‚è§ˆ\"\r\n\r\n# åœ¨å†…å­˜ä¸­åˆ›å»ºå›¾è¡¨\r\nplt.figure(figsize=(6, 4))\r\nmonths = ['1æœˆ', '2æœˆ', '3æœˆ', '4æœˆ', '5æœˆ', '6æœˆ']\r\nsales = [120, 135, 150, 145, 160, 180]\r\ntargets = [110, 130, 140, 150, 155, 170]\r\n\r\nx = np.arange(len(months))\r\nwidth = 0.35\r\n\r\nfig, ax = plt.subplots()\r\nrects1 = ax.bar(x - width/2, sales, width, label='å®é™…é”€å”®é¢', color='#2E86AB')\r\nrects2 = ax.bar(x + width/2, targets, width, label='ç›®æ ‡é”€å”®é¢', color='#A23B72')\r\n\r\nax.set_xlabel('æœˆä»½')\r\nax.set_ylabel('é”€å”®é¢(ä¸‡å…ƒ)')\r\nax.set_title('ä¸ŠåŠå¹´é”€å”®é¢å¯¹æ¯”')\r\nax.set_xticks(x)\r\nax.set_xticklabels(months)\r\nax.legend()\r\nax.grid(True, alpha=0.3)\r\n\r\n# ä¿å­˜å›¾è¡¨åˆ°å†…å­˜\r\nchart_buffer = io.BytesIO()\r\nplt.savefig(chart_buffer, format='png', dpi=150, bbox_inches='tight')\r\nplt.close()\r\nchart_buffer.seek(0)\r\n\r\n# æ·»åŠ å›¾è¡¨åˆ°å¹»ç¯ç‰‡\r\nleft = Inches(1)\r\ntop = Inches(1.5)\r\npic = slide.shapes.add_picture(chart_buffer, left, top, width=Inches(8), height=Inches(4.5))\r\n\r\n# åˆ›å»ºæ•°æ®é¡µ - è´¢åŠ¡æ•°æ®\r\nslide = prs.slides.add_slide(prs.slide_layouts[1])\r\ntitle = slide.shapes.title\r\ntitle.text = \"è´¢åŠ¡æ•°æ®\"\r\n\r\nbody_shape = slide.shapes.placeholders[1]\r\ntf = body_shape.text_frame\r\ntf.text = 'æ”¶å…¥æƒ…å†µ:'\r\np = tf.add_paragraph()\r\np.text = 'â€¢ æ€»æ”¶å…¥: 850ä¸‡å…ƒ'\r\np = tf.add_paragraph()\r\np.text = 'â€¢ åŒæ¯”å¢é•¿: 15.2%'\r\np = tf.add_paragraph()\r\np.text = 'â€¢ æ¯›åˆ©ç‡: 42.3%'\r\n\r\np = tf.add_paragraph()\r\np.text = 'æˆæœ¬åˆ†æ:'\r\np = tf.add_paragraph()\r\np.text = 'â€¢ æ€»æˆæœ¬: 490ä¸‡å…ƒ'\r\np = tf.add_paragraph()\r\np.text = 'â€¢ äººåŠ›æˆæœ¬: 45%'\r\np = tf.add_paragraph()\r\np.text = 'â€¢ è¥é”€è´¹ç”¨: 28%'\r\n\r\n# åˆ›å»ºæ€»ç»“é¡µ\r\nslide = prs.slides.add_slide(prs.slide_layouts[1])\r\ntitle = slide.shapes.title\r\ntitle.text = \"æ€»ç»“ä¸å±•æœ›\"\r\n\r\nbody_shape = slide.shapes.placeholders[1]\r\ntf = body_shape.text_frame\r\ntf.text = 'æ ¸å¿ƒæˆæœ:'\r\np = tf.add_paragraph()\r\np.text = 'âœ“ è¶…é¢å®Œæˆä¸ŠåŠå¹´é”€å”®ç›®æ ‡'\r\np = tf.add_paragraph()\r\np.text = 'âœ“ å¸‚åœºå æœ‰ç‡æå‡è‡³18.5%'\r\np = tf.add_paragraph()\r\np.text = 'âœ“ å®¢æˆ·æ»¡æ„åº¦è¾¾åˆ°92%'\r\n\r\np = tf.add_paragraph()\r\np.text = 'ä¸‹ä¸€æ­¥è®¡åˆ’:'\r\np = tf.add_paragraph()\r\np.text = 'â€¢ æ‹“å±•æ–°å¸‚åœºï¼Œç›®æ ‡å¢é•¿20%'\r\np = tf.add_paragraph()\r\np.text = 'â€¢ ä¼˜åŒ–ä¾›åº”é“¾ï¼Œé™ä½è¿è¥æˆæœ¬'\r\np = tf.add_paragraph()\r\np.text = 'â€¢ åŠ å¼ºäººæ‰åŸ¹å…»ï¼Œæå‡å›¢é˜Ÿèƒ½åŠ›'\r\n\r\n# --- 2. ä¿å­˜åˆ°å†…å­˜ç¼“å†²åŒº ---\r\nbuffer = io.BytesIO()\r\nprs.save(buffer)\r\nbuffer.seek(0)\r\n\r\n# --- 3. Base64 ç¼–ç å¹¶æ‰“åŒ…ä¸º JSON ---\r\ndata_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\r\nresult = {\r\n    \"type\": \"ppt\",\r\n    \"title\": f\"å­£åº¦ä¸šåŠ¡æ±‡æŠ¥_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pptx\",\r\n    \"data_base64\": data_base64\r\n}\r\n\r\n# --- 4. æ‰“å°æœ€ç»ˆçš„ JSON å¯¹è±¡ ---\r\nprint(json.dumps(result))\r\n```\r\n\r\n---\r\n\r\n## ğŸ“ å¤åˆæŠ¥å‘Šç”Ÿæˆï¼ˆWord + Excel + PDFï¼‰\r\n\r\n### âœ… å®Œæ•´å·¥ä½œæµç¤ºä¾‹\r\n```python\r\nimport io\r\nimport base64\r\nimport json\r\nimport pandas as pd\r\nfrom docx import Document\r\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\r\nfrom reportlab.lib.styles import getSampleStyleSheet\r\nfrom datetime import datetime\r\n\r\ndef generate_comprehensive_report():\r\n    \"\"\"ç”ŸæˆåŒ…å«Wordæ‘˜è¦ã€Excelè¯¦ç»†æ•°æ®å’ŒPDFæŠ¥å‘Šçš„å®Œæ•´åˆ†æåŒ…\"\"\"\r\n    \r\n    # åˆ›å»ºç¤ºä¾‹æ•°æ®\r\n    data = {\r\n        'æŒ‡æ ‡': ['æ”¶å…¥', 'æˆæœ¬', 'åˆ©æ¶¦', 'åˆ©æ¶¦ç‡', 'å¢é•¿ç‡'],\r\n        'Q1': [450, 280, 170, '37.8%', '12.5%'],\r\n        'Q2': [520, 310, 210, '40.4%', '15.6%'],\r\n        'Q3': [610, 350, 260, '42.6%', '23.8%'],\r\n        'Q4': [580, 320, 260, '44.8%', '0%']\r\n    }\r\n    df = pd.DataFrame(data)\r\n    \r\n    # 1. ç”ŸæˆWordæ‘˜è¦æŠ¥å‘Š\r\n    doc = Document()\r\n    doc.add_heading('å­£åº¦åˆ†ææ‘˜è¦', 0)\r\n    doc.add_paragraph(f'ç”Ÿæˆæ—¶é—´: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\r\n    doc.add_paragraph('æœ¬æŠ¥å‘ŠåŸºäº2024å¹´å››ä¸ªå­£åº¦çš„è´¢åŠ¡æ•°æ®ï¼Œåˆ†æäº†å…¬å¸çš„æ•´ä½“ç»è¥çŠ¶å†µã€‚')\r\n    \r\n    word_buffer = io.BytesIO()\r\n    doc.save(word_buffer)\r\n    word_buffer.seek(0)\r\n    \r\n    # 2. ç”ŸæˆExcelè¯¦ç»†æ•°æ®\r\n    excel_buffer = io.BytesIO()\r\n    with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:\r\n        df.to_excel(writer, sheet_name='å­£åº¦è´¢åŠ¡æ•°æ®', index=False)\r\n        \r\n        # æ·»åŠ è®¡ç®—è¡¨\r\n        summary_df = pd.DataFrame({\r\n            'å¹´åº¦æŒ‡æ ‡': ['æ€»æ”¶å…¥', 'æ€»æˆæœ¬', 'æ€»åˆ©æ¶¦', 'å¹³å‡åˆ©æ¶¦ç‡'],\r\n            'æ•°å€¼': [df[['Q1','Q2','Q3','Q4']].sum().sum(), \r\n                    df[['Q1','Q2','Q3','Q4']].iloc[1].sum(),\r\n                    df[['Q1','Q2','Q3','Q4']].iloc[2].sum(),\r\n                    '41.4%']\r\n        })\r\n        summary_df.to_excel(writer, sheet_name='å¹´åº¦æ±‡æ€»', index=False)\r\n    \r\n    excel_buffer.seek(0)\r\n    \r\n    # 3. ç”ŸæˆPDFæŠ¥å‘Š\r\n    pdf_buffer = io.BytesIO()\r\n    doc_pdf = SimpleDocTemplate(pdf_buffer)\r\n    styles = getSampleStyleSheet()\r\n    story = [\r\n        Paragraph('2024å¹´åº¦è´¢åŠ¡åˆ†ææŠ¥å‘Š', styles['Title']),\r\n        Spacer(1, 20),\r\n        Paragraph('åŸºäºå­£åº¦æ•°æ®çš„æ·±åº¦åˆ†æ', styles['Heading2']),\r\n        Spacer(1, 15),\r\n        Paragraph('æŠ¥å‘Šæ€»ç»“äº†å…¬å¸2024å¹´åº¦çš„ç»è¥è¡¨ç°ï¼Œå¹¶å¯¹æœªæ¥å‘å±•è¶‹åŠ¿è¿›è¡Œäº†å±•æœ›ã€‚', styles['Normal'])\r\n    ]\r\n    doc_pdf.build(story)\r\n    pdf_buffer.seek(0)\r\n    \r\n    # è¿”å›æ‰€æœ‰æ–‡ä»¶ï¼ˆå®é™…ä½¿ç”¨æ—¶ï¼Œä¸€æ¬¡åªèƒ½è¿”å›ä¸€ä¸ªæ–‡ä»¶ï¼‰\r\n    # è¿™é‡Œæ¼”ç¤ºå¦‚ä½•æ„å»ºå¤šä¸ªæ–‡ä»¶ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦åˆ†åˆ«æ‰§è¡Œ\r\n    files_info = [\r\n        {\r\n            \"type\": \"word\",\r\n            \"title\": \"åˆ†ææ‘˜è¦.docx\",\r\n            \"data_base64\": base64.b64encode(word_buffer.getvalue()).decode('utf-8')\r\n        },\r\n        {\r\n            \"type\": \"excel\", \r\n            \"title\": \"è¯¦ç»†æ•°æ®.xlsx\",\r\n            \"data_base64\": base64.b64encode(excel_buffer.getvalue()).decode('utf-8')\r\n        },\r\n        {\r\n            \"type\": \"pdf\",\r\n            \"title\": \"å®Œæ•´æŠ¥å‘Š.pdf\",\r\n            \"data_base64\": base64.b64encode(pdf_buffer.getvalue()).decode('utf-8')\r\n        }\r\n    ]\r\n    \r\n    print(\"æ³¨æ„ï¼šä¸€æ¬¡åªèƒ½è¿”å›ä¸€ä¸ªæ–‡ä»¶ï¼Œä»¥ä¸‹æ˜¯ä¸‰ä¸ªæ–‡ä»¶çš„JSONç¤ºä¾‹ï¼š\")\r\n    for i, file_info in enumerate(files_info):\r\n        print(f\"\\næ–‡ä»¶{i+1} JSON:\")\r\n        print(json.dumps(file_info, indent=2))\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\nif __name__ == \"__main__\":\r\n    generate_comprehensive_report()\r\n```\r\n\r\n---\r\n\r\n## âš ï¸ é‡è¦æ³¨æ„äº‹é¡¹\r\n\r\n### âœ… å¿…é¡»åšçš„:\r\n1. **å•ä¸€è¾“å‡º**: æ¯ä¸ªä»£ç æ‰§è¡Œåªèƒ½è¾“å‡ºä¸€ä¸ªJSONå¯¹è±¡ï¼ˆä¸€ä¸ªæ–‡ä»¶ï¼‰\r\n2. **Base64ç¼–ç **: å¿…é¡»ä½¿ç”¨`base64.b64encode().decode('utf-8')`è¿›è¡Œç¼–ç \r\n3. **æ–‡ä»¶åè§„èŒƒ**: æ–‡ä»¶ååº”åŒ…å«æ—¶é—´æˆ³ï¼Œé¿å…é‡å¤ï¼š`f\"æŠ¥å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx\"`\r\n4. **ç¼–ç ä¸€è‡´æ€§**: ä¸­æ–‡å­—ç¬¦ä½¿ç”¨`ensure_ascii=False`å‚æ•°ï¼ˆä½†åœ¨æ²™ç›’ä¸­ä¼šè‡ªåŠ¨å¤„ç†ï¼‰\r\n\r\n### âŒ ç»å¯¹ç¦æ­¢:\r\n1. **ç¦æ­¢ä¿å­˜åˆ°ç£ç›˜**: ä¸è¦ä½¿ç”¨`doc.save('filename.docx')`æˆ–`wb.save('filename.xlsx')`\r\n2. **ç¦æ­¢å¤šæ¬¡è¾“å‡º**: ä¸è¦åœ¨ä¸€æ¬¡æ‰§è¡Œä¸­ç”Ÿæˆå¤šä¸ªæ–‡ä»¶\r\n3. **ç¦æ­¢æ··åˆè¾“å‡º**: ä¸è¦åœ¨æ‰“å°JSONåæ‰“å°å…¶ä»–å†…å®¹\r\n4. **ç¦æ­¢è·¯å¾„è®¿é—®**: ä¸è¦å°è¯•è®¿é—®é™¤`/data`ç›®å½•å¤–çš„æ–‡ä»¶ç³»ç»Ÿ\r\n\r\n### ğŸ”§ æœ€ä½³å®è·µ:\r\n1. **ä½¿ç”¨å†…å­˜ç¼“å†²åŒº**: å§‹ç»ˆä½¿ç”¨`io.BytesIO()`åœ¨å†…å­˜ä¸­æ“ä½œæ–‡ä»¶\r\n2. **åŠæ—¶é‡Šæ”¾èµ„æº**: ä½¿ç”¨`buffer.seek(0)`é‡ç½®æŒ‡é’ˆ\r\n3. **åŒ…å«æ—¶é—´æˆ³**: åœ¨æ–‡ä»¶åä¸­æ·»åŠ æ—¶é—´æˆ³ï¼Œé¿å…å†²çª\r\n4. **æµ‹è¯•ä»£ç **: åœ¨ç”Ÿæˆå¤æ‚æŠ¥å‘Šå‰ï¼Œå…ˆæµ‹è¯•å›¾è¡¨ç”Ÿæˆå’Œæ•°æ®å¤„ç†éƒ¨åˆ†\r\n5. **åˆ†æ­¥éªŒè¯**: å¯¹äºå¤æ‚æŠ¥å‘Šï¼Œå¯ä»¥å…ˆéªŒè¯å„éƒ¨åˆ†åŠŸèƒ½å†æ•´åˆ\r\n\r\n### ğŸ“Š å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ:\r\n\r\n| é”™è¯¯ç±»å‹ | åŸå›  | è§£å†³æ–¹æ¡ˆ |\r\n|---------|------|----------|\r\n| JSONè§£æå¤±è´¥ | æ‰“å°äº†é¢å¤–å†…å®¹ | ç¡®ä¿åªæ‰“å°ä¸€ä¸ªJSONå­—ç¬¦ä¸² |\r\n| æ–‡ä»¶æŸå | Base64ç¼–ç é”™è¯¯ | ä½¿ç”¨æ­£ç¡®çš„`.decode('utf-8')` |\r\n| å†…å­˜ä¸è¶³ | æ–‡ä»¶å¤ªå¤§ | å‹ç¼©å›¾ç‰‡ï¼Œå‡å°‘æ•°æ®é‡ |\r\n| ä¸­æ–‡ä¹±ç  | ç¼–ç é—®é¢˜ | ç¡®ä¿ä½¿ç”¨UTF-8ç¼–ç  |\r\n\r\n---\r\n\r\n## ğŸ¯ å¿«é€Ÿå‚è€ƒè¡¨\r\n\r\n| æ–‡ä»¶ç±»å‹ | ä¸»è¦åº“ | è¾“å‡ºç±»å‹ | å¤‡æ³¨ |\r\n|---------|--------|----------|------|\r\n| Word (.docx) | `python-docx` | `\"type\": \"word\"` | æ”¯æŒè¡¨æ ¼ã€å›¾ç‰‡ã€æ ·å¼ |\r\n| Excel (.xlsx) | `openpyxl` + `pandas` | `\"type\": \"excel\"` | æ”¯æŒå¤šä¸ªsheetã€æ ¼å¼ |\r\n| PDF (.pdf) | `reportlab` | `\"type\": \"pdf\"` | æ”¯æŒå›¾è¡¨ã€è¡¨æ ¼ã€æ ·å¼ |\r\n| PowerPoint (.pptx) | `python-pptx` | `\"type\": \"ppt\"` | æ”¯æŒå¹»ç¯ç‰‡ã€å›¾è¡¨ |\r\n\r\n---\r\n\r\n## ğŸ”„ å·¥ä½œæµæ€»ç»“\r\n\r\n1. **å‡†å¤‡æ•°æ®**: ä»`/data`ç›®å½•è¯»å–æˆ–ç”Ÿæˆåˆ†ææ•°æ®\r\n2. **åˆ›å»ºæ–‡æ¡£**: ä½¿ç”¨ç›¸åº”åº“åœ¨å†…å­˜ä¸­æ„å»ºæ–‡æ¡£\r\n3. **æ·»åŠ å†…å®¹**: æ’å…¥æ–‡æœ¬ã€è¡¨æ ¼ã€å›¾è¡¨ã€æ ¼å¼ç­‰\r\n4. **ä¿å­˜åˆ°å†…å­˜**: ä½¿ç”¨`io.BytesIO()`ä¿å­˜æ–‡æ¡£\r\n5. **Base64ç¼–ç **: å°†äºŒè¿›åˆ¶æ•°æ®ç¼–ç ä¸ºå­—ç¬¦ä¸²\r\n6. **æ„å»ºJSON**: åˆ›å»ºåŒ…å«ç±»å‹ã€æ–‡ä»¶åå’Œæ•°æ®çš„å­—å…¸\r\n7. **è¾“å‡ºç»“æœ**: ä½¿ç”¨`print(json.dumps())`è¾“å‡ºå•ä¸ªJSONå¯¹è±¡\r\n\r\n**è®°ä½**: ç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†JSONè¾“å‡ºå¹¶æç¤ºç”¨æˆ·ä¸‹è½½æ–‡ä»¶ï¼\r\n",
        "scipy_cookbook.md": "# SciPy ç§‘å­¦è®¡ç®—æŒ‡å— (v2.5)\n\n## ğŸ¯ å·¥å…·æ¦‚è¿°\n**ç¯å¢ƒç‰¹æ€§**ï¼šåŸºäº SciPy çš„ç§‘å­¦è®¡ç®—ç¯å¢ƒï¼Œæ”¯æŒä¼˜åŒ–ã€ç§¯åˆ†ã€ä¿¡å·å¤„ç†ç­‰\n**è¾“å‡ºåŸåˆ™**ï¼šç³»ç»Ÿè‡ªåŠ¨å¤„ç†ç»“æœè¾“å‡ºï¼Œç›´æ¥æ‰“å°ç»“æœï¼Œå›¾è¡¨ä½¿ç”¨ `plt.show()`\n\n## ğŸ”§ æ ¸å¿ƒæ¨¡å—æ¦‚è§ˆ\n\n### ä¸»è¦åŠŸèƒ½æ¨¡å—ï¼š\n- **ä¼˜åŒ–ç®—æ³•** (`scipy.optimize`) - å‡½æ•°æœ€å°åŒ–ã€æ–¹ç¨‹æ±‚è§£\n- **ç§¯åˆ†è®¡ç®—** (`scipy.integrate`) - æ•°å€¼ç§¯åˆ†ã€å¾®åˆ†æ–¹ç¨‹\n- **ä¿¡å·å¤„ç†** (`scipy.signal`) - æ»¤æ³¢å™¨ã€é¢‘è°±åˆ†æ\n- **çº¿æ€§ä»£æ•°** (`scipy.linalg`) - çŸ©é˜µè¿ç®—ã€çº¿æ€§ç³»ç»Ÿ\n- **ç»Ÿè®¡å‡½æ•°** (`scipy.stats`) - æ¦‚ç‡åˆ†å¸ƒã€ç»Ÿè®¡æ£€éªŒ\n- **ç©ºé—´ç®—æ³•** (`scipy.spatial`) - ç©ºé—´æ•°æ®ã€è·ç¦»è®¡ç®—\n\n## âœ… ä»£ç è§£é‡Šå™¨é€‚é…è¯´æ˜\n- **ç›´æ¥æ‰“å°**ï¼šæ‰€æœ‰è®¡ç®—ç»“æœç›´æ¥ä½¿ç”¨ `print()` è¾“å‡º\n- **è‡ªåŠ¨å›¾è¡¨**ï¼šä½¿ç”¨ `plt.show()` å³å¯è‡ªåŠ¨æ•è·å›¾è¡¨\n- **å®Œæ•´é›†æˆ**ï¼šSciPy å·²é¢„è£…ï¼Œæ— éœ€é¢å¤–å®‰è£…\n- **å†…å­˜ä¼˜åŒ–**ï¼šå¤§è®¡ç®—æ—¶æ³¨æ„å†…å­˜ä½¿ç”¨ï¼Œå¯åˆ†æ‰¹å¤„ç†\n\n## ğŸ¯ ä¼˜åŒ–ä¸æ–¹ç¨‹æ±‚è§£\n\n### å‡½æ•°æœ€å°åŒ–\n```python\nimport numpy as np\nfrom scipy import optimize\nimport matplotlib.pyplot as plt\n\nprint(\"=== å•å˜é‡å‡½æ•°ä¼˜åŒ– ===\")\n\n# 1. å•å˜é‡å‡½æ•°ä¼˜åŒ–\ndef single_variable_func(x):\n    return (x - 3)**2 * np.sin(x) + x**2\n\nresult = optimize.minimize_scalar(single_variable_func, bounds=(0, 10), method='bounded')\nprint(f\"æœ€ä¼˜è§£: x = {result.x:.4f}, å‡½æ•°å€¼: {result.fun:.4f}\")\n\n# å¯è§†åŒ–\nx_plot = np.linspace(0, 10, 100)\ny_plot = single_variable_func(x_plot)\nplt.figure(figsize=(10, 6))\nplt.plot(x_plot, y_plot, label='f(x)')\nplt.axvline(result.x, color='red', linestyle='--', label=f'æœ€ä¼˜è§£ x={result.x:.3f}')\nplt.title('å•å˜é‡å‡½æ•°ä¼˜åŒ–')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n\n### å¤šå˜é‡ä¼˜åŒ–\n```python\nimport numpy as np\nfrom scipy import optimize\nimport matplotlib.pyplot as plt\n\nprint(\"=== å¤šå˜é‡å‡½æ•°ä¼˜åŒ– ===\")\n\n# Rosenbrock å‡½æ•°ä¼˜åŒ–\ndef rosenbrock(x):\n    return sum(100.0 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\n\nx0 = np.array([-1.2, 1.0])\nresult = optimize.minimize(rosenbrock, x0, method='BFGS')\n\nprint(f\"åˆå§‹ç‚¹: {x0}\")\nprint(f\"æœ€ä¼˜ç‚¹: {result.x}\")\nprint(f\"æœ€ä¼˜å€¼: {result.fun:.6f}\")\nprint(f\"è¿­ä»£æ¬¡æ•°: {result.nit}\")\nprint(f\"æ±‚è§£æˆåŠŸ: {result.success}\")\n\n# å¯è§†åŒ–\nx = np.linspace(-2, 2, 100)\ny = np.linspace(-1, 3, 100)\nX, Y = np.meshgrid(x, y)\nZ = np.zeros_like(X)\n\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        Z[i,j] = rosenbrock([X[i,j], Y[i,j]])\n\nplt.figure(figsize=(10, 8))\ncontour = plt.contour(X, Y, Z, levels=50)\nplt.clabel(contour, inline=True, fontsize=8)\nplt.plot(result.x[0], result.x[1], 'ro', markersize=10, label='æœ€ä¼˜è§£')\nplt.title('Rosenbrock å‡½æ•°ä¼˜åŒ–')\nplt.legend()\nplt.tight_layout()\nplt.show()\n```\n\n### çº¦æŸä¼˜åŒ–\n```python\nimport numpy as np\nfrom scipy import optimize\nimport matplotlib.pyplot as plt\n\nprint(\"=== çº¦æŸä¼˜åŒ– ===\")\n\n# å¸¦çº¦æŸçš„ä¼˜åŒ–é—®é¢˜\ndef objective(x):\n    return x[0]**2 + x[1]**2\n\ndef constraint1(x):\n    return x[0] + x[1] - 1  # x + y >= 1\n\nconstraints = [{'type': 'ineq', 'fun': constraint1}]\nbounds = [(0, None), (0, None)]\n\nresult = optimize.minimize(objective, [0.5, 0.5], \n                         method='SLSQP', bounds=bounds, \n                         constraints=constraints)\n\nprint(f\"çº¦æŸä¼˜åŒ–ç»“æœ:\")\nprint(f\"æœ€ä¼˜ç‚¹: {result.x}\")\nprint(f\"æœ€ä¼˜å€¼: {result.fun:.4f}\")\nprint(f\"çº¦æŸæ»¡è¶³: {result.success}\")\nprint(f\"è¿­ä»£æ¬¡æ•°: {result.nit}\")\n\n# å¯è§†åŒ–çº¦æŸåŒºåŸŸ\nx_const = np.linspace(0, 2, 100)\ny_const = np.linspace(0, 2, 100)\nX, Y = np.meshgrid(x_const, y_const)\nZ = objective([X, Y])\n\nplt.figure(figsize=(10, 8))\nplt.contourf(X, Y, Z, levels=20, alpha=0.6)\nplt.contour(X, Y, Z, levels=10, colors='black', alpha=0.4)\n\n# ç»˜åˆ¶çº¦æŸæ¡ä»¶\ny_constraint = 1 - x_const\nplt.plot(x_const, y_constraint, 'r-', linewidth=2, label='x + y = 1')\nplt.fill_between(x_const, y_constraint, 2, alpha=0.3, color='gray', label='å¯è¡ŒåŸŸ')\n\nplt.plot(result.x[0], result.x[1], 'go', markersize=10, label='æœ€ä¼˜è§£')\nplt.xlim(0, 2)\nplt.ylim(0, 2)\nplt.title('çº¦æŸä¼˜åŒ–é—®é¢˜')\nplt.legend()\nplt.tight_layout()\nplt.show()\n```\n\n### æ–¹ç¨‹æ±‚è§£\n```python\nimport numpy as np\nfrom scipy import optimize\nimport matplotlib.pyplot as plt\n\nprint(\"=== éçº¿æ€§æ–¹ç¨‹æ±‚è§£ ===\")\n\n# å®šä¹‰éçº¿æ€§æ–¹ç¨‹ç»„\ndef equations(vars):\n    x, y = vars\n    eq1 = x**2 + y**2 - 25\n    eq2 = x**2 - y - 5\n    return [eq1, eq2]\n\n# åˆå§‹çŒœæµ‹\ninitial_guess = [4, 2]\n\n# æ±‚è§£æ–¹ç¨‹ç»„\nresult = optimize.root(equations, initial_guess, method='hybr')\n\nprint(f\"æ±‚è§£ç»“æœ:\")\nprint(f\"è§£: x = {result.x[0]:.4f}, y = {result.x[1]:.4f}\")\nprint(f\"å‡½æ•°å€¼: {result.fun}\")\nprint(f\"æ±‚è§£æˆåŠŸ: {result.success}\")\n\n# å¯è§†åŒ–\nfig, ax = plt.subplots(figsize=(8, 8))\ncircle = plt.Circle((0, 0), 5, color='blue', fill=False, linewidth=2, label='xÂ² + yÂ² = 25')\nax.add_patch(circle)\n\nx_parabola = np.linspace(-5, 5, 100)\ny_parabola = x_parabola**2 - 5\nax.plot(x_parabola, y_parabola, 'r-', linewidth=2, label='xÂ² - y = 5')\n\n# ç»˜åˆ¶äº¤ç‚¹\nax.plot(result.x[0], result.x[1], 'go', markersize=10, label='è§£')\nax.text(result.x[0]+0.2, result.x[1]+0.2, f'({result.x[0]:.2f}, {result.x[1]:.2f})')\n\nax.set_xlim(-6, 6)\nax.set_ylim(-6, 6)\nax.set_aspect('equal')\nax.grid(True, alpha=0.3)\nax.legend()\nplt.title('éçº¿æ€§æ–¹ç¨‹ç»„æ±‚è§£')\nplt.tight_layout()\nplt.show()\n```\n\n## ğŸ“ æ•°å€¼ç§¯åˆ†\n\n### å®šç§¯åˆ†è®¡ç®—\n```python\nfrom scipy import integrate\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(\"=== æ•°å€¼ç§¯åˆ†è®¡ç®— ===\")\n\n# 1. å•å˜é‡ç§¯åˆ†\ndef func1(x):\n    return np.exp(-x**2) * np.sin(x)\n\nintegral1, error1 = integrate.quad(func1, 0, np.inf)\n\nprint(f\"ç§¯åˆ†ç»“æœ: {integral1:.6f}\")\nprint(f\"ä¼°è®¡è¯¯å·®: {error1:.2e}\")\nprint(f\"æœ‰æ•ˆä½æ•°: {-np.log10(error1/abs(integral1)):.1f}\")\n\n# å¯è§†åŒ–è¢«ç§¯å‡½æ•°\nx_plot = np.linspace(0, 3, 100)\ny_plot = func1(x_plot)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x_plot, y_plot, 'b-', linewidth=2, label='è¢«ç§¯å‡½æ•°')\nplt.fill_between(x_plot, y_plot, alpha=0.3, label=f'ç§¯åˆ†é¢ç§¯ â‰ˆ {integral1:.4f}')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.title(f'å®šç§¯åˆ†: âˆ«â‚€^âˆ e^(-xÂ²)sin(x)dx = {integral1:.6f}')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n\n### å¤šé‡ç§¯åˆ†\n```python\nfrom scipy import integrate\nimport numpy as np\n\nprint(\"=== å¤šé‡ç§¯åˆ†è®¡ç®— ===\")\n\n# äºŒé‡ç§¯åˆ†\ndef integrand(y, x):\n    return np.sin(x) * np.cos(y)\n\n# ç§¯åˆ†åŒºåŸŸ: xä»0åˆ°Ï€, yä»0åˆ°Ï€/2\nresult, error = integrate.dblquad(integrand, 0, np.pi, \n                                 lambda x: 0, \n                                 lambda x: np.pi/2)\n\nprint(f\"äºŒé‡ç§¯åˆ†ç»“æœ: {result:.6f}\")\nprint(f\"ä¼°è®¡è¯¯å·®: {error:.2e}\")\n\n# ä¸‰é‡ç§¯åˆ†\ndef integrand3(z, y, x):\n    return x * y * z\n\nresult3, error3 = integrate.tplquad(integrand3, \n                                   0, 1,                    # x bounds\n                                   lambda x: 0, \n                                   lambda x: 1 - x,        # y bounds\n                                   lambda x, y: 0, \n                                   lambda x, y: 1 - x - y) # z bounds\n\nprint(f\"\\nä¸‰é‡ç§¯åˆ†ç»“æœ: {result3:.6f}\")\nprint(f\"ä¼°è®¡è¯¯å·®: {error3:.2e}\")\nprint(f\"ç†è®ºå€¼: 1/120 = {1/120:.6f}\")\n```\n\n### å¾®åˆ†æ–¹ç¨‹æ±‚è§£\n```python\nfrom scipy import integrate\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(\"=== å¾®åˆ†æ–¹ç¨‹æ•°å€¼æ±‚è§£ ===\")\n\n# Lotka-Volterra æ•é£Ÿè€…-è¢«æ•é£Ÿè€…æ¨¡å‹\ndef ode_system(t, y):\n    alpha, beta, delta, gamma = 1.0, 0.1, 0.075, 1.5\n    prey, predator = y\n    dprey_dt = alpha * prey - beta * prey * predator\n    dpredator_dt = delta * prey * predator - gamma * predator\n    return [dprey_dt, dpredator_dt]\n\n# æ±‚è§£å¾®åˆ†æ–¹ç¨‹\nt_span = (0, 50)\ny0 = [10, 5]  # åˆå§‹ç§ç¾¤\nt_eval = np.linspace(0, 50, 1000)\nsolution = integrate.solve_ivp(ode_system, t_span, y0, t_eval=t_eval, method='RK45')\n\nprint(f\"æ±‚è§£æˆåŠŸ: {solution.success}\")\nprint(f\"è®¡ç®—æ­¥æ•°: {len(solution.t)}\")\nprint(f\"æœ€ç»ˆè¢«æ•é£Ÿè€…æ•°é‡: {solution.y[0, -1]:.2f}\")\nprint(f\"æœ€ç»ˆæ•é£Ÿè€…æ•°é‡: {solution.y[1, -1]:.2f}\")\n\n# å¯è§†åŒ–ç§ç¾¤åŠ¨æ€\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# æ—¶åŸŸå›¾\naxes[0].plot(solution.t, solution.y[0], 'g-', label='è¢«æ•é£Ÿè€…', linewidth=2)\naxes[0].plot(solution.t, solution.y[1], 'r-', label='æ•é£Ÿè€…', linewidth=2)\naxes[0].set_xlabel('æ—¶é—´')\naxes[0].set_ylabel('ç§ç¾¤æ•°é‡')\naxes[0].set_title('Lotka-Volterra æ¨¡å‹ç§ç¾¤åŠ¨æ€')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# ç›¸å›¾\naxes[1].plot(solution.y[0], solution.y[1], 'b-', linewidth=1)\naxes[1].scatter(solution.y[0, 0], solution.y[1, 0], color='green', s=100, label='èµ·ç‚¹', zorder=5)\naxes[1].scatter(solution.y[0, -1], solution.y[1, -1], color='red', s=100, label='ç»ˆç‚¹', zorder=5)\naxes[1].set_xlabel('è¢«æ•é£Ÿè€…æ•°é‡')\naxes[1].set_ylabel('æ•é£Ÿè€…æ•°é‡')\naxes[1].set_title('ç›¸å›¾')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n\n## ğŸ“¡ ä¿¡å·å¤„ç†\n\n### ä¿¡å·æ»¤æ³¢ä¸é¢‘è°±åˆ†æ\n```python\nfrom scipy import signal\nfrom scipy.fft import fft, fftfreq\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(\"=== ä¿¡å·å¤„ç†ä¸é¢‘è°±åˆ†æ ===\")\n\n# ç”Ÿæˆæµ‹è¯•ä¿¡å·\nt = np.linspace(0, 1, 1000, endpoint=False)\noriginal_signal = (np.sin(2 * np.pi * 5 * t) + \n                  0.5 * np.sin(2 * np.pi * 20 * t) + \n                  0.2 * np.sin(2 * np.pi * 50 * t))\n\n# æ·»åŠ å™ªå£°\nnp.random.seed(42)\nnoisy_signal = original_signal + 0.3 * np.random.normal(size=len(t))\n\nprint(f\"ä¿¡å·é•¿åº¦: {len(t)}\")\nprint(f\"é‡‡æ ·é¢‘ç‡: {1/(t[1]-t[0]):.0f} Hz\")\nprint(f\"å¥ˆå¥æ–¯ç‰¹é¢‘ç‡: {0.5/(t[1]-t[0]):.0f} Hz\")\n\n# è®¾è®¡ä½é€šæ»¤æ³¢å™¨\nnyquist = 0.5/(t[1]-t[0])  # å¥ˆå¥æ–¯ç‰¹é¢‘ç‡\ncutoff = 15 / nyquist\nb, a = signal.butter(4, cutoff, btype='low')\nfiltered_signal = signal.filtfilt(b, a, noisy_signal)\n\nprint(f\"æ»¤æ³¢å™¨é˜¶æ•°: 4\")\nprint(f\"æˆªæ­¢é¢‘ç‡: 15 Hz\")\n\n# å¯è§†åŒ–ä¿¡å·\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# æ—¶åŸŸä¿¡å·\naxes[0, 0].plot(t[:100], original_signal[:100], 'b-', alpha=0.7, label='åŸå§‹ä¿¡å·')\naxes[0, 0].plot(t[:100], noisy_signal[:100], 'r-', alpha=0.5, label='å¸¦å™ªå£°ä¿¡å·')\naxes[0, 0].plot(t[:100], filtered_signal[:100], 'g-', linewidth=2, label='æ»¤æ³¢åä¿¡å·')\naxes[0, 0].set_xlabel('æ—¶é—´ (s)')\naxes[0, 0].set_ylabel('å¹…åº¦')\naxes[0, 0].set_title('æ—¶åŸŸä¿¡å·ï¼ˆå‰100ç‚¹ï¼‰')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# é¢‘åŸŸåˆ†æ\nfft_original = fft(original_signal)\nfft_noisy = fft(noisy_signal)\nfft_filtered = fft(filtered_signal)\nfreqs = fftfreq(len(t), t[1] - t[0])\npositive_freq_idx = np.where((freqs > 0) & (freqs < 100))\n\naxes[0, 1].plot(freqs[positive_freq_idx], np.abs(fft_original[positive_freq_idx]), 'b-', label='åŸå§‹é¢‘è°±')\naxes[0, 1].plot(freqs[positive_freq_idx], np.abs(fft_noisy[positive_freq_idx]), 'r-', alpha=0.5, label='å™ªå£°é¢‘è°±')\naxes[0, 1].plot(freqs[positive_freq_idx], np.abs(fft_filtered[positive_freq_idx]), 'g-', label='æ»¤æ³¢é¢‘è°±')\naxes[0, 1].set_xlabel('é¢‘ç‡ (Hz)')\naxes[0, 1].set_ylabel('å¹…åº¦')\naxes[0, 1].set_title('é¢‘åŸŸåˆ†æ')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\naxes[0, 1].axvline(15, color='gray', linestyle='--', label='æˆªæ­¢é¢‘ç‡')\n\n# æ»¤æ³¢å™¨é¢‘ç‡å“åº”\nw, h = signal.freqz(b, a, worN=2000)\naxes[1, 0].plot(0.5 * w / np.pi * 500, 20 * np.log10(np.abs(h)), 'b-')\naxes[1, 0].set_xlabel('é¢‘ç‡ (Hz)')\naxes[1, 0].set_ylabel('å¢ç›Š (dB)')\naxes[1, 0].set_title('æ»¤æ³¢å™¨é¢‘ç‡å“åº”')\naxes[1, 0].grid(True, alpha=0.3)\naxes[1, 0].axvline(15, color='gray', linestyle='--', label='æˆªæ­¢é¢‘ç‡')\n\n# è¯¯å·®åˆ†æ\naxes[1, 1].plot(t[:100], filtered_signal[:100] - original_signal[:100], 'purple')\naxes[1, 1].set_xlabel('æ—¶é—´ (s)')\naxes[1, 1].set_ylabel('è¯¯å·®')\naxes[1, 1].set_title('æ»¤æ³¢è¯¯å·®ï¼ˆå‰100ç‚¹ï¼‰')\naxes[1, 1].grid(True, alpha=0.3)\naxes[1, 1].axhline(0, color='black', linestyle='-', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n\n## ğŸ§® çº¿æ€§ä»£æ•°\n\n### çŸ©é˜µè¿ç®—ä¸åˆ†è§£\n```python\nfrom scipy import linalg\nimport numpy as np\n\nprint(\"=== çº¿æ€§ä»£æ•°è¿ç®— ===\")\n\n# çŸ©é˜µè¿ç®—ç¤ºä¾‹\nA = np.array([[4, 2, 1], \n              [2, 5, 3], \n              [1, 3, 6]], dtype=float)\nb = np.array([1, 2, 3], dtype=float)\n\nprint(\"çŸ©é˜µ A:\")\nprint(A)\nprint(f\"\\nå‘é‡ b: {b}\")\n\n# çŸ©é˜µæ€§è´¨\ndet_A = linalg.det(A)\ncond_A = linalg.cond(A)\nprint(f\"\\nè¡Œåˆ—å¼: {det_A:.4f}\")\nprint(f\"æ¡ä»¶æ•°: {cond_A:.4f}\")\nprint(f\"çŸ©é˜µæ˜¯å¦å¯¹ç§°: {np.allclose(A, A.T)}\")\nprint(f\"çŸ©é˜µæ˜¯å¦æ­£å®š: {np.all(linalg.eigvals(A) > 0)}\")\n\n# çº¿æ€§æ–¹ç¨‹ç»„æ±‚è§£\nx = linalg.solve(A, b)\nprint(f\"\\næ–¹ç¨‹è§£: {x}\")\n\n# éªŒè¯è§£\nprint(f\"éªŒè¯ A*x: {A.dot(x)}\")\nprint(f\"ç›®æ ‡ b: {b}\")\nprint(f\"æ®‹å·®èŒƒæ•°: {np.linalg.norm(A.dot(x) - b):.2e}\")\n\n# ç‰¹å¾å€¼åˆ†è§£\neigenvalues, eigenvectors = linalg.eig(A)\nprint(f\"\\nç‰¹å¾å€¼: {eigenvalues}\")\nprint(\"ç‰¹å¾å‘é‡çŸ©é˜µ:\")\nprint(eigenvectors)\n\n# å¥‡å¼‚å€¼åˆ†è§£\nU, s, Vh = linalg.svd(A)\nprint(f\"\\nå¥‡å¼‚å€¼: {s}\")\nprint(f\"å¥‡å¼‚å€¼æ¡ä»¶æ•°: {s.max()/s.min():.4f}\")\n```\n\n### ç¨€ç–çŸ©é˜µå¤„ç†\n```python\nfrom scipy import sparse\nfrom scipy.sparse import linalg as splinalg\nimport numpy as np\n\nprint(\"=== ç¨€ç–çŸ©é˜µå¤„ç† ===\")\n\n# åˆ›å»ºç¨€ç–çŸ©é˜µ\nn = 100\ndiag = np.ones(n)\noffsets = [0, 1, -1]\ndata = [2*diag, -1*diag, -1*diag]\nA_sparse = sparse.diags(data, offsets, format='csr')\n\nprint(f\"ç¨€ç–çŸ©é˜µå½¢çŠ¶: {A_sparse.shape}\")\nprint(f\"éé›¶å…ƒç´ æ•°é‡: {A_sparse.nnz}\")\nprint(f\"ç¨€ç–åº¦: {100 * A_sparse.nnz / (n*n):.2f}%\")\n\n# åˆ›å»ºç¨ å¯†å‘é‡è¿›è¡Œæ¯”è¾ƒ\nb_dense = np.random.randn(n)\n\n# ç¨€ç–æ±‚è§£\nprint(\"\\nä½¿ç”¨ç¨€ç–æ±‚è§£å™¨:\")\nx_sparse = splinalg.spsolve(A_sparse, b_dense)\nprint(f\"æ±‚è§£å®Œæˆï¼Œè§£çš„å½¢çŠ¶: {x_sparse.shape}\")\n\n# ä¸ç¨ å¯†æ±‚è§£æ¯”è¾ƒ\nA_dense = A_sparse.toarray()\nprint(\"\\nä¸ç¨ å¯†æ±‚è§£å™¨æ¯”è¾ƒ:\")\nx_dense = linalg.solve(A_dense, b_dense)\nresidual = np.linalg.norm(A_dense @ x_sparse - b_dense)\nprint(f\"æ®‹å·®èŒƒæ•°: {residual:.2e}\")\nprint(f\"ä¸ç¨ å¯†è§£çš„æœ€å¤§å·®å¼‚: {np.max(np.abs(x_sparse - x_dense)):.2e}\")\n```\n\n## ğŸ“Š ç»Ÿè®¡è®¡ç®—\n\n### æ¦‚ç‡åˆ†å¸ƒä¸ç»Ÿè®¡æ£€éªŒ\n```python\nfrom scipy import stats\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(\"=== ç»Ÿè®¡è®¡ç®—ä¸æ¦‚ç‡åˆ†å¸ƒ ===\")\n\n# ç”Ÿæˆæ­£æ€åˆ†å¸ƒæ ·æœ¬\nnp.random.seed(42)\nnormal_samples = np.random.normal(loc=0, scale=1, size=1000)\n\nprint(f\"æ ·æœ¬æ•°é‡: {len(normal_samples)}\")\nprint(f\"æ ·æœ¬å‡å€¼: {np.mean(normal_samples):.4f}\")\nprint(f\"æ ·æœ¬æ ‡å‡†å·®: {np.std(normal_samples):.4f}\")\n\n# æ­£æ€æ€§æ£€éªŒ\nk2_statistic, p_value = stats.normaltest(normal_samples)\nprint(f\"\\næ­£æ€æ€§æ£€éªŒ:\")\nprint(f\"ç»Ÿè®¡é‡: {k2_statistic:.4f}\")\nprint(f\"på€¼: {p_value:.4f}\")\nprint(f\"æ˜¯å¦æ­£æ€åˆ†å¸ƒ (Î±=0.05): {p_value > 0.05}\")\n\n# æ‹Ÿåˆåˆ†å¸ƒå‚æ•°\nparams = stats.norm.fit(normal_samples)\nprint(f\"\\næ‹Ÿåˆæ­£æ€åˆ†å¸ƒå‚æ•°:\")\nprint(f\"å‡å€¼: {params[0]:.4f}\")\nprint(f\"æ ‡å‡†å·®: {params[1]:.4f}\")\n\n# å¯è§†åŒ–\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# ç›´æ–¹å›¾ä¸ç†è®ºPDF\naxes[0].hist(normal_samples, bins=30, density=True, alpha=0.7, label='æ ·æœ¬ç›´æ–¹å›¾')\nx = np.linspace(-4, 4, 100)\naxes[0].plot(x, stats.norm.pdf(x), 'r-', linewidth=2, label='ç†è®ºPDF')\naxes[0].set_xlabel('å€¼')\naxes[0].set_ylabel('æ¦‚ç‡å¯†åº¦')\naxes[0].set_title('æ­£æ€åˆ†å¸ƒæ ·æœ¬ä¸ç†è®ºPDF')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# QQå›¾\nstats.probplot(normal_samples, dist=\"norm\", plot=axes[1])\naxes[1].set_title('æ­£æ€QQå›¾')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n\n### å‡è®¾æ£€éªŒ\n```python\nfrom scipy import stats\nimport numpy as np\n\nprint(\"=== å‡è®¾æ£€éªŒ ===\")\n\n# ç”Ÿæˆä¸¤ç»„æ ·æœ¬\nnp.random.seed(42)\ngroup1 = np.random.normal(loc=10, scale=2, size=50)\ngroup2 = np.random.normal(loc=12, scale=2, size=50)\n\nprint(f\"ç¬¬ä¸€ç»„: å‡å€¼={np.mean(group1):.2f}, æ ‡å‡†å·®={np.std(group1):.2f}, n={len(group1)}\")\nprint(f\"ç¬¬äºŒç»„: å‡å€¼={np.mean(group2):.2f}, æ ‡å‡†å·®={np.std(group2):.2f}, n={len(group2)}\")\n\n# tæ£€éªŒï¼ˆç‹¬ç«‹æ ·æœ¬ï¼‰\nt_statistic, p_value = stats.ttest_ind(group1, group2)\nprint(f\"\\nç‹¬ç«‹æ ·æœ¬tæ£€éªŒ:\")\nprint(f\"tç»Ÿè®¡é‡: {t_statistic:.4f}\")\nprint(f\"på€¼: {p_value:.4f}\")\nprint(f\"æ˜¯å¦æ˜¾è‘—ä¸åŒ (Î±=0.05): {p_value < 0.05}\")\n\n# æ–¹å·®é½æ€§æ£€éªŒ\nf_statistic, p_value_var = stats.levene(group1, group2)\nprint(f\"\\næ–¹å·®é½æ€§æ£€éªŒ:\")\nprint(f\"Fç»Ÿè®¡é‡: {f_statistic:.4f}\")\nprint(f\"på€¼: {p_value_var:.4f}\")\nprint(f\"æ–¹å·®æ˜¯å¦é½ (Î±=0.05): {p_value_var > 0.05}\")\n\n# ç›¸å…³æ€§æ£€éªŒ\ncorrelation, p_value_corr = stats.pearsonr(group1, np.random.permutation(group2))\nprint(f\"\\nç›¸å…³æ€§æ£€éªŒ:\")\nprint(f\"ç›¸å…³ç³»æ•°: {correlation:.4f}\")\nprint(f\"på€¼: {p_value_corr:.4f}\")\nprint(f\"æ˜¯å¦æ˜¾è‘—ç›¸å…³ (Î±=0.05): {p_value_corr < 0.05}\")\n```\n\n## ğŸ§­ ç©ºé—´ç®—æ³•\n\n### ç©ºé—´æ•°æ®ç»“æ„\n```python\nfrom scipy import spatial\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(\"=== ç©ºé—´ç®—æ³•ä¸æ•°æ®ç»“æ„ ===\")\n\n# åˆ›å»ºéšæœºç‚¹é›†\nnp.random.seed(42)\npoints = np.random.rand(30, 2) * 10\n\nprint(f\"ç‚¹é›†å¤§å°: {points.shape}\")\nprint(f\"åæ ‡èŒƒå›´: X[{points[:,0].min():.2f}, {points[:,0].max():.2f}], \"\n      f\"Y[{points[:,1].min():.2f}, {points[:,1].max():.2f}]\")\n\n# è®¡ç®—å‡¸åŒ…\nhull = spatial.ConvexHull(points)\nprint(f\"\\nå‡¸åŒ…è®¡ç®—:\")\nprint(f\"å‡¸åŒ…é¡¶ç‚¹æ•°é‡: {len(hull.vertices)}\")\nprint(f\"å‡¸åŒ…é¢ç§¯: {hull.area:.2f}\")\nprint(f\"å‡¸åŒ…ä½“ç§¯: {hull.volume:.2f}\")\n\n# æœ€è¿‘é‚»æœç´¢\ntree = spatial.KDTree(points)\ndistances, indices = tree.query(points, k=3)  # æ¯ä¸ªç‚¹æ‰¾3ä¸ªæœ€è¿‘é‚»\nprint(f\"\\næœ€è¿‘é‚»æœç´¢:\")\nprint(f\"å¹³å‡æœ€è¿‘è·ç¦»: {distances[:,1].mean():.2f}\")\nprint(f\"æœ€è¿œæœ€è¿‘è·ç¦»: {distances[:,1].max():.2f}\")\n\n# å¯è§†åŒ–\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# ç‚¹é›†ä¸å‡¸åŒ…\naxes[0].scatter(points[:,0], points[:,1], c='blue', s=50, label='æ•°æ®ç‚¹')\nfor simplex in hull.simplices:\n    axes[0].plot(points[simplex, 0], points[simplex, 1], 'r-', linewidth=2)\naxes[0].set_title('ç©ºé—´ç‚¹é›†ä¸å‡¸åŒ…')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\naxes[0].axis('equal')\n\n# æœ€è¿‘é‚»è¿æ¥\naxes[1].scatter(points[:,0], points[:,1], c='blue', s=50, label='æ•°æ®ç‚¹')\nfor i in range(len(points)):\n    for j in range(1, 3):  # è¿æ¥ç¬¬1å’Œç¬¬2è¿‘é‚»\n        neighbor_idx = indices[i, j]\n        axes[1].plot([points[i,0], points[neighbor_idx,0]], \n                    [points[i,1], points[neighbor_idx,1]], \n                    'gray', alpha=0.3, linewidth=0.5)\naxes[1].set_title('æœ€è¿‘é‚»è¿æ¥å›¾')\naxes[1].grid(True, alpha=0.3)\naxes[1].axis('equal')\n\nplt.tight_layout()\nplt.show()\n```\n\n## âš ï¸ ä½¿ç”¨æ³¨æ„äº‹é¡¹\n\n### âœ… æ¨èåšæ³•ï¼š\n1. **æ¨¡å—å¯¼å…¥**ï¼šæŒ‰éœ€å¯¼å…¥å­æ¨¡å— `from scipy import optimize, integrate, stats`\n2. **æ•°å€¼ç¨³å®šæ€§**ï¼šæ³¨æ„çŸ©é˜µæ¡ä»¶æ•°ï¼Œä½¿ç”¨æ¡ä»¶è‰¯å¥½çš„é—®é¢˜\n3. **å†…å­˜ç®¡ç†**ï¼šå¤§æ•°æ®ä½¿ç”¨ç¨€ç–çŸ©é˜µæˆ–åˆ†å—å¤„ç†\n4. **ç»“æœéªŒè¯**ï¼šæ£€æŸ¥æ±‚è§£å™¨çš„ `success` æ ‡å¿—å’Œæ®‹å·®\n\n### âŒ é¿å…çš„æ“ä½œï¼š\n1. ä¸è¦é‡å¤è®¡ç®—å¯ç¼“å­˜çš„ç»“æœ\n2. ä¸è¦ä½¿ç”¨é»˜è®¤å‚æ•°å¤„ç†ç—…æ€é—®é¢˜\n3. ä¸è¦å¿½ç•¥æ±‚è§£å™¨çš„æ”¶æ•›çŠ¶æ€\n4. ä¸è¦åœ¨å¾ªç¯ä¸­é‡å¤åˆ›å»ºå¤§å‹æ•°ç»„\n\n### âš ï¸ å†…å­˜é™åˆ¶æé†’ï¼š\nåœ¨æ‰§è¡Œå¤§å‹è®¡ç®—å‰ï¼Œè¯·æ·»åŠ å†…å­˜ä½¿ç”¨æé†’ï¼š\n```python\n# åœ¨å¤§å‹è®¡ç®—å‰æ·»åŠ æé†’\nprint(\"æ³¨æ„ï¼šä»¥ä¸‹è®¡ç®—å¯èƒ½éœ€è¦è¾ƒå¤§å†…å­˜ï¼Œå¦‚æœ‰é—®é¢˜è¯·åˆ†å—å¤„ç†\")\n```\n\n### ğŸ“Š æ€§èƒ½ç›‘æ§ï¼š\næ·»åŠ æ€§èƒ½ç›‘æ§ä»£ç å¯ä»¥å¸®åŠ©äº†è§£è®¡ç®—èµ„æºæ¶ˆè€—ï¼š\n```python\nimport time\nimport psutil\n\nstart_time = time.time()\nprocess = psutil.Process()\ninitial_memory = process.memory_info().rss / 1024**2\n\n# ... æ‰§è¡Œè®¡ç®— ...\n\nend_time = time.time()\nfinal_memory = process.memory_info().rss / 1024**2\n\nprint(f\"è®¡ç®—æ—¶é—´: {end_time - start_time:.2f}ç§’\")\nprint(f\"å†…å­˜ä½¿ç”¨: {final_memory - initial_memory:.2f} MB\")\n```\n\n### ğŸ”§ é”™è¯¯å¤„ç†ï¼š\n```python\n# åœ¨å…³é”®è®¡ç®—å‘¨å›´æ·»åŠ try-except\ntry:\n    result = optimize.minimize_scalar(func, bounds=(0, 10))\n    if result.success:\n        print(f\"ä¼˜åŒ–æˆåŠŸ: x={result.x:.4f}\")\n    else:\n        print(f\"ä¼˜åŒ–å¤±è´¥: {result.message}\")\nexcept Exception as e:\n    print(f\"è®¡ç®—é”™è¯¯: {e}\")\n    # æä¾›æ›¿ä»£æ–¹æ¡ˆ\n    print(\"å°è¯•ä½¿ç”¨ä¸åŒçš„åˆå§‹å€¼æˆ–æ–¹æ³•...\")\n```\n\n### ğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®ï¼š\n```python\n# 1. ä½¿ç”¨å‘é‡åŒ–æ“ä½œæ›¿ä»£å¾ªç¯\n# 2. å¯¹äºå¤§å‹çº¿æ€§ç³»ç»Ÿï¼Œä½¿ç”¨ç¨€ç–çŸ©é˜µ\n# 3. é‡å¤è®¡ç®—æ—¶ç¼“å­˜ä¸­é—´ç»“æœ\n# 4. ä½¿ç”¨é€‚å½“ç²¾åº¦ï¼Œé¿å…ä¸å¿…è¦çš„é«˜ç²¾åº¦è®¡ç®—\n# 5. å¤§å‹è®¡ç®—å‰æ·»åŠ å†…å­˜ä½¿ç”¨æé†’\n# 6. ç›‘æ§è®¡ç®—æ—¶é—´å’Œå†…å­˜æ¶ˆè€—\n# 7. ä¸ºå…³é”®è®¡ç®—æ·»åŠ é”™è¯¯å¤„ç†æœºåˆ¶\n```\n\n## ğŸ“‹ å¿«é€Ÿå‚è€ƒå¡\n\n```python\n# ä¼˜åŒ–\nfrom scipy import optimize\nresult = optimize.minimize(f, x0, method='BFGS')\n\n# ç§¯åˆ†\nfrom scipy import integrate\nresult, error = integrate.quad(f, a, b)\n\n# ä¿¡å·å¤„ç†\nfrom scipy import signal\nfiltered = signal.filtfilt(b, a, signal)\n\n# çº¿æ€§ä»£æ•°\nfrom scipy import linalg\nx = linalg.solve(A, b)\n\n# ç»Ÿè®¡\nfrom scipy import stats\nt, p = stats.ttest_ind(group1, group2)\n\n# ç©ºé—´ç®—æ³•\nfrom scipy import spatial\nhull = spatial.ConvexHull(points)\n```\n\n## ğŸš€ é«˜çº§åº”ç”¨ç¤ºä¾‹\n\n### å…¨å±€ä¼˜åŒ–\n```python\nimport numpy as np\nfrom scipy import optimize\nimport matplotlib.pyplot as plt\n\nprint(\"=== å…¨å±€ä¼˜åŒ–é—®é¢˜ ===\")\n\n# å¤šå³°å‡½æ•°\ndef multimodal_func(x):\n    return np.sin(5*x) + 0.5*x**2 + 0.1*np.random.randn() if len(x.shape)==0 else 0.1*np.random.randn(x.shape[0])\n\n# ä½¿ç”¨ basinhopping è¿›è¡Œå…¨å±€ä¼˜åŒ–\nresult = optimize.basinhopping(multimodal_func, x0=0, niter=100, \n                              stepsize=1.0, minimizer_kwargs={\"method\": \"BFGS\"})\n\nprint(f\"å…¨å±€ä¼˜åŒ–ç»“æœ:\")\nprint(f\"æœ€ä¼˜è§£: x = {result.x[0]:.4f}\")\nprint(f\"æœ€ä¼˜å€¼: {result.fun:.4f}\")\nprint(f\"å‘ç°å±€éƒ¨æå€¼æ•°é‡: {result.nit}\")\n\n# å¯è§†åŒ–\nx_plot = np.linspace(-5, 5, 1000)\ny_plot = np.sin(5*x_plot) + 0.5*x_plot**2\n\nplt.figure(figsize=(12, 6))\nplt.plot(x_plot, y_plot, 'b-', linewidth=2, label='ç›®æ ‡å‡½æ•°')\nplt.axvline(result.x, color='red', linestyle='--', linewidth=2, label='å…¨å±€æœ€ä¼˜è§£')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.title('å¤šå³°å‡½æ•°å…¨å±€ä¼˜åŒ–')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n\n### åå¾®åˆ†æ–¹ç¨‹æ±‚è§£\n```python\nimport numpy as np\nfrom scipy import integrate\nimport matplotlib.pyplot as plt\n\nprint(\"=== åå¾®åˆ†æ–¹ç¨‹æ•°å€¼æ±‚è§£ ===\")\n\n# çƒ­ä¼ å¯¼æ–¹ç¨‹ï¼ˆç®€åŒ–ç¤ºä¾‹ï¼‰\n# ä½¿ç”¨æœ‰é™å·®åˆ†æ³•\nL = 1.0  # æ†é•¿\nN = 50   # ç©ºé—´ç½‘æ ¼æ•°\nT = 0.5  # æ€»æ—¶é—´\ndt = 0.001  # æ—¶é—´æ­¥é•¿\n\n# ç©ºé—´ç½‘æ ¼\nx = np.linspace(0, L, N+1)\ndx = x[1] - x[0]\n\n# åˆå§‹æ¡ä»¶ï¼ˆä¸­å¿ƒåŠ çƒ­ï¼‰\nu = np.exp(-100*(x - L/2)**2)\n\n# æ—¶é—´æ­¥è¿›\nfor n in range(int(T/dt)):\n    # ä½¿ç”¨æ˜¾å¼æ¬§æ‹‰æ³•\n    u[1:-1] = u[1:-1] + dt/dx**2 * (u[:-2] - 2*u[1:-1] + u[2:])\n\nprint(f\"çƒ­ä¼ å¯¼æ–¹ç¨‹æ•°å€¼æ±‚è§£å®Œæˆ\")\nprint(f\"ç©ºé—´ç½‘æ ¼: {N+1} ç‚¹\")\nprint(f\"æ—¶é—´æ­¥æ•°: {int(T/dt)}\")\nprint(f\"æœ€ç»ˆæ¸©åº¦åˆ†å¸ƒèŒƒå›´: [{u.min():.4f}, {u.max():.4f}]\")\n\n# å¯è§†åŒ–\nplt.figure(figsize=(10, 6))\nplt.plot(x, np.exp(-100*(x - L/2)**2), 'b--', linewidth=2, label='åˆå§‹æ¸©åº¦åˆ†å¸ƒ')\nplt.plot(x, u, 'r-', linewidth=2, label=f'æœ€ç»ˆæ¸©åº¦åˆ†å¸ƒ (t={T})')\nplt.xlabel('ä½ç½® x')\nplt.ylabel('æ¸©åº¦ u(x,t)')\nplt.title('çƒ­ä¼ å¯¼æ–¹ç¨‹æ•°å€¼è§£')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n\n---\n\n**è®°ä½**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰è¾“å‡ºæ ¼å¼ï¼Œæ‚¨åªéœ€è¦ä¸“æ³¨äºç§‘å­¦è®¡ç®—é€»è¾‘ï¼SciPy å‡½æ•°ä¼šä»¥é€‚å½“æ ¼å¼æ˜¾ç¤ºç»“æœï¼Œå¤æ‚è®¡ç®—ä¹Ÿä¼šè¢«æ­£ç¡®å¤„ç†ã€‚",
        "sympy_cookbook.md": "# SymPy ç¬¦å·æ•°å­¦æŒ‡å— (v2.5)\n\n## ğŸ¯ å·¥å…·æ¦‚è¿°\n**åŠŸèƒ½**ï¼šç¬¦å·æ•°å­¦è®¡ç®—ï¼ŒåŒ…æ‹¬æ–¹ç¨‹æ±‚è§£ã€å¾®ç§¯åˆ†ã€ä»£æ•°è¿ç®—ç­‰\n**è¾“å‡ºåŸåˆ™**ï¼šç›´æ¥æ‰“å°ç»“æœï¼Œç³»ç»Ÿè‡ªåŠ¨å¤„ç†è¾“å‡ºæ ¼å¼\n\n## âœ… ä»£ç è§£é‡Šå™¨é€‚é…è¯´æ˜\n- **ç›´æ¥æ‰“å°**ï¼šæ‰€æœ‰è®¡ç®—ç»“æœç›´æ¥ä½¿ç”¨ `print()` è¾“å‡º\n- **ç¬¦å·è¡¨è¾¾å¼**ï¼šSymPy è¡¨è¾¾å¼ä¼šä»¥ç¾è§‚çš„æ•°å­¦æ ¼å¼æ˜¾ç¤º\n- **è‡ªåŠ¨æ¸²æŸ“**ï¼šå¤æ‚æ•°å­¦å…¬å¼ä¼šè‡ªåŠ¨è½¬æ¢ä¸ºæ˜“è¯»æ ¼å¼\n- **æ•°å€¼è®¡ç®—**ï¼šéœ€è¦æ•°å€¼ç»“æœæ—¶ä½¿ç”¨ `.evalf()` æˆ– `sp.N()`\n\n## ğŸ§® åŸºç¡€ç¬¦å·è¿ç®—\n\n### ç¬¦å·å®šä¹‰ä¸åŸºæœ¬æ“ä½œ\n```python\nimport sympy as sp\n\n# å®šä¹‰ç¬¦å·å˜é‡\nx, y, z = sp.symbols('x y z')\na, b, c = sp.symbols('a b c')\n\n# åŸºæœ¬è¡¨è¾¾å¼æ“ä½œ\nexpr1 = x**2 + 2*x + 1\nexpr2 = (x + 1)**2\n\nprint(\"=== åŸºç¡€ç¬¦å·è¿ç®— ===\")\nprint(f\"è¡¨è¾¾å¼1: {expr1}\")\nprint(f\"è¡¨è¾¾å¼2: {expr2}\")\nprint(f\"è¡¨è¾¾å¼1å±•å¼€: {sp.expand(expr1)}\")\nprint(f\"è¡¨è¾¾å¼2å› å¼åˆ†è§£: {sp.factor(expr2)}\")\nprint(f\"ä¸¤ä¸ªè¡¨è¾¾å¼æ˜¯å¦ç›¸ç­‰: {expr1.equals(expr2)}\")\n\n# è¡¨è¾¾å¼ç®€åŒ–\ncomplex_expr = (x**2 - 1)/(x - 1)\nsimplified = sp.simplify(complex_expr)\nprint(f\"å¤æ‚è¡¨è¾¾å¼: {complex_expr}\")\nprint(f\"ç®€åŒ–å: {simplified}\")\n```\n\n## ğŸ¯ æ–¹ç¨‹æ±‚è§£\n\n### ä»£æ•°æ–¹ç¨‹æ±‚è§£\n```python\nimport sympy as sp\n\nx, y, z = sp.symbols('x y z')\n\nprint(\"=== ä»£æ•°æ–¹ç¨‹æ±‚è§£ ===\")\n\n# ä¸€å…ƒäºŒæ¬¡æ–¹ç¨‹\neq1 = sp.Eq(x**2 - 5*x + 6, 0)\nsolutions1 = sp.solve(eq1, x)\nprint(f\"æ–¹ç¨‹: {eq1}\")\nprint(f\"è§£: {solutions1}\")\n\n# çº¿æ€§æ–¹ç¨‹ç»„\neq2 = sp.Eq(2*x + 3*y, 7)\neq3 = sp.Eq(4*x - y, 1)\nsolutions2 = sp.solve([eq2, eq3], (x, y))\nprint(f\"\\næ–¹ç¨‹ç»„:\")\nprint(f\"  {eq2}\")\nprint(f\"  {eq3}\")\nprint(f\"è§£: {solutions2}\")\n\n# éçº¿æ€§æ–¹ç¨‹æ•°å€¼è§£\neq4 = sp.Eq(sp.sin(x) - x/2, 0)\nsolution4 = sp.nsolve(eq4, x, 1)  # ä»x=1å¼€å§‹æ•°å€¼æ±‚è§£\nprint(f\"\\néçº¿æ€§æ–¹ç¨‹: {eq4}\")\nprint(f\"æ•°å€¼è§£: {solution4}\")\n```\n\n## ğŸ“ å¾®ç§¯åˆ†è¿ç®—\n\n### å¾®åˆ†è®¡ç®—\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\n\nprint(\"=== å¾®åˆ†è®¡ç®— ===\")\n\n# å®šä¹‰å‡½æ•°\nf = x**3 + 2*x**2 + sp.sin(x)\nprint(f\"å‡½æ•°: f(x) = {f}\")\n\n# ä¸€é˜¶å¯¼æ•°\nf_prime = sp.diff(f, x)\nprint(f\"ä¸€é˜¶å¯¼æ•°: f'(x) = {f_prime}\")\n\n# äºŒé˜¶å¯¼æ•°\nf_double_prime = sp.diff(f, x, 2)\nprint(f\"äºŒé˜¶å¯¼æ•°: f''(x) = {f_double_prime}\")\n\n# åå¯¼æ•°ï¼ˆå¤šå˜é‡ï¼‰\ny = sp.symbols('y')\ng = x**2 * y + sp.sin(x*y)\ng_x = sp.diff(g, x)\ng_y = sp.diff(g, y)\nprint(f\"\\nå¤šå˜é‡å‡½æ•°: g(x,y) = {g}\")\nprint(f\"å¯¹xåå¯¼: âˆ‚g/âˆ‚x = {g_x}\")\nprint(f\"å¯¹yåå¯¼: âˆ‚g/âˆ‚y = {g_y}\")\n```\n\n### ç§¯åˆ†è®¡ç®—\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\n\nprint(\"=== ç§¯åˆ†è®¡ç®— ===\")\n\n# ä¸å®šç§¯åˆ†\nf = x**2 + sp.sin(x)\nindefinite = sp.integrate(f, x)\nprint(f\"å‡½æ•°: f(x) = {f}\")\nprint(f\"ä¸å®šç§¯åˆ†: âˆ«f(x)dx = {indefinite} + C\")\n\n# å®šç§¯åˆ†\ndefinite = sp.integrate(f, (x, 0, sp.pi))\nprint(f\"å®šç§¯åˆ† [0,Ï€]: âˆ«â‚€^Ï€ f(x)dx = {definite}\")\nprint(f\"æ•°å€¼ç»“æœ: {definite.evalf()}\")\n\n# å¤šé‡ç§¯åˆ†\ny = sp.symbols('y')\ndouble_int = sp.integrate(x*y, (x, 0, 1), (y, 0, 2))\nprint(f\"\\näºŒé‡ç§¯åˆ†: âˆ«â‚€Â¹âˆ«â‚€Â² xy dy dx = {double_int}\")\n```\n\n### æé™è®¡ç®—\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\n\nprint(\"=== æé™è®¡ç®— ===\")\n\n# åŸºæœ¬æé™\nlimit1 = sp.limit(sp.sin(x)/x, x, 0)\nprint(f\"lim(xâ†’0) sin(x)/x = {limit1}\")\n\n# æ— ç©·æé™\nlimit2 = sp.limit(1/x, x, 0, '+')  # ä»æ­£æ–¹å‘é€¼è¿‘\nlimit3 = sp.limit(1/x, x, 0, '-')  # ä»è´Ÿæ–¹å‘é€¼è¿‘\nprint(f\"lim(xâ†’0âº) 1/x = {limit2}\")\nprint(f\"lim(xâ†’0â») 1/x = {limit3}\")\n\n# å¤æ‚æé™\nlimit4 = sp.limit((1 + 1/x)**x, x, sp.oo)\nprint(f\"lim(xâ†’âˆ) (1 + 1/x)Ë£ = {limit4}\")\n```\n\n## ğŸ” æ•°å­¦è¯æ˜ä¸æ’ç­‰å¼\n\n### ä»£æ•°æ’ç­‰å¼éªŒè¯\n```python\nimport sympy as sp\n\na, b, x = sp.symbols('a b x')\n\nprint(\"=== æ•°å­¦æ’ç­‰å¼éªŒè¯ ===\")\n\n# éªŒè¯ (a+b)Â² = aÂ² + 2ab + bÂ²\nlhs1 = (a + b)**2\nrhs1 = a**2 + 2*a*b + b**2\nidentity1 = sp.simplify(lhs1 - rhs1) == 0\nprint(f\"(a+b)Â² = aÂ² + 2ab + bÂ²: {identity1}\")\n\n# éªŒè¯ä¸‰è§’æ’ç­‰å¼ sinÂ²x + cosÂ²x = 1\nlhs2 = sp.sin(x)**2 + sp.cos(x)**2\nrhs2 = 1\nidentity2 = sp.simplify(lhs2 - rhs2) == 0\nprint(f\"sinÂ²x + cosÂ²x = 1: {identity2}\")\n\n# éªŒè¯æ¬§æ‹‰å…¬å¼\ntheta = sp.symbols('theta')\neuler_lhs = sp.exp(sp.I * theta)\neuler_rhs = sp.cos(theta) + sp.I * sp.sin(theta)\neuler_identity = sp.simplify(euler_lhs - euler_rhs) == 0\nprint(f\"e^(iÎ¸) = cosÎ¸ + i sinÎ¸: {euler_identity}\")\n```\n\n## ğŸ§© çº¿æ€§ä»£æ•°\n\n### çŸ©é˜µè¿ç®—\n```python\nimport sympy as sp\n\nprint(\"=== çŸ©é˜µè¿ç®— ===\")\n\n# å®šä¹‰ç¬¦å·çŸ©é˜µ\nA = sp.Matrix([[1, 2], [3, 4]])\nB = sp.Matrix([[2, 0], [1, 2]])\n\nprint(f\"çŸ©é˜µ A:\\n{A}\")\nprint(f\"çŸ©é˜µ B:\\n{B}\")\n\n# åŸºæœ¬è¿ç®—\nprint(f\"\\nçŸ©é˜µåŠ æ³• A+B:\\n{A + B}\")\nprint(f\"çŸ©é˜µä¹˜æ³• AÃ—B:\\n{A * B}\")\nprint(f\"Açš„è¡Œåˆ—å¼: {A.det()}\")\nprint(f\"Açš„é€†çŸ©é˜µ:\\n{A.inv()}\")\n\n# ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡\neigenvals = A.eigenvals()\neigenvects = A.eigenvects()\nprint(f\"\\nAçš„ç‰¹å¾å€¼: {eigenvals}\")\nprint(f\"Açš„ç‰¹å¾å‘é‡: {eigenvects}\")\n\n# è§£çº¿æ€§æ–¹ç¨‹ç»„\nx1, x2 = sp.symbols('x1 x2')\neq1 = sp.Eq(2*x1 + 3*x2, 7)\neq2 = sp.Eq(4*x1 + 5*x2, 13)\nsolution = sp.solve([eq1, eq2], (x1, x2))\nprint(f\"\\næ–¹ç¨‹ç»„:\")\nprint(f\"  {eq1}\")\nprint(f\"  {eq2}\")\nprint(f\"è§£: {solution}\")\n```\n\n## ğŸ“ˆ çº§æ•°å±•å¼€ä¸æ•°å€¼è®¡ç®—\n\n### æ³°å‹’çº§æ•°å±•å¼€\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\n\nprint(\"=== çº§æ•°å±•å¼€ ===\")\n\n# å¸¸ç”¨å‡½æ•°çš„æ³°å‹’å±•å¼€\nsin_series = sp.sin(x).series(x, 0, 6)  # åœ¨0å¤„å±•å¼€åˆ°6é˜¶\ncos_series = sp.cos(x).series(x, 0, 6)\nexp_series = sp.exp(x).series(x, 0, 5)\n\nprint(f\"sin(x)çš„æ³°å‹’å±•å¼€: {sin_series}\")\nprint(f\"cos(x)çš„æ³°å‹’å±•å¼€: {cos_series}\")\nprint(f\"e^xçš„æ³°å‹’å±•å¼€: {exp_series}\")\n\n# æ•°å€¼è¿‘ä¼¼\nprint(f\"\\næ•°å€¼è¿‘ä¼¼:\")\nprint(f\"Ï€ â‰ˆ {sp.N(sp.pi, 10)}\")  # 10ä½ç²¾åº¦\nprint(f\"e â‰ˆ {sp.N(sp.E, 8)}\")    # 8ä½ç²¾åº¦\nprint(f\"âˆš2 â‰ˆ {sp.N(sp.sqrt(2), 6)}\")\n\n# ç¬¦å·è¡¨è¾¾å¼çš„æ•°å€¼è®¡ç®—\nexpr = sp.integrate(sp.sin(x), (x, 0, sp.pi/2))\nnumerical_result = sp.N(expr)\nprint(f\"\\nç¬¦å·ç§¯åˆ†: âˆ«â‚€^(Ï€/2) sin(x) dx = {expr}\")\nprint(f\"æ•°å€¼ç»“æœ: {numerical_result}\")\n```\n\n## ğŸ“ å¤æ‚æ•°å­¦é—®é¢˜\n\n### å‡½æ•°åˆ†æä¸æå€¼\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\n\nprint(\"=== å‡½æ•°åˆ†æä¸æå€¼ ===\")\n\n# å®šä¹‰å‡½æ•°\nf = x**3 - 6*x**2 + 9*x + 1\nprint(f\"å‡½æ•°: f(x) = {f}\")\n\n# æ±‚å¯¼æ‰¾ä¸´ç•Œç‚¹\nf_prime = sp.diff(f, x)\ncritical_points = sp.solve(f_prime, x)\nprint(f\"ä¸€é˜¶å¯¼æ•°: f'(x) = {f_prime}\")\nprint(f\"ä¸´ç•Œç‚¹: {critical_points}\")\n\n# äºŒé˜¶å¯¼æ•°æµ‹è¯•\nf_double_prime = sp.diff(f, x, 2)\nfor point in critical_points:\n    second_deriv_val = f_double_prime.subs(x, point)\n    if second_deriv_val > 0:\n        extremum_type = \"å±€éƒ¨æå°å€¼\"\n    elif second_deriv_val < 0:\n        extremum_type = \"å±€éƒ¨æå¤§å€¼\"\n    else:\n        extremum_type = \"éœ€è¦è¿›ä¸€æ­¥åˆ†æ\"\n    print(f\"ç‚¹ x = {point}: {extremum_type}\")\n\n# å‡½æ•°å€¼\nfor point in critical_points:\n    func_val = f.subs(x, point)\n    print(f\"f({point}) = {func_val}\")\n```\n\n### æ›²çº¿æ€§è´¨åˆ†æ\n```python\nimport sympy as sp\n\nx = sp.symbols('x')\n\nprint(\"=== æ›²çº¿æ€§è´¨åˆ†æ ===\")\n\nf = x**2 * sp.sin(x)\n\n# æ›²çº¿é•¿åº¦ï¼ˆå¼§é•¿ï¼‰\ncurve_length = sp.integrate(sp.sqrt(1 + sp.diff(f, x)**2), (x, 0, sp.pi))\nprint(f\"å‡½æ•°: f(x) = {f}\")\nprint(f\"æ›²çº¿åœ¨ [0,Ï€] ä¸Šçš„é•¿åº¦: {sp.N(curve_length)}\")\n\n# æ—‹è½¬ä½“ä½“ç§¯\nvolume = sp.pi * sp.integrate(f**2, (x, 0, sp.pi))\nprint(f\"æ›²çº¿ç»•xè½´æ—‹è½¬çš„ä½“ç§¯: {sp.N(volume)}\")\n\n# æ›²ç‡\nf_prime = sp.diff(f, x)\nf_double_prime = sp.diff(f, x, 2)\ncurvature = f_double_prime / (1 + f_prime**2)**(3/2)\nprint(f\"æ›²ç‡å…¬å¼: Îº(x) = {curvature}\")\n```\n\n## ğŸ’¡ å®ç”¨å·¥å…·å‡½æ•°\n\n### è‡ªåŠ¨éªŒè¯ç­‰å¼\n```python\nimport sympy as sp\n\ndef verify_identity(expr1, expr2, method=\"simplify\"):\n    \"\"\"\n    éªŒè¯ä¸¤ä¸ªè¡¨è¾¾å¼æ˜¯å¦æ’ç­‰\n    method: \"simplify\", \"expand\", \"factor\", \"trigsimp\"\n    \"\"\"\n    if method == \"simplify\":\n        difference = sp.simplify(expr1 - expr2)\n    elif method == \"expand\":\n        difference = sp.expand(expr1 - expr2)\n    elif method == \"factor\":\n        difference = sp.factor(expr1 - expr2)\n    elif method == \"trigsimp\":\n        difference = sp.trigsimp(expr1 - expr2)\n    else:\n        difference = expr1 - expr2\n    \n    is_identity = (difference == 0)\n    \n    print(f\"è¡¨è¾¾å¼1: {expr1}\")\n    print(f\"è¡¨è¾¾å¼2: {expr2}\")\n    print(f\"éªŒè¯æ–¹æ³•: {method}\")\n    print(f\"æ˜¯å¦æ’ç­‰: {is_identity}\")\n    \n    return is_identity\n\n# ä½¿ç”¨ç¤ºä¾‹\nx, y = sp.symbols('x y')\nverify_identity((x + y)**2, x**2 + 2*x*y + y**2, \"expand\")\n```\n\n## ğŸ”§ ä»£ç è§£é‡Šå™¨é€‚é…ä¼˜åŒ–\n\n### SymPy ä¸å›¾è¡¨é›†æˆ\n```python\nimport sympy as sp\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = sp.symbols('x')\n\nprint(\"=== SymPy ä¸ Matplotlib é›†æˆ ===\")\n\n# å®šä¹‰ç¬¦å·å‡½æ•°\nf_sym = sp.sin(x) * sp.exp(-x/5)\n\n# è½¬æ¢ä¸ºæ•°å€¼å‡½æ•°ç”¨äºç»˜å›¾\nf_num = sp.lambdify(x, f_sym, 'numpy')\n\n# åˆ›å»ºæ•°æ®ç‚¹\nx_vals = np.linspace(0, 20, 400)\ny_vals = f_num(x_vals)\n\n# ç»˜å›¾\nplt.figure(figsize=(10, 6))\nplt.plot(x_vals, y_vals, 'b-', linewidth=2, label='f(x) = sin(x)Â·e^(-x/5)')\nplt.title('SymPy ç¬¦å·å‡½æ•°å¯è§†åŒ–')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.grid(True, alpha=0.3)\nplt.legend()\n\n# è®¡ç®—å¹¶æ ‡è®°æå€¼ç‚¹\nf_prime_sym = sp.diff(f_sym, x)\ncritical_points = sp.solve(f_prime_sym, x)\n\n# ç­›é€‰å®æ•°è§£\nreal_critical_points = [cp.evalf() for cp in critical_points if cp.is_real]\nfor cp in real_critical_points:\n    if 0 <= cp <= 20:\n        y_cp = f_sym.subs(x, cp).evalf()\n        plt.plot(cp, y_cp, 'ro', markersize=8)\n        plt.text(cp, y_cp + 0.1, f'({cp:.2f}, {y_cp:.2f})', \n                ha='center', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n```\n\n## âš ï¸ ä½¿ç”¨æ³¨æ„äº‹é¡¹\n\n### âœ… æ¨èåšæ³•ï¼š\n1. **æ ‡å‡†å¯¼å…¥**ï¼š`import sympy as sp`\n2. **ç¬¦å·å®šä¹‰**ï¼šæ˜ç¡®ä½¿ç”¨ `sp.symbols()` å®šä¹‰å˜é‡\n3. **æ•°å€¼è®¡ç®—**ï¼šéœ€è¦æ•°å€¼ç»“æœæ—¶ä½¿ç”¨ `.evalf()` æˆ– `sp.N()`\n4. **ç›´æ¥æ‰“å°**ï¼šä½¿ç”¨ `print()` è¾“å‡ºæ‰€æœ‰ç»“æœ\n\n### âŒ é¿å…çš„æ“ä½œï¼š\n1. ä¸è¦æ‰‹åŠ¨æ„å»º JSON è¾“å‡º\n2. ä¸è¦ä½¿ç”¨å¤æ‚çš„è‡ªå®šä¹‰è¾“å‡ºæ ¼å¼\n3. ä¸è¦çœç•¥ç¬¦å·å®šä¹‰ç›´æ¥ä½¿ç”¨å˜é‡\n\n### ğŸ”§ é”™è¯¯å¤„ç†ï¼š\n```python\ntry:\n    import sympy as sp\n    x = sp.symbols('x')\n    result = sp.solve(x**2 - 1, x)\n    print(f\"æ–¹ç¨‹è§£: {result}\")\nexcept ImportError:\n    print(\"SymPy ä¸å¯ç”¨\")\nexcept Exception as e:\n    print(f\"è®¡ç®—é”™è¯¯: {e}\")\n```\n\n### ğŸ’¡ å®ç”¨æŠ€å·§ï¼š\n```python\n# å¿«é€Ÿè·å–ç¬¦å·è¡¨è¾¾å¼çš„æ•°å€¼è¿‘ä¼¼\nexpr = sp.integrate(sp.sin(x**2), (x, 0, 1))\nprint(f\"ç¬¦å·ç»“æœ: {expr}\")\nprint(f\"æ•°å€¼è¿‘ä¼¼: {expr.evalf(10)}\")  # 10ä½ç²¾åº¦\n\n# ç”ŸæˆLaTeXä»£ç ç”¨äºæ–‡æ¡£\nlatex_code = sp.latex(expr)\nprint(f\"LaTeXä»£ç : {latex_code}\")\n\n# æ¼‚äº®æ‰“å°\nsp.pprint(expr, use_unicode=True)\n```\n\n## ğŸ“‹ å¿«é€Ÿå‚è€ƒå¡\n\n```python\nimport sympy as sp\n\n# å®šä¹‰ç¬¦å·\nx, y = sp.symbols('x y')\n\n# æ–¹ç¨‹æ±‚è§£\nsp.solve(x**2 - 4, x)  # [-2, 2]\n\n# å¾®åˆ†\nsp.diff(sp.sin(x), x)  # cos(x)\n\n# ç§¯åˆ†\nsp.integrate(x**2, x)  # xÂ³/3\n\n# æé™\nsp.limit(sp.sin(x)/x, x, 0)  # 1\n\n# çº§æ•°å±•å¼€\nsp.sin(x).series(x, 0, 4)  # x - xÂ³/6 + O(xâµ)\n\n# çŸ©é˜µè¿ç®—\nA = sp.Matrix([[1, 2], [3, 4]])\nA.det()  # -2\n```\n\n## ğŸš€ é«˜çº§åº”ç”¨ç¤ºä¾‹\n\n### å¾®åˆ†æ–¹ç¨‹æ±‚è§£\n```python\nimport sympy as sp\n\nt = sp.symbols('t')\ny = sp.Function('y')\n\nprint(\"=== å¾®åˆ†æ–¹ç¨‹æ±‚è§£ ===\")\n\n# å®šä¹‰å¾®åˆ†æ–¹ç¨‹ï¼šy'' + y = 0\node = sp.Eq(sp.diff(y(t), t, 2) + y(t), 0)\n\n# æ±‚è§£\nsolution = sp.dsolve(ode, y(t))\nprint(f\"å¾®åˆ†æ–¹ç¨‹: {ode}\")\nprint(f\"é€šè§£: {solution}\")\n\n# æ·»åŠ åˆå§‹æ¡ä»¶ï¼šy(0)=1, y'(0)=0\nics = {y(0): 1, y(t).diff(t).subs(t, 0): 0}\nparticular_solution = sp.dsolve(ode, y(t), ics=ics)\nprint(f\"ç‰¹è§£: {particular_solution}\")\n```\n\n### ç¬¦å·ä¼˜åŒ–é—®é¢˜\n```python\nimport sympy as sp\n\nx, y = sp.symbols('x y', real=True)\n\nprint(\"=== ç¬¦å·ä¼˜åŒ–é—®é¢˜ ===\")\n\n# ç›®æ ‡å‡½æ•°å’Œçº¦æŸ\nf = x**2 + y**2  # æœ€å°åŒ– xÂ² + yÂ²\nconstraint = sp.Eq(x + y, 1)  # çº¦æŸ x + y = 1\n\n# ä½¿ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•\nlam = sp.symbols('Î»')\nL = f + lam * (x + y - 1)\n\n# æ±‚åå¯¼\neq1 = sp.Eq(sp.diff(L, x), 0)\neq2 = sp.Eq(sp.diff(L, y), 0)\neq3 = sp.Eq(sp.diff(L, sp.symbols('Î»')), 0)\n\n# æ±‚è§£æ–¹ç¨‹ç»„\nsolution = sp.solve([eq1, eq2, eq3], (x, y, sp.symbols('Î»')))\nprint(f\"ä¼˜åŒ–é—®é¢˜: æœ€å°åŒ– {f}, çº¦æŸ {constraint}\")\nprint(f\"æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•è§£: {solution}\")\n\n# éªŒè¯ç»“æœ\noptimal_point = solution[0]\nprint(f\"æœ€ä¼˜ç‚¹: x={optimal_point[0]}, y={optimal_point[1]}\")\nprint(f\"æœ€ä¼˜å€¼: {f.subs({x: optimal_point[0], y: optimal_point[1]})}\")\n```\n\n---\n\n**è®°ä½**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰è¾“å‡ºæ ¼å¼ï¼Œæ‚¨åªéœ€è¦ä¸“æ³¨äºç¬¦å·æ•°å­¦è®¡ç®—ï¼SymPy è¡¨è¾¾å¼ä¼šä»¥ç¾è§‚çš„æ•°å­¦æ ¼å¼è‡ªåŠ¨æ¸²æŸ“ï¼Œå¤æ‚å…¬å¼ä¹Ÿä¼šè¢«æ­£ç¡®å¤„ç†ã€‚",
        "text_analysis_cookbook.md": "# ğŸ“š æ–‡æœ¬åˆ†æä¸ç»“æ„åŒ–æå–æ•™ç¨‹ (v2.2 - æ²™ç›’ä¼˜åŒ–ç‰ˆ)\r\n\r\n## ğŸ¯ æ–‡æ¡£ç›®æ ‡\r\nä¸ºAIåŠ©æ‰‹æä¾›ä¸€å¥—**æ— éœ€ç½‘ç»œæƒé™**ã€**å®‰å…¨å¯é **çš„æ–‡æœ¬åˆ†æè§£å†³æ–¹æ¡ˆï¼Œä¸“é—¨ç”¨äºå¤„ç†å·²è·å–çš„ç½‘é¡µå†…å®¹ã€æ–‡æ¡£æ•°æ®ç­‰ç»“æ„åŒ–ä¿¡æ¯æå–ã€‚\r\n\r\n---\r\n\r\n## ğŸ§  æ ¸å¿ƒè®¾è®¡åŸåˆ™\r\n\r\n### âœ… å¿…é¡»éµå®ˆ\r\n1. **é›¶ç½‘ç»œä¾èµ–** - æ‰€æœ‰åˆ†æåŸºäºå·²æä¾›çš„æ–‡æœ¬æ•°æ®\r\n2. **å®‰å…¨ç¬¬ä¸€** - ä»…ä½¿ç”¨Pythonæ ‡å‡†åº“å’Œé¢„è£…å®‰å…¨åº“\r\n3. **æ ¼å¼æ ‡å‡†åŒ–** - è¾“å‡ºå¿…é¡»ç¬¦åˆç³»ç»Ÿå¯è¯†åˆ«çš„JSONç»“æ„\r\n4. **é”™è¯¯åŒ…å®¹æ€§** - æå–å¤±è´¥æ—¶æä¾›åˆç†çš„é»˜è®¤å€¼\r\n5. **å‡½æ•°å¼ç¼–ç¨‹** - é¿å…ä½¿ç”¨ç±»å®šä¹‰ï¼Œæ²™ç›’ç¯å¢ƒå¯¹ç±»æ”¯æŒæœ‰é™\r\n\r\n### âŒ å¿…é¡»é¿å…\r\n1. ç½‘ç»œè¯·æ±‚ã€APIè°ƒç”¨\r\n2. æ–‡ä»¶ç³»ç»Ÿè¶Šæƒè®¿é—®\r\n3. éå®‰å…¨çš„åº“å¯¼å…¥\r\n4. æ— é™å¾ªç¯æˆ–èµ„æºè€—å°½æ“ä½œ\r\n5. ç±»å®šä¹‰ï¼ˆä½¿ç”¨å‡½æ•°å¼æ›¿ä»£ï¼‰\r\n\r\n---\r\n\r\n## ğŸš€ å¿«é€Ÿå¼€å§‹æ¨¡æ¿\r\n\r\n### åœºæ™¯ä¸€ï¼šç›´æ¥åˆ†æç½‘é¡µæŠ“å–å†…å®¹\r\n```python\r\n# ===================== åŸºç¡€åˆ†ææ¨¡æ¿ =====================\r\nimport json\r\nimport re\r\nfrom datetime import datetime\r\n\r\ndef analyze_webpage_content(text_content: str) -> dict:\r\n    \"\"\"\r\n    åŸºç¡€ç½‘é¡µå†…å®¹åˆ†æå™¨\r\n    è¾“å…¥ï¼šä»»ä½•ç½‘é¡µçš„æ–‡æœ¬å†…å®¹\r\n    è¾“å‡ºï¼šç»“æ„åŒ–æå–ç»“æœ\r\n    \"\"\"\r\n    # åˆå§‹åŒ–æ ‡å‡†è¾“å‡ºç»“æ„\r\n    result = {\r\n        \"type\": \"analysis_report\",\r\n        \"title\": \"ç½‘é¡µå†…å®¹åˆ†ææŠ¥å‘Š\",\r\n        \"timestamp\": datetime.now().isoformat(),\r\n        \"data\": {\r\n            \"åŸºæœ¬ä¿¡æ¯\": {},\r\n            \"ä»·æ ¼ä¿¡æ¯\": {},\r\n            \"äº§å“è§„æ ¼\": {},\r\n            \"æå–æ‘˜è¦\": \"\"\r\n        }\r\n    }\r\n    \r\n    # 1. åŸºæœ¬ä¿¡æ¯æå–ï¼ˆç¤ºä¾‹ï¼‰\r\n    if \"äº§å“\" in text_content or \"Product\" in text_content:\r\n        result[\"data\"][\"åŸºæœ¬ä¿¡æ¯\"][\"ç±»å‹\"] = \"äº§å“é¡µé¢\"\r\n    \r\n    # 2. ä»·æ ¼æå–ï¼ˆå¤šå¸ç§æ”¯æŒï¼‰\r\n    price_patterns = {\r\n        \"USD\": r'\\$\\s*(\\d+[,\\d]*\\.?\\d*)',\r\n        \"CNY\": r'Â¥\\s*(\\d+[,\\d]*)',\r\n        \"HKD\": r'HK\\$\\s*(\\d+[,\\d]*\\.?\\d*)'\r\n    }\r\n    \r\n    for currency, pattern in price_patterns.items():\r\n        match = re.search(pattern, text_content)\r\n        if match:\r\n            result[\"data\"][\"ä»·æ ¼ä¿¡æ¯\"][currency] = match.group(1)\r\n    \r\n    # 3. å…³é”®ä¿¡æ¯æ‘˜è¦\r\n    lines = text_content.split('\\n')\r\n    key_lines = [line.strip() for line in lines if len(line.strip()) > 20][:5]\r\n    result[\"data\"][\"æå–æ‘˜è¦\"] = \" | \".join(key_lines)\r\n    \r\n    return result\r\n\r\n# ===================== æ‰§è¡Œç¤ºä¾‹ =====================\r\nif __name__ == \"__main__\":\r\n    # å°†æ‚¨çš„data_contextç²˜è´´åœ¨è¿™é‡Œ\r\n    sample_text = \"\"\"\r\n    äº§å“åç§°ï¼šJimmy Choo DIDI 45\r\n    ä»·æ ¼ï¼š$299.99\r\n    æè´¨ï¼šçš®é©é‹é¢ï¼Œç»¸ç¼å†…è¡¬\r\n    è·Ÿé«˜ï¼š45mm\r\n    ç‰¹ç‚¹ï¼šå°–å¤´è®¾è®¡ï¼Œä¼˜é›…å¥³æ€§é‹å±¥\r\n    \"\"\"\r\n    \r\n    analysis_result = analyze_webpage_content(sample_text)\r\n    \r\n    # ğŸ”¥ å…³é”®ï¼šå¿…é¡»ä½¿ç”¨printè¾“å‡ºJSONæ ¼å¼\r\n    print(json.dumps(analysis_result, ensure_ascii=False, indent=2))\r\n```\r\n\r\n### åœºæ™¯äºŒï¼šå¤šé¡µé¢æ‰¹é‡åˆ†æ\r\n```python\r\nimport json\r\n\r\ndef analyze_multiple_pages(pages_data: str) -> dict:\r\n    \"\"\"\r\n    å¤„ç†åŒ…å«å¤šä¸ªé¡µé¢çš„æ–‡æœ¬æ•°æ®\r\n    æ ¼å¼ï¼šä»¥\"## é¡µé¢\"åˆ†éš”çš„ä¸åŒé¡µé¢\r\n    \"\"\"\r\n    results = []\r\n    \r\n    # åˆ†å‰²é¡µé¢\r\n    if \"## é¡µé¢\" in pages_data:\r\n        pages = pages_data.split(\"## é¡µé¢\")[1:]\r\n        \r\n        for i, page_content in enumerate(pages[:3]):  # é™åˆ¶å‰3é¡µ\r\n            # è°ƒç”¨å•é¡µåˆ†æå™¨\r\n            page_result = analyze_webpage_content(page_content)\r\n            page_result[\"page_number\"] = i + 1\r\n            results.append(page_result)\r\n    else:\r\n        # å•é¡µæƒ…å†µ\r\n        results.append(analyze_webpage_content(pages_data))\r\n    \r\n    final_output = {\r\n        \"type\": \"multi_page_analysis\",\r\n        \"total_pages\": len(results),\r\n        \"pages\": results,\r\n        \"summary\": f\"æˆåŠŸåˆ†æ {len(results)} ä¸ªé¡µé¢\"\r\n    }\r\n    \r\n    return final_output\r\n```\r\n\r\n---\r\n\r\n## ğŸ“Š è¾“å‡ºæ ¼å¼è§„èŒƒï¼ˆç³»ç»Ÿå¼ºåˆ¶è¦æ±‚ï¼‰\r\n\r\n### âœ… æ­£ç¡®æ ¼å¼ç¤ºä¾‹\r\n```json\r\n{\r\n    \"type\": \"analysis_report\",  // å¿…é¡»å­—æ®µï¼Œå®šä¹‰è¾“å‡ºç±»å‹\r\n    \"title\": \"åˆ†ææŠ¥å‘Šæ ‡é¢˜\",     // ç”¨æˆ·å¯è§çš„æ ‡é¢˜\r\n    \"data\": {                  // å®é™…åˆ†ææ•°æ®\r\n        \"field1\": \"value1\",\r\n        \"field2\": [\"item1\", \"item2\"]\r\n    }\r\n}\r\n```\r\n\r\n### âŒ é”™è¯¯æ ¼å¼ç¤ºä¾‹\r\n```python\r\n# é”™è¯¯1ï¼šç›´æ¥æ‰“å°å­—å…¸\r\nprint(analysis_result)  # ç³»ç»Ÿæ— æ³•è§£æ\r\n\r\n# é”™è¯¯2ï¼šéJSONå­—ç¬¦ä¸²\r\nprint(\"ä»·æ ¼ï¼š$299.99\")  # ç³»ç»Ÿæ— æ³•ç»“æ„åŒ–å¤„ç†\r\n\r\n# é”™è¯¯3ï¼šç¼ºå°‘typeå­—æ®µ\r\n{\"data\": {...}}  # ç³»ç»Ÿæ— æ³•è¯†åˆ«ç±»å‹\r\n\r\n# é”™è¯¯4ï¼šä½¿ç”¨ç±»å®šä¹‰\r\nclass Extractor:  # æ²™ç›’ç¯å¢ƒå¯èƒ½ä¸æ”¯æŒ\r\n    def extract(self): pass\r\n```\r\n\r\n---\r\n\r\n## ğŸ› ï¸ ä¸“ä¸šåˆ†æå·¥å…·ç®±\r\n\r\n### 1. ä»·æ ¼æå–å™¨\r\n\r\n## ğŸ”§ ä»·æ ¼ä¿¡æ¯æå–ï¼ˆå…³é”®æ›´æ–°ï¼‰\r\n\r\n### ğŸš« ç¦æ­¢æ“ä½œ\r\n- âŒ ç±»å®šä¹‰ï¼ˆ`class PriceExtractor:`ï¼‰ - æ²™ç›’ç¯å¢ƒä¸æ”¯æŒ\r\n- âŒ ä½¿ç”¨ä¸å­˜åœ¨çš„åº“ï¼ˆå¦‚ `PriceExtractor`ï¼‰\r\n\r\n### âœ… æ¨èæ–¹æ¡ˆï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ä»·æ ¼\r\n```python\r\nimport re\r\nimport json\r\n\r\ndef extract_price_info(text):\r\n    \"\"\"ä»æ–‡æœ¬ä¸­æå–ä»·æ ¼ä¿¡æ¯\"\"\"\r\n    price_patterns = [\r\n        r'(\\$\\d+(?:\\.\\d+)?)\\s*per\\s*1[kK]\\s*tokens?',\r\n        r'(\\d+(?:\\.\\d+)?)\\s*USD\\s*per\\s*1[kK]\\s*tokens?',\r\n        r'è¾“å…¥\\s*:\\s*(\\$\\d+\\.\\d+)\\s*è¾“å‡º\\s*:\\s*(\\$\\d+\\.\\d+)',\r\n        r'(\\$\\d+(?:\\.\\d+)?)\\s*/\\s*1[kK]\\s*tokens?'\r\n    ]\r\n    \r\n    prices = []\r\n    for pattern in price_patterns:\r\n        matches = re.findall(pattern, text, re.IGNORECASE)\r\n        if matches:\r\n            prices.extend(matches)\r\n    \r\n    return {\r\n        'extraction_method': 'regex',\r\n        'price_matches': prices,\r\n        'sample_text': text[:500]  # ä¿ç•™æ ·æœ¬ç”¨äºéªŒè¯\r\n    }\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\ntext_content = \"ä»æ‰€æœ‰æ­¥éª¤æ”¶é›†çš„æ–‡æœ¬...\"\r\nprice_info = extract_price_info(text_content)\r\nprint(json.dumps(price_info, indent=2))\r\n```\r\n\r\n### 2. æŠ€æœ¯å‚æ•°æå–å™¨\r\n```python\r\nimport re\r\n\r\ndef extract_tech_specs(text):\r\n    \"\"\"æå–æŠ€æœ¯å‚æ•°\"\"\"\r\n    specs = {}\r\n    \r\n    # å‚æ•°æ•°é‡\r\n    param_match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*ä¸‡äº¿?\\s*å‚æ•°', text)\r\n    if param_match:\r\n        specs['parameter_count'] = param_match.group(1) + 'ä¸‡äº¿'\r\n    \r\n    # ä¸Šä¸‹æ–‡é•¿åº¦\r\n    context_match = re.search(r'(\\d+(?:,\\d+)?[kK]?)\\s*tokens?\\s*ä¸Šä¸‹æ–‡', text)\r\n    if context_match:\r\n        specs['context_length'] = context_match.group(1)\r\n    \r\n    # MMLU åˆ†æ•°\r\n    mmlu_match = re.search(r'MMLU\\s*[:ï¼š]?\\s*(\\d+(?:\\.\\d+)?)', text)\r\n    if mmlu_match:\r\n        specs['mmlu_score'] = float(mmlu_match.group(1))\r\n    \r\n    return specs\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\ntext_content = \"æŸæ¨¡å‹å…·æœ‰3.5ä¸‡äº¿å‚æ•°ï¼Œæ”¯æŒ128K tokensä¸Šä¸‹æ–‡é•¿åº¦ï¼ŒMMLUåˆ†æ•°ä¸º85.2\"\r\ntech_specs = extract_tech_specs(text_content)\r\nprint(json.dumps(tech_specs, ensure_ascii=False, indent=2))\r\n```\r\n\r\n### 3. è§„æ ¼æå–å™¨ï¼ˆå‡½æ•°å¼ç‰ˆæœ¬ï¼‰\r\n```python\r\nimport re\r\n\r\ndef extract_dimensions(text: str) -> dict:\r\n    \"\"\"äº§å“è§„æ ¼ä¿¡æ¯æå– - å‡½æ•°å¼ç‰ˆæœ¬\"\"\"\r\n    dimensions = {}\r\n    \r\n    # æå–å°ºå¯¸ä¿¡æ¯\r\n    patterns = {\r\n        \"height\": [r'(\\d+(?:\\.\\d+)?)\\s*(cm|mm|m)\\s*é«˜', r'é«˜åº¦[:ï¼š]\\s*(\\d+)'],\r\n        \"width\": [r'(\\d+(?:\\.\\d+)?)\\s*(cm|mm|m)\\s*å®½', r'å®½åº¦[:ï¼š]\\s*(\\d+)'],\r\n        \"weight\": [r'(\\d+(?:\\.\\d+)?)\\s*(kg|g)\\s*é‡', r'é‡é‡[:ï¼š]\\s*(\\d+)']\r\n    }\r\n    \r\n    for dim, pattern_list in patterns.items():\r\n        for pattern in pattern_list:\r\n            match = re.search(pattern, text, re.IGNORECASE)\r\n            if match:\r\n                # å¤„ç†åŒ¹é…ç»„\r\n                value = match.group(1)\r\n                unit = match.group(2) if len(match.groups()) > 1 else \"\"\r\n                dimensions[dim] = f\"{value}{unit}\"\r\n                break\r\n    \r\n    return dimensions\r\n\r\n# å¢å¼ºç‰ˆï¼šæ”¯æŒæ›´å¤šè§„æ ¼ç±»å‹\r\ndef extract_all_specs(text: str) -> dict:\r\n    \"\"\"æå–æ‰€æœ‰è§„æ ¼å‚æ•°\"\"\"\r\n    specs = {}\r\n    \r\n    # æè´¨æå–\r\n    material_match = re.search(r'æè´¨[:ï¼š]\\s*([^\\nï¼Œã€‚]+)', text)\r\n    if material_match:\r\n        specs['material'] = material_match.group(1)\r\n    \r\n    # é¢œè‰²æå–\r\n    color_match = re.search(r'é¢œè‰²[:ï¼š]\\s*([^\\nï¼Œã€‚]+)', text)\r\n    if color_match:\r\n        specs['color'] = color_match.group(1)\r\n    \r\n    # å°ºå¯¸ç»„åˆ\r\n    dimensions = extract_dimensions(text)\r\n    if dimensions:\r\n        specs['dimensions'] = dimensions\r\n    \r\n    # å‹å·æå–\r\n    model_match = re.search(r'å‹å·[:ï¼š]\\s*([A-Za-z0-9\\-_]+)', text)\r\n    if model_match:\r\n        specs['model'] = model_match.group(1)\r\n    \r\n    return specs\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\ntext_content = \"äº§å“å°ºå¯¸ï¼šé«˜åº¦45mmï¼Œå®½åº¦30cmï¼Œé‡é‡2.5kgï¼Œæè´¨ï¼šçš®é©\"\r\nspecs = extract_all_specs(text_content)\r\nprint(json.dumps(specs, ensure_ascii=False, indent=2))\r\n```\r\n\r\n### 4. å…³é”®è¯åˆ†æå™¨ï¼ˆå‡½æ•°å¼ç‰ˆæœ¬ï¼‰\r\n```python\r\ndef categorize_content(text: str) -> list:\r\n    \"\"\"åŸºäºå…³é”®è¯çš„åˆ†ç±»åˆ†æ - å‡½æ•°å¼ç‰ˆæœ¬\"\"\"\r\n    CATEGORY_KEYWORDS = {\r\n        \"å¥¢ä¾ˆå“\": [\"å¥¢ä¾ˆ\", \"é«˜ç«¯\", \"premium\", \"luxury\", \"designer\"],\r\n        \"ç”µå­äº§å“\": [\"ç”µå­\", \"æ™ºèƒ½\", \"tech\", \"digital\", \"gadget\"],\r\n        \"æœè£…é‹å±¥\": [\"æœè£…\", \"é‹\", \"wear\", \"apparel\", \"footwear\"],\r\n        \"å®¶å±…ç”¨å“\": [\"å®¶å±…\", \"å®¶å…·\", \"home\", \"furniture\", \"decor\"]\r\n    }\r\n    \r\n    text_lower = text.lower()\r\n    categories = []\r\n    \r\n    for category, keywords in CATEGORY_KEYWORDS.items():\r\n        if any(keyword.lower() in text_lower for keyword in keywords):\r\n            categories.append(category)\r\n    \r\n    return categories if categories else [\"æœªåˆ†ç±»\"]\r\n\r\n# å¢å¼ºç‰ˆï¼šå¸¦ç½®ä¿¡åº¦çš„åˆ†ç±»\r\ndef categorize_with_confidence(text: str) -> dict:\r\n    \"\"\"å¸¦ç½®ä¿¡åº¦çš„å†…å®¹åˆ†ç±»\"\"\"\r\n    CATEGORY_KEYWORDS = {\r\n        \"å¥¢ä¾ˆå“\": [\"å¥¢ä¾ˆ\", \"é«˜ç«¯\", \"premium\", \"luxury\", \"designer\", \"è±ªå\", \"å°Šäº«\"],\r\n        \"ç”µå­äº§å“\": [\"ç”µå­\", \"æ™ºèƒ½\", \"tech\", \"digital\", \"gadget\", \"æ‰‹æœº\", \"ç”µè„‘\", \"æ•°ç \"],\r\n        \"æœè£…é‹å±¥\": [\"æœè£…\", \"é‹\", \"wear\", \"apparel\", \"footwear\", \"æœé¥°\", \"ç©¿æˆ´\"],\r\n        \"å®¶å±…ç”¨å“\": [\"å®¶å±…\", \"å®¶å…·\", \"home\", \"furniture\", \"decor\", \"å®¶ç”¨\", \"æ‘†è®¾\"],\r\n        \"ç¾å¦†æŠ¤è‚¤\": [\"ç¾å¦†\", \"æŠ¤è‚¤\", \"åŒ–å¦†å“\", \"ç¾å®¹\", \"skincare\", \"makeup\"]\r\n    }\r\n    \r\n    text_lower = text.lower()\r\n    scores = {}\r\n    \r\n    for category, keywords in CATEGORY_KEYWORDS.items():\r\n        score = sum(1 for keyword in keywords if keyword.lower() in text_lower)\r\n        if score > 0:\r\n            scores[category] = min(score / 5, 1.0)  # å½’ä¸€åŒ–åˆ°0-1\r\n    \r\n    if scores:\r\n        # æŒ‰ç½®ä¿¡åº¦æ’åº\r\n        sorted_categories = sorted(scores.items(), key=lambda x: x[1], reverse=True)\r\n        return {\r\n            \"primary_category\": sorted_categories[0][0],\r\n            \"confidence\": round(sorted_categories[0][1], 2),\r\n            \"all_categories\": {cat: round(conf, 2) for cat, conf in sorted_categories[:3]}\r\n        }\r\n    else:\r\n        return {\"primary_category\": \"æœªåˆ†ç±»\", \"confidence\": 0.0, \"all_categories\": {}}\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\ntext_content = \"è¿™æ¬¾å¥¢ä¾ˆå“æ‰‹è¡¨é‡‡ç”¨é«˜ç«¯è®¾è®¡ï¼Œé€‚åˆå•†åŠ¡åœºåˆ\"\r\ncategorization = categorize_with_confidence(text_content)\r\nprint(json.dumps(categorization, ensure_ascii=False, indent=2))\r\n```\r\n\r\n### 5. HTMLç»“æ„åŒ–æå–å™¨ï¼ˆå‡½æ•°å¼ç‰ˆæœ¬ï¼‰\r\n```python\r\ndef extract_html_title_and_links(html_content: str) -> dict:\r\n    \"\"\"\r\n    æå–HTMLé¡µé¢æ ‡é¢˜å’Œé“¾æ¥ - å‡½æ•°å¼ç‰ˆæœ¬\r\n    æ³¨æ„ï¼šæ²™ç›’ç¯å¢ƒä¸­å¯èƒ½æ²¡æœ‰BeautifulSoupï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼\r\n    \"\"\"\r\n    # ä½¿ç”¨æ­£åˆ™æå–æ ‡é¢˜\r\n    title_match = re.search(r'<title[^>]*>(.*?)</title>', html_content, re.IGNORECASE | re.DOTALL)\r\n    title = title_match.group(1).strip() if title_match else \"æ— æ ‡é¢˜\"\r\n    \r\n    # ä½¿ç”¨æ­£åˆ™æå–é“¾æ¥\r\n    links = []\r\n    link_pattern = r'<a[^>]*href=\"([^\"]*)\"[^>]*>(.*?)</a>'\r\n    \r\n    for match in re.finditer(link_pattern, html_content, re.IGNORECASE | re.DOTALL):\r\n        href = match.group(1)\r\n        text = re.sub(r'<[^>]+>', '', match.group(2)).strip()  # ç§»é™¤HTMLæ ‡ç­¾\r\n        \r\n        if href and (href.startswith('http://') or href.startswith('https://') or href.startswith('/')):\r\n            links.append({\r\n                \"text\": text[:50],  # é™åˆ¶æ–‡æœ¬é•¿åº¦\r\n                \"href\": href[:200]  # é™åˆ¶URLé•¿åº¦\r\n            })\r\n        \r\n        if len(links) >= 10:  # æœ€å¤šæå–10ä¸ªé“¾æ¥\r\n            break\r\n    \r\n    return {\r\n        \"title\": title,\r\n        \"links\": links,\r\n        \"total_links_found\": len(links)\r\n    }\r\n\r\ndef extract_simple_table_data(html_content: str) -> list:\r\n    \"\"\"\r\n    ç®€å•æå–HTMLè¡¨æ ¼æ•°æ® - å‡½æ•°å¼ç‰ˆæœ¬\r\n    ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼Œä¸ä¾èµ–å¤–éƒ¨åº“\r\n    \"\"\"\r\n    tables = []\r\n    \r\n    # æŸ¥æ‰¾æ‰€æœ‰<table>æ ‡ç­¾\r\n    table_pattern = r'<table[^>]*>(.*?)</table>'\r\n    \r\n    for table_match in re.finditer(table_pattern, html_content, re.IGNORECASE | re.DOTALL):\r\n        table_html = table_match.group(1)\r\n        rows = []\r\n        \r\n        # æå–è¡Œ\r\n        row_pattern = r'<tr[^>]*>(.*?)</tr>'\r\n        for row_match in re.finditer(row_pattern, table_html, re.IGNORECASE | re.DOTALL):\r\n            row_html = row_match.group(1)\r\n            cells = []\r\n            \r\n            # æå–å•å…ƒæ ¼\r\n            cell_pattern = r'<t[dh][^>]*>(.*?)</t[dh]>'\r\n            for cell_match in re.finditer(cell_pattern, row_html, re.IGNORECASE | re.DOTALL):\r\n                cell_content = re.sub(r'<[^>]+>', '', cell_match.group(1)).strip()\r\n                cells.append(cell_content)\r\n            \r\n            if cells:  # åªæ·»åŠ éç©ºè¡Œ\r\n                rows.append(cells)\r\n        \r\n        if rows:  # åªæ·»åŠ æœ‰æ•°æ®çš„è¡¨æ ¼\r\n            tables.append({\r\n                \"row_count\": len(rows),\r\n                \"col_count\": len(rows[0]) if rows else 0,\r\n                \"data\": rows[:20]  # é™åˆ¶è¡Œæ•°\r\n            })\r\n    \r\n    return tables\r\n\r\n# ä½¿ç”¨ç¤ºä¾‹\r\nhtml_content = \"\"\"\r\n<html>\r\n<head><title>ç¤ºä¾‹é¡µé¢</title></head>\r\n<body>\r\n    <h1>äº§å“åˆ—è¡¨</h1>\r\n    <a href=\"/products/1\">äº§å“1</a>\r\n    <a href=\"/products/2\">äº§å“2</a>\r\n    <table>\r\n        <tr><th>åç§°</th><th>ä»·æ ¼</th></tr>\r\n        <tr><td>äº§å“A</td><td>$100</td></tr>\r\n    </table>\r\n</body>\r\n</html>\r\n\"\"\"\r\n\r\ntitle_links = extract_html_title_and_links(html_content)\r\ntables = extract_simple_table_data(html_content)\r\n\r\nprint(\"æ ‡é¢˜å’Œé“¾æ¥:\", json.dumps(title_links, ensure_ascii=False, indent=2))\r\nprint(\"\\nè¡¨æ ¼æ•°æ®:\", json.dumps(tables, ensure_ascii=False, indent=2))\r\n```\r\n\r\n---\r\n\r\n## ğŸ¯ AIä½¿ç”¨æŒ‡å—\r\n\r\n### æ­¥éª¤ä¸€ï¼šè¯†åˆ«åˆ†æéœ€æ±‚\r\nå½“ç”¨æˆ·è¯·æ±‚åˆ†ææ–‡æœ¬æ—¶ï¼ŒAIåº”ï¼š\r\n1. ç¡®è®¤æ–‡æœ¬å†…å®¹æ˜¯å¦å·²æä¾›\r\n2. è¯†åˆ«åˆ†æç›®æ ‡ï¼ˆä»·æ ¼ã€è§„æ ¼ã€åˆ†ç±»ç­‰ï¼‰\r\n3. é€‰æ‹©åˆé€‚çš„æå–å™¨ç»„åˆ\r\n4. **é¿å…ä½¿ç”¨ç±»å®šä¹‰ï¼Œä½¿ç”¨å‡½æ•°å¼ç¼–ç¨‹**\r\n\r\n### æ­¥éª¤äºŒï¼šç”Ÿæˆæ‰§è¡Œä»£ç \r\n```python\r\ndef generate_analysis_code_for_ai(user_text: str, analysis_type: str) -> str:\r\n    \"\"\"\r\n    AIè°ƒç”¨æ­¤å‡½æ•°ç”Ÿæˆå¯æ‰§è¡Œçš„æ²™ç›’ä»£ç \r\n    æ³¨æ„ï¼šè¿™æ˜¯ç»™AIçœ‹çš„æ¨¡æ¿ï¼Œä¸æ˜¯ç›´æ¥åœ¨æ²™ç›’ä¸­æ‰§è¡Œçš„ä»£ç \r\n    \"\"\"\r\n    # ç¤ºä¾‹ä»£ç æ¨¡æ¿\r\n    code_template = f'''\r\nimport json\r\nimport re\r\nfrom datetime import datetime\r\n\r\n# ç”¨æˆ·æä¾›çš„åˆ†ææ–‡æœ¬\r\nTEXT_TO_ANALYZE = \"\"\"{user_text}\"\"\"\r\n\r\ndef analyze_content(text):\r\n    \"\"\"åˆ†æå‡½æ•° - å‡½æ•°å¼ç‰ˆæœ¬\"\"\"\r\n    result = {{\r\n        \"type\": \"analysis_report\",\r\n        \"title\": \"{analysis_type}åˆ†æç»“æœ\",\r\n        \"timestamp\": datetime.now().isoformat(),\r\n        \"data\": {{}}\r\n    }}\r\n    \r\n    # ä»·æ ¼æå–\r\n    price_match = re.search(r'\\\\$\\\\s*(\\\\d+[,\\\\d]*\\\\.?\\\\d*)', text)\r\n    if price_match:\r\n        result[\"data\"][\"price_usd\"] = price_match.group(1)\r\n    \r\n    # è§„æ ¼æå–\r\n    dimensions = {{\r\n        \"height\": re.search(r'(\\\\d+(?:\\\\.\\\\d+)?)\\\\s*(cm|mm|m)\\\\s*é«˜', text, re.IGNORECASE),\r\n        \"width\": re.search(r'(\\\\d+(?:\\\\.\\\\d+)?)\\\\s*(cm|mm|m)\\\\s*å®½', text, re.IGNORECASE)\r\n    }}\r\n    \r\n    for key, match in dimensions.items():\r\n        if match:\r\n            result[\"data\"][key] = match.group(1) + (match.group(2) if match.group(2) else \"\")\r\n    \r\n    return result\r\n\r\n# æ‰§è¡Œåˆ†æ\r\nanalysis_result = analyze_content(TEXT_TO_ANALYZE)\r\n\r\n# ğŸ”¥ å¿…é¡»ï¼šä»¥JSONæ ¼å¼è¾“å‡º\r\nprint(json.dumps(analysis_result, ensure_ascii=False, indent=2))\r\n'''\r\n    return code_template\r\n```\r\n\r\n### æ­¥éª¤ä¸‰ï¼šå¤„ç†è¿”å›ç»“æœ\r\nAIæ”¶åˆ°æ²™ç›’æ‰§è¡Œç»“æœåï¼š\r\n1. éªŒè¯è¾“å‡ºæ ¼å¼æ˜¯å¦æ­£ç¡®\r\n2. æå–å…³é”®ä¿¡æ¯å‘ˆç°ç»™ç”¨æˆ·\r\n3. æä¾›è¿›ä¸€æ­¥åˆ†æå»ºè®®\r\n\r\n---\r\n\r\n## ğŸ”§ æ•…éšœæ’é™¤ä¸æœ€ä½³å®è·µ\r\n\r\n### å¸¸è§é—®é¢˜è§£å†³æ–¹æ¡ˆ\r\n\r\n| é—®é¢˜ | åŸå›  | è§£å†³æ–¹æ¡ˆ |\r\n|------|------|----------|\r\n| æ— è¾“å‡º | ä»£ç æœªæ‰§è¡Œprint | ç¡®ä¿æœ€åä¸€è¡Œæ˜¯print(json.dumps(...)) |\r\n| æ ¼å¼é”™è¯¯ | éJSONè¾“å‡º | ä½¿ç”¨json.dumps()è€Œéstr() |\r\n| æå–ä¸ºç©º | æ–‡æœ¬æ ¼å¼ä¸åŒ¹é… | æ·»åŠ æ›´çµæ´»çš„æ­£åˆ™è¡¨è¾¾å¼ |\r\n| ç¼–ç é—®é¢˜ | ä¸­æ–‡å­—ç¬¦ä¹±ç  | ä½¿ç”¨ensure_ascii=Falseå‚æ•° |\r\n| ç±»å®šä¹‰é”™è¯¯ | æ²™ç›’ä¸æ”¯æŒç±» | ä½¿ç”¨å‡½æ•°å¼ç¼–ç¨‹æ›¿ä»£ |\r\n\r\n### ä¼˜åŒ–å»ºè®®\r\n1. **å¢é‡æå–**ï¼šå…ˆå°è¯•ç®€å•è§„åˆ™ï¼Œå†é€æ­¥å¤æ‚åŒ–\r\n2. **é”™è¯¯æ¢å¤**ï¼šæå–å¤±è´¥æ—¶æä¾›é»˜è®¤å€¼è€Œéä¸­æ–­\r\n3. **æ€§èƒ½ä¼˜åŒ–**ï¼šé™åˆ¶æ­£åˆ™è¡¨è¾¾å¼å¤æ‚åº¦\r\n4. **ç»“æœéªŒè¯**ï¼šæ£€æŸ¥æå–ç»“æœçš„åˆç†æ€§\r\n5. **å‡½æ•°å¼ä¼˜å…ˆ**ï¼šé¿å…ç±»å®šä¹‰ï¼Œä½¿ç”¨çº¯å‡½æ•°\r\n\r\n---\r\n\r\n## ğŸ“‹ å®Œæ•´å·¥ä½œæµç¤ºä¾‹\r\n\r\n```python\r\n# ===================== å®Œæ•´åˆ†æå·¥ä½œæµï¼ˆå‡½æ•°å¼ç‰ˆæœ¬ï¼‰=====================\r\nimport json\r\nimport re\r\nfrom datetime import datetime\r\n\r\ndef complete_analysis_workflow(data_context: str) -> str:\r\n    \"\"\"\r\n    ç«¯åˆ°ç«¯çš„æ–‡æœ¬åˆ†æå·¥ä½œæµ - å‡½æ•°å¼ç‰ˆæœ¬\r\n    è¾“å…¥ï¼šçˆ¬è™«è·å–çš„æ–‡æœ¬æ•°æ®\r\n    è¾“å‡ºï¼šæ ‡å‡†åŒ–çš„åˆ†ææŠ¥å‘Š\r\n    \"\"\"\r\n    \r\n    # 1. å¹¶è¡Œæå–å„ç±»ä¿¡æ¯ï¼ˆä½¿ç”¨å‡½æ•°è€Œéç±»ï¼‰\r\n    price_info = extract_price_info(data_context)\r\n    dimensions = extract_dimensions(data_context)\r\n    categories = categorize_with_confidence(data_context)\r\n    \r\n    # 2. æ„å»ºç»“æœ\r\n    report = {\r\n        \"type\": \"comprehensive_analysis\",\r\n        \"title\": \"ç»¼åˆæ–‡æœ¬åˆ†ææŠ¥å‘Š\",\r\n        \"data\": {\r\n            \"ä»·æ ¼ä¿¡æ¯\": price_info,\r\n            \"è§„æ ¼å‚æ•°\": dimensions,\r\n            \"å†…å®¹åˆ†ç±»\": categories,\r\n            \"æ–‡æœ¬é•¿åº¦\": len(data_context),\r\n            \"å…³é”®å¥å­\": extract_key_sentences(data_context)\r\n        },\r\n        \"metadata\": {\r\n            \"åˆ†æå·¥å…·\": \"æ²™ç›’å†…ç½®åˆ†æå¥—ä»¶\",\r\n            \"åˆ†ææ—¶é—´\": datetime.now().isoformat(),\r\n            \"ç½®ä¿¡åº¦\": calculate_confidence(price_info, dimensions)\r\n        }\r\n    }\r\n    \r\n    return json.dumps(report, ensure_ascii=False, indent=2)\r\n\r\n# è¾…åŠ©å‡½æ•°\r\ndef extract_key_sentences(text: str, max_sentences: int = 3) -> list:\r\n    \"\"\"æå–å…³é”®å¥å­\"\"\"\r\n    # ç®€å•åˆ†å¥é€»è¾‘\r\n    sentences = []\r\n    current = \"\"\r\n    \r\n    for char in text:\r\n        current += char\r\n        if char in 'ã€‚ï¼ï¼Ÿ.!?':\r\n            sentence = current.strip()\r\n            if len(sentence) > 10:\r\n                sentences.append(sentence)\r\n            current = \"\"\r\n        \r\n        if len(sentences) >= max_sentences:\r\n            break\r\n    \r\n    # å¦‚æœæ²¡æ‰¾åˆ°è¶³å¤Ÿå¥å­ï¼ŒæŒ‰æ¢è¡Œåˆ†å‰²\r\n    if len(sentences) < max_sentences:\r\n        lines = [line.strip() for line in text.split('\\n') if len(line.strip()) > 10]\r\n        sentences.extend(lines[:max_sentences - len(sentences)])\r\n    \r\n    return sentences[:max_sentences]\r\n\r\ndef calculate_confidence(price_info: dict, dimensions: dict) -> str:\r\n    \"\"\"è®¡ç®—åˆ†æç½®ä¿¡åº¦\"\"\"\r\n    price_matches = price_info.get('price_matches', [])\r\n    has_dimensions = bool(dimensions)\r\n    \r\n    if price_matches and has_dimensions:\r\n        return \"é«˜\"\r\n    elif price_matches or has_dimensions:\r\n        return \"ä¸­\"\r\n    else:\r\n        return \"ä½\"\r\n\r\n# ä¸»æ‰§è¡Œé€»è¾‘\r\nif __name__ == \"__main__\":\r\n    # ç¤ºä¾‹æ–‡æœ¬\r\n    sample_text = \"\"\"\r\n    äº§å“ï¼šé«˜ç«¯æ™ºèƒ½æ‰‹è¡¨\r\n    ä»·æ ¼ï¼š$299.99\r\n    å°ºå¯¸ï¼šé«˜åº¦45mmï¼Œå®½åº¦38mm\r\n    æè´¨ï¼šä¸é”ˆé’¢è¡¨å£³ï¼Œè“å®çŸ³ç»ç’ƒ\r\n    åŠŸèƒ½ï¼šå¿ƒç‡ç›‘æµ‹ï¼ŒGPSå®šä½\r\n    \"\"\"\r\n    \r\n    result = complete_analysis_workflow(sample_text)\r\n    print(result)\r\n```\r\n\r\n---\r\n\r\n## âœ… éªŒè¯æµ‹è¯•\r\n\r\nè¿è¡Œä»¥ä¸‹ä»£ç éªŒè¯æ‚¨çš„åˆ†æå™¨ï¼š\r\n\r\n```python\r\n# æµ‹è¯•ç”¨ä¾‹ - å‡½æ•°å¼ç‰ˆæœ¬\r\nimport json\r\n\r\ntest_cases = [\r\n    (\"Jimmy Choo DIDI 45 ä»·æ ¼ $299.99 æè´¨çš®é© é«˜åº¦45mm\", \"äº§å“é¡µé¢åˆ†æ\"),\r\n    (\"iPhone 15 Pro Max å”®ä»· Â¥9999 é‡é‡ 221g å®½åº¦78mm\", \"ç”µå­äº§å“åˆ†æ\"),\r\n    (\"å®æœ¨é¤æ¡Œ å°ºå¯¸ 180x90cm ä»·æ ¼ â‚¬459 é«˜åº¦75cm\", \"å®¶å±…äº§å“åˆ†æ\")\r\n]\r\n\r\nfor test_text, expected_type in test_cases:\r\n    # ä½¿ç”¨å‡½æ•°å¼åˆ†æå™¨\r\n    dimensions = extract_dimensions(test_text)\r\n    categories = categorize_content(test_text)\r\n    \r\n    result = {\r\n        \"type\": \"test_result\",\r\n        \"test_case\": expected_type,\r\n        \"dimensions\": dimensions,\r\n        \"categories\": categories,\r\n        \"has_price\": \"$\" in test_text or \"Â¥\" in test_text or \"â‚¬\" in test_text\r\n    }\r\n    \r\n    print(f\"æµ‹è¯•: {expected_type}\")\r\n    print(f\"ç»“æœ: {json.dumps(result, ensure_ascii=False, indent=2)}\")\r\n    print(\"-\" * 50)\r\n```\r\n\r\n---\r\n\r\n## ğŸ“Œ æ€»ç»“è¦ç‚¹\r\n\r\n1. **å®‰å…¨ç¬¬ä¸€**ï¼šæ‰€æœ‰ä»£ç åœ¨æ²™ç›’ä¸­è¿è¡Œï¼Œæ— ç½‘ç»œæ— æ–‡ä»¶é£é™©\r\n2. **æ ¼å¼ä¸ºç‹**ï¼šè¾“å‡ºå¿…é¡»ç¬¦åˆæ ‡å‡†JSONç»“æ„ï¼ŒåŒ…å«typeå­—æ®µ\r\n3. **å‡½æ•°å¼ä¼˜å…ˆ**ï¼šé¿å…ç±»å®šä¹‰ï¼Œä½¿ç”¨çº¯å‡½æ•°è¿›è¡Œæ•°æ®æå–\r\n4. **æ¸è¿›æå–**ï¼šä»ç®€å•è§„åˆ™å¼€å§‹ï¼Œé€æ­¥å¢åŠ å¤æ‚æ€§\r\n5. **é”™è¯¯å¤„ç†**ï¼šæå–å¤±è´¥æ—¶æä¾›åˆç†é»˜è®¤å€¼\r\n6. **æ€§èƒ½æ„è¯†**ï¼šé¿å…å¤æ‚æ­£åˆ™å’Œæ— é™å¾ªç¯\r\n\r\n## ğŸ”„ ä»ç±»åˆ°å‡½æ•°çš„è½¬æ¢æŒ‡å—\r\n\r\n| åŸç±»å®šä¹‰ | è½¬æ¢åçš„å‡½æ•° | ä½¿ç”¨æ–¹å¼ |\r\n|---------|------------|---------|\r\n| `class Extractor:`<br>`def extract(self, text):` | `def extract_data(text):` | `result = extract_data(text)` |\r\n| `obj = Extractor()`<br>`obj.extract(text)` | ç›´æ¥è°ƒç”¨å‡½æ•° | `extract_data(text)` |\r\n| ç±»å±æ€§ï¼ˆ`self.config`ï¼‰ | å‡½æ•°å‚æ•°æˆ–å…¨å±€å¸¸é‡ | `def func(text, config={})` |\r\n| å¤šä¸ªç›¸å…³æ–¹æ³• | å¤šä¸ªç‹¬ç«‹å‡½æ•°æˆ–ä¸»å‡½æ•°è°ƒç”¨å­å‡½æ•° | `def main_func():`<br>`data1 = func1()`<br>`data2 = func2()` |\r\n\r\n## ğŸ¯ æœ€ç»ˆæ£€æŸ¥æ¸…å•\r\n\r\nåœ¨ç”Ÿæˆæ²™ç›’ä»£ç å‰ï¼Œè¯·ç¡®è®¤ï¼š\r\n- [ ] æ²¡æœ‰`class`å…³é”®å­—\r\n- [ ] æ‰€æœ‰åŠŸèƒ½éƒ½æ˜¯å‡½æ•°\r\n- [ ] è¾“å‡ºåŒ…å«`type`å­—æ®µ\r\n- [ ] ä½¿ç”¨`json.dumps()`è¾“å‡º\r\n- [ ] æ²¡æœ‰ç½‘ç»œè¯·æ±‚æˆ–æ–‡ä»¶ç³»ç»Ÿè®¿é—®\r\n- [ ] æ­£åˆ™è¡¨è¾¾å¼æœ‰é™åˆ¶ï¼ˆé¿å…ReDoSï¼‰\r\n\r\n---\r\n"
      }
    },
    "filePath": "D:\\Github_10110531\\gemini_chat\\src\\skills\\python_sandbox",
    "lastUpdated": "2025-12-25T09:04:36.443Z"
  },
  "stockfish_analyzer": {
    "metadata": {
      "name": "stockfish_analyzer",
      "description": "å›½é™…è±¡æ£‹å¼•æ“åˆ†æå·¥å…·ï¼Œæä¾›æœ€ä½³èµ°æ³•æ¨èã€å±€é¢è¯„ä¼°å’Œå¤šç§èµ°æ³•é€‰æ‹©åˆ†æã€‚æ”¯æŒFENå­—ç¬¦ä¸²ç›´æ¥è¾“å…¥åˆ†æã€‚",
      "tool_name": "stockfish_analyzer",
      "category": "chess",
      "priority": 6,
      "tags": [
        "chess",
        "analysis",
        "game",
        "strategy",
        "evaluation",
        "FEN",
        "SAN",
        "position",
        "move",
        "best-move",
        "top-moves",
        "chess-engine",
        "stockfish",
        "board",
        "æ£‹å±€",
        "èµ°æ³•",
        "è¯„ä¼°",
        "å±€é¢"
      ],
      "version": 1.1
    },
    "content": "# å›½é™…è±¡æ£‹AIåŠ©æ•™æŒ‡å—\n\nä½ æ˜¯ä¸€ä½é¡¶çº§çš„å›½é™…è±¡æ£‹AIåŠ©æ•™ã€‚ä½ çš„æ ¸å¿ƒä»»åŠ¡æ˜¯ä½œä¸ºç”¨æˆ·å’Œå¼ºå¤§çš„ \"stockfish_analyzer\" å·¥å…·ä¹‹é—´çš„æ™ºèƒ½æ¡¥æ¢ã€‚ä½  **ä¸è‡ªå·±ä¸‹æ£‹**ï¼Œè€Œæ˜¯ **è°ƒç”¨å·¥å…·** å¹¶ **è§£é‡Šç»“æœ**ã€‚\n\n## ğŸ¯ æ ¸å¿ƒå·¥ä½œæµç¨‹\n\n### 1. **è¯†åˆ«FENå­—ç¬¦ä¸²å’Œç”¨æˆ·æ„å›¾**\n- **FENå­—ç¬¦ä¸²ç‰¹å¾**: è¯†åˆ«å¦‚ `rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1` æ ¼å¼çš„å­—ç¬¦ä¸²\n- **è‡ªåŠ¨è§¦å‘**: å½“æ£€æµ‹åˆ°æœ‰æ•ˆçš„FENå­—ç¬¦ä¸²æ—¶ï¼Œè‡ªåŠ¨è°ƒç”¨åˆ†æå·¥å…·\n- **æ„å›¾åˆ†æ**: æ ¹æ®ç”¨æˆ·é—®é¢˜é€‰æ‹©åˆé€‚æ¨¡å¼ï¼š\n  - **æœ€ä½³èµ°æ³•**: \"æˆ‘è¯¥æ€ä¹ˆèµ°ï¼Ÿ\"ã€\"æœ€ä½³èµ°æ³•\"ã€\"ä¸‹ä¸€æ­¥\" â†’ `get_best_move`\n  - **å¤šç§é€‰æ‹©**: \"å‰ä¸‰æ­¥æ¨è\"ã€\"æœ‰å“ªäº›é€‰æ‹©\"ã€\"å‡ ä¸ªå¥½èµ°æ³•\" â†’ `get_top_moves`\n  - **å±€é¢è¯„ä¼°**: \"è°ä¼˜åŠ¿\"ã€\"å±€é¢å¦‚ä½•\"ã€\"è¯„ä¼°\" â†’ `evaluate_position`\n\n### 2. **è°ƒç”¨æ­£ç¡®å·¥å…·**\næ ¹æ®ç”¨æˆ·æ„å›¾é€‰æ‹©å¯¹åº”çš„åˆ†ææ¨¡å¼ã€‚\n\n### 3. **è§£é‡Šå·¥å…·ç»“æœ**\nå°†ä¸“ä¸šçš„å¼•æ“è¾“å‡ºè½¬åŒ–ä¸ºæ˜“æ‡‚çš„æ•™å­¦è¯­è¨€ã€‚\n\n## ğŸ“‹ å¿«é€Ÿä½¿ç”¨æŒ‡å—\n\n### åœºæ™¯1ï¼šç›´æ¥FENåˆ†æ\n**ç”¨æˆ·è¾“å…¥**: `rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1`\n**è‡ªåŠ¨å“åº”**: åˆ†æåˆå§‹å±€é¢ï¼Œæä¾›æœ€ä½³èµ°æ³•å’Œè¯„ä¼°\n\n### åœºæ™¯2ï¼šFEN + ç®€å•æŒ‡ä»¤  \n**ç”¨æˆ·è¾“å…¥**: `r1bqkbnr/pp1ppppp/2n5/2p5/4P3/5N2/PPPP1PPP/RNBQKB1R w KQkq - 2 3` å‰ä¸‰æ­¥æ¨è\n**å·¥å…·è°ƒç”¨**: `get_top_moves` with `top_n: 3`\n\n### åœºæ™¯3ï¼šå±€é¢è¯„ä¼°è¯·æ±‚\n**ç”¨æˆ·è¾“å…¥**: `r1bqkbnr/pp1ppppp/2n5/2p5/4P3/5N2/PPPP1PPP/RNBQKB1R w KQkq - 2 3` ç°åœ¨è°ä¼˜åŠ¿ï¼Ÿ\n**å·¥å…·è°ƒç”¨**: `evaluate_position`\n\n## ğŸ”§ å·¥å…·è°ƒç”¨è§„èŒƒ\n\n**é‡è¦æç¤º**: å½“ä½ å†³å®šè°ƒç”¨ `stockfish_analyzer` å·¥å…·æ—¶ï¼Œä½ çš„æ€è€ƒè¿‡ç¨‹åº”è¯¥ç”Ÿæˆä¸€ä¸ªåŒ…å« `tool_name` å’Œ `parameters` å­—æ®µçš„JSONå¯¹è±¡ã€‚`parameters` å­—æ®µçš„å€¼å¿…é¡»ä¸¥æ ¼éµå®ˆå·¥å…·çš„è¾“å…¥æ¨¡å¼ã€‚\n\n### âœ… æ­£ç¡®çš„è°ƒç”¨ç»“æ„\n```json\n{\n  \"tool_name\": \"stockfish_analyzer\",\n  \"parameters\": {\n    \"fen\": \"<FENå­—ç¬¦ä¸²>\",\n    \"mode\": \"<åŠŸèƒ½æ¨¡å¼>\",\n    \"options\": {\n      \"<é€‰é¡¹å>\": \"<é€‰é¡¹å€¼>\"\n    }\n  }\n}\n```\n\n### åŠŸèƒ½æ¨¡å¼è¯¦è§£\n\n#### 1. è·å–æœ€ä½³èµ°æ³• (`get_best_move`)\n**é€‚ç”¨åœºæ™¯**: ç”¨æˆ·è¯¢é—®\"æœ€ä½³èµ°æ³•\"ã€\"ä¸‹ä¸€æ­¥æ€ä¹ˆèµ°\"\n```json\n{\n  \"tool_name\": \"stockfish_analyzer\",\n  \"parameters\": {\n    \"fen\": \"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\",\n    \"mode\": \"get_best_move\"\n  }\n}\n```\n\n#### 2. è·å–å¤šä¸ªèµ°æ³•é€‰é¡¹ (`get_top_moves`)\n**é€‚ç”¨åœºæ™¯**: ç”¨æˆ·è¯¢é—®\"å‰ä¸‰æ­¥\"ã€\"æœ‰å“ªäº›é€‰æ‹©\"ã€\"å‡ ä¸ªå¥½èµ°æ³•\"\n```json\n{\n  \"tool_name\": \"stockfish_analyzer\", \n  \"parameters\": {\n    \"fen\": \"r1bqkbnr/pp1ppppp/2n5/2p5/4P3/5N2/PPPP1PPP/RNBQKB1R w KQkq - 2 3\",\n    \"mode\": \"get_top_moves\",\n    \"options\": {\n      \"top_n\": 3\n    }\n  }\n}\n```\n\n#### 3. è¯„ä¼°å±€é¢ (`evaluate_position`)\n**é€‚ç”¨åœºæ™¯**: ç”¨æˆ·è¯¢é—®\"å±€é¢å¦‚ä½•\"ã€\"è°ä¼˜åŠ¿\"ã€\"è¯„ä¼°ä¸€ä¸‹\"\n```json\n{\n  \"tool_name\": \"stockfish_analyzer\",\n  \"parameters\": {\n    \"fen\": \"r1bqkbnr/pp1ppppp/2n5/2p5/4P3/5N2/PPPP1PPP/RNBQKB1R w KQkq - 2 3\", \n    \"mode\": \"evaluate_position\"\n  }\n}\n```\n\n## âŒ é”™è¯¯ç¤ºä¾‹ (è¯·é¿å…ä»¥ä¸‹å¸¸è§é”™è¯¯)\n\n- **ç¼ºå°‘ `fen` å‚æ•°**: `{\"tool_name\": \"stockfish_analyzer\", \"parameters\": {\"mode\": \"get_best_move\"}}`\n- **é”™è¯¯çš„ `mode` åç§°**: `{\"tool_name\": \"stockfish_analyzer\", \"parameters\": {\"fen\": \"...\", \"mode\": \"best_move\"}}` (åº”ä¸º \"get_best_move\")\n- **options æ ¼å¼é”™è¯¯**: `{\"tool_name\": \"stockfish_analyzer\", \"parameters\": {\"fen\": \"...\", \"mode\": \"get_top_moves\", \"options\": 3}}` (options å¿…é¡»æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œå¦‚ `{\"top_n\": 3}`)\n\n## ğŸ’¡ ç»“æœè§£é‡ŠæŒ‡å—\n\n### è¯„ä¼°åˆ†æ•°è§£é‡Š\n- **å…µå€¼ä¼˜åŠ¿**: `\"evaluation\": {\"type\": \"cp\", \"value\": 250}` â†’ \"ç™½æ–¹æœ‰æ˜æ˜¾ä¼˜åŠ¿ï¼Œç›¸å½“äºå¤š2.5ä¸ªå…µ\"\n- **è½»å¾®ä¼˜åŠ¿**: `\"evaluation\": {\"type\": \"cp\", \"value\": -120}` â†’ \"é»‘æ–¹ç¨å ä¼˜ï¼Œä¼˜åŠ¿çº¦1.2ä¸ªå…µ\"  \n- **å°†æ­»å±€é¢**: `\"evaluation\": {\"type\": \"mate\", \"value\": 3}` â†’ \"ç™½æ–¹3æ­¥å†…å¯å°†æ­»å¯¹æ–¹\"\n\n### èµ°æ³•è§£é‡Š\n- **UCIè½¬SAN**: `\"best_move\": \"g1f3\"` â†’ \"æœ€ä½³èµ°æ³•æ˜¯ **Nf3**\"\n- **æˆ˜ç•¥æ„å›¾**: è§£é‡Šèµ°æ³•çš„ç›®çš„å’Œæˆ˜ç•¥æ„ä¹‰\n- **å¤šèµ°æ³•æ¯”è¾ƒ**: å½“æœ‰å¤šä¸ªé€‰é¡¹æ—¶ï¼Œåˆ†æå„è‡ªçš„ä¼˜ç¼ºç‚¹\n\n## ğŸš€ æ™ºèƒ½è¯†åˆ«å¢å¼º\n\n### FENå­—ç¬¦ä¸²ç‰¹å¾è¯†åˆ«\n- **æ ¼å¼ç‰¹å¾**: åŒ…å« `/` åˆ†éš”çš„è¡Œã€`w`/`b` èµ°å­æ–¹ã€æ˜“ä½æƒåˆ©ç­‰\n- **è‡ªåŠ¨æ£€æµ‹**: æ£€æµ‹åˆ°FENæ ¼å¼æ—¶è‡ªåŠ¨è§¦å‘åˆ†æ\n- **å®¹é”™å¤„ç†**: å¤„ç†å¸¸è§çš„FENæ ¼å¼å˜ä½“\n\n### ç”¨æˆ·æ„å›¾å…³é”®è¯\n- **æœ€ä½³èµ°æ³•ç±»**: \"æœ€ä½³\"ã€\"æœ€å¥½\"ã€\"æ€ä¹ˆèµ°\"ã€\"ä¸‹ä¸€æ­¥\"\n- **å¤šé€‰é¡¹ç±»**: \"å‡ ä¸ª\"ã€\"å“ªäº›\"ã€\"é€‰æ‹©\"ã€\"æ¨è\"ã€\"å‰ä¸‰\"  \n- **è¯„ä¼°ç±»**: \"è¯„ä¼°\"ã€\"ä¼˜åŠ¿\"ã€\"å±€é¢\"ã€\"è°å¥½\"\n- **ä¸­è‹±æ–‡æ··åˆ**: æ”¯æŒä¸­æ–‡æŒ‡ä»¤å¦‚\"æ£‹å±€\"ã€\"èµ°æ³•\"ã€\"è¯„ä¼°\"\n\n## âš ï¸ å¸¸è§é—®é¢˜å¤„ç†\n\n### FENè¯†åˆ«é—®é¢˜\n**ç”¨æˆ·è¾“å…¥ä¸åŒ…å«FEN**:\n```\n\"è¯·æä¾›å½“å‰å±€é¢çš„FENå­—ç¬¦ä¸²ï¼Œæ ¼å¼å¦‚: rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\"\n```\n\n**æ— æ•ˆFENæ ¼å¼**:\n```\n\"è¿™ä¸ªFENå­—ç¬¦ä¸²æ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·æ£€æŸ¥å¹¶é‡æ–°æä¾›æœ‰æ•ˆçš„FENå­—ç¬¦ä¸²\"\n```\n\n### æ¨¡å¼é€‰æ‹©å»ºè®®\n**æ¨¡ç³ŠæŒ‡ä»¤**:\n```\n\"æ‚¨æ˜¯æƒ³çŸ¥é“æœ€ä½³èµ°æ³•ï¼Œè¿˜æ˜¯æƒ³çœ‹çœ‹å¤šä¸ªé€‰æ‹©ï¼Ÿ\"\n```\n\n## ğŸ“ æœ€ä½³å®è·µ\n\n### å“åº”æ¨¡æ¿\n1. **ç¡®è®¤å±€é¢**: \"åˆ†ææ‚¨æä¾›çš„å±€é¢...\"\n2. **è°ƒç”¨å·¥å…·**: [è‡ªåŠ¨è°ƒç”¨å¯¹åº”æ¨¡å¼]\n3. **è§£é‡Šç»“æœ**: ç”¨é€šä¿—è¯­è¨€è§£é‡Šå¼•æ“åˆ†æ\n4. **æ•™å­¦æŒ‡å¯¼**: æä¾›æˆ˜ç•¥å»ºè®®å’Œå­¦ä¹ è¦ç‚¹\n\n### é”™è¯¯å¤„ç†\n- **ç¼ºå°‘FEN**: å‹å¥½æç¤ºç”¨æˆ·æä¾›FEN\n- **æ— æ•ˆFEN**: è¯´æ˜æ­£ç¡®æ ¼å¼è¦æ±‚  \n- **ç½‘ç»œé—®é¢˜**: æç¤ºç¨åé‡è¯•\n\n---\n\n**é‡è¦æç¤º**: ä¸¥æ ¼éµå®ˆ\"ä¸åˆ›é€ èµ°æ³•ã€ä¸è‡ªè¡Œè¯„ä¼°\"çš„åŸåˆ™ï¼Œæ‰€æœ‰åˆ†æå¿…é¡»åŸºäºå·¥å…·è¾“å‡ºã€‚ä½ çš„ä»·å€¼åœ¨äºå°†ä¸“ä¸šçš„å¼•æ“åˆ†æè½¬åŒ–ä¸ºæ˜“æ‡‚çš„æ•™å­¦æŒ‡å¯¼ã€‚",
    "resources": {},
    "filePath": "D:\\Github_10110531\\gemini_chat\\src\\skills\\stockfish_analyzer",
    "lastUpdated": "2025-12-25T09:04:36.444Z"
  },
  "tavily_search": {
    "metadata": {
      "name": "tavily_search",
      "description": "ä½¿ç”¨Tavily APIè¿›è¡Œç½‘ç»œæœç´¢ï¼Œè·å–å®æ—¶ä¿¡æ¯ã€å›ç­”é—®é¢˜æˆ–ç ”ç©¶ä¸»é¢˜",
      "tool_name": "tavily_search",
      "category": "search",
      "priority": 8,
      "tags": [
        "search",
        "research",
        "real-time",
        "information"
      ],
      "version": 1
    },
    "content": "# å·¥å…·è°ƒç”¨ç¤ºä¾‹ï¼ˆTavily Searchï¼‰\r\n\r\nå½“æ‚¨å†³å®šè°ƒç”¨ tavily_search å·¥å…·æ—¶ï¼Œæ‚¨çš„å“åº”åº”è¯¥æ˜¯ä¸€ä¸ªåŒ…å« tool_name å’Œ parameters å­—æ®µçš„ JSON å¯¹è±¡ã€‚parameters å­—æ®µçš„å€¼åº”æ˜¯å·¥å…·æ‰€éœ€çš„å‚æ•°å¯¹è±¡ã€‚\r\n\r\n## âœ… æ­£ç¡®ç¤ºä¾‹\r\n\r\n**parameters å­—æ®µå†…å®¹:**\r\n```json\r\n{\"query\": \"latest AI news\"}\r\n```\r\n\r\n**å®Œæ•´å·¥å…·è°ƒç”¨å“åº”ç¤ºä¾‹:**\r\n```json\r\n{\"tool_name\": \"tavily_search\", \"parameters\": {\"query\": \"latest AI news\"}}\r\n```\r\n\r\n## âŒ é”™è¯¯ç¤ºä¾‹ (è¯·é¿å…ä»¥ä¸‹å¸¸è§é”™è¯¯)\r\n\r\n- **åœ¨ JSON ä¸­åµŒå…¥ Markdown åˆ†éš”ç¬¦:** \r\n  ```json\r\n  \"```json\\n{\\\"query\\\": \\\"latest AI news\\\"}\\n```\"\r\n  ```\r\n  (Qwen æ¨¡å‹ä¼šå°†æ­¤ä½œä¸º JSON å­—ç¬¦ä¸²çš„ä¸€éƒ¨åˆ†ï¼Œå¯¼è‡´è§£æå¤±è´¥)\r\n\r\n- **å‚æ•°åé”™è¯¯:** \r\n  ```json\r\n  {\"q\": \"latest AI news\"}\r\n  ```\r\n  (åº”ä¸º \"query\" è€Œé \"q\")\r\n\r\n- **å‚æ•°å€¼é”™è¯¯:** \r\n  ```json\r\n  {\"query\": 123}\r\n  ```\r\n  (query å‚æ•°å€¼åº”ä¸ºå­—ç¬¦ä¸²ï¼Œè€Œä¸æ˜¯æ•°å­—)\r\n\r\n## å…³é”®æŒ‡ä»¤\r\n1. **æŸ¥è¯¢æ„å»º**: æŸ¥è¯¢åº”è¯¥å…·ä½“ä¸”ç›¸å…³\r\n2. **å®æ—¶æ€§**: é€‚ç”¨äºéœ€è¦æœ€æ–°ä¿¡æ¯çš„é—®é¢˜\r\n3. **éªŒè¯**: å¯ç”¨äºéªŒè¯å…¶ä»–ä¿¡æ¯æ¥æº\r\n\r\n## ä½¿ç”¨åœºæ™¯\r\n1. è·å–å®æ—¶æ–°é—»å’Œä¿¡æ¯\r\n2. å›ç­”éœ€è¦æœ€æ–°æ•°æ®çš„é—®é¢˜\r\n3. ç ”ç©¶ç‰¹å®šä¸»é¢˜çš„èƒŒæ™¯ä¿¡æ¯\r\n4. éªŒè¯äº‹å®å’Œæ•°æ®çš„å‡†ç¡®æ€§\r\n\r\n## æœ€ä½³å®è·µ\r\n- æŸ¥è¯¢åº”è¯¥å…·ä½“ä¸”ç›¸å…³\r\n- å¯¹äºå¤æ‚é—®é¢˜ï¼Œå¯ä»¥åˆ†è§£ä¸ºå¤šä¸ªæœç´¢æŸ¥è¯¢\r\n- ç»“åˆæœç´¢ç»“æœè¿›è¡Œç»¼åˆåˆ†æ\r\n- ä¼˜å…ˆä½¿ç”¨è‹±æ–‡å…³é”®è¯è·å–æ›´å‡†ç¡®çš„ç»“æœ\r\n\r\n## ç¤ºä¾‹æŸ¥è¯¢\r\n- \"2024å¹´äººå·¥æ™ºèƒ½æœ€æ–°å‘å±•\"\r\n- \"OpenAIæœ€æ–°æ¨¡å‹å‘å¸ƒä¿¡æ¯\"\r\n- \"æ¯”ç‰¹å¸å½“å‰ä»·æ ¼å’Œè¶‹åŠ¿\"\r\n- \"Python 3.12æ–°ç‰¹æ€§è¯¦è§£\"",
    "resources": {},
    "filePath": "D:\\Github_10110531\\gemini_chat\\src\\skills\\tavily_search",
    "lastUpdated": "2025-12-25T09:04:36.444Z"
  }
};

export function getSkillsRegistry() {
  const map = new Map();
  Object.entries(SKILLS_DATA).forEach(([key, value]) => {
    map.set(key, value);
  });
  return map;
}